{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural-collaborative-filtering by pytorch",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1lF4hHD9u6V0An876TNjTmlG8khUfNwNc",
      "authorship_tag": "ABX9TyOBWM1y3nwpUWSW1GorZ/w9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/overwenyan/neural-collaborative-filtering/blob/master/neural_collaborative_filtering_by_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YLb7Sn7NxTs",
        "outputId": "cf25e2e5-f1fd-44bf-fb39-41181523fc5f"
      },
      "source": [
        "%%shell\n",
        "# pip install pymanopt\n",
        "pip install tensorboardX\n",
        "# pip install git+https://github.com/geoopt/geoopt.git\n",
        "if [ -d  neural-collaborative-filtering ]\n",
        "then\n",
        "    rm -rf  neural-collaborative-filtering\n",
        "fi\n",
        "\n",
        "git clone https://github.com/overwenyan/neural-collaborative-filtering.git\n",
        "# cp NeuralCollapse/problem.py /usr/local/lib/python3.7/dist-packages/pymanopt/core/problem.py\n",
        "# cp -r neural-collaborative-filtering/* .\n",
        "# cd ./src"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Cloning into 'neural-collaborative-filtering'...\n",
            "remote: Enumerating objects: 135, done.\u001b[K\n",
            "remote: Total 135 (delta 0), reused 0 (delta 0), pack-reused 135\u001b[K\n",
            "Receiving objects: 100% (135/135), 14.30 MiB | 11.91 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKt2xWuzv1M9"
      },
      "source": [
        "%load_ext autoreload \n",
        "%autoreload 2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "exoQ8F4zv4PV",
        "outputId": "0f2e2251-6ad9-4594-ff76-55402f08434a"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# cd ./src\n",
        "ls\n",
        "cd ./neural-collaborative-filtering/src\n",
        "python train.py"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "[Training Epoch 59] Batch 4281, Loss 0.2538762092590332\n",
            "[Training Epoch 59] Batch 4282, Loss 0.24847270548343658\n",
            "[Training Epoch 59] Batch 4283, Loss 0.2718367576599121\n",
            "[Training Epoch 59] Batch 4284, Loss 0.25051647424697876\n",
            "[Training Epoch 59] Batch 4285, Loss 0.22778761386871338\n",
            "[Training Epoch 59] Batch 4286, Loss 0.2482559084892273\n",
            "[Training Epoch 59] Batch 4287, Loss 0.24881896376609802\n",
            "[Training Epoch 59] Batch 4288, Loss 0.26798155903816223\n",
            "[Training Epoch 59] Batch 4289, Loss 0.2903740108013153\n",
            "[Training Epoch 59] Batch 4290, Loss 0.27537259459495544\n",
            "[Training Epoch 59] Batch 4291, Loss 0.2655603587627411\n",
            "[Training Epoch 59] Batch 4292, Loss 0.23004359006881714\n",
            "[Training Epoch 59] Batch 4293, Loss 0.27497270703315735\n",
            "[Training Epoch 59] Batch 4294, Loss 0.25590959191322327\n",
            "[Training Epoch 59] Batch 4295, Loss 0.28101441264152527\n",
            "[Training Epoch 59] Batch 4296, Loss 0.2391044646501541\n",
            "[Training Epoch 59] Batch 4297, Loss 0.27761155366897583\n",
            "[Training Epoch 59] Batch 4298, Loss 0.26648661494255066\n",
            "[Training Epoch 59] Batch 4299, Loss 0.24794866144657135\n",
            "[Training Epoch 59] Batch 4300, Loss 0.262939989566803\n",
            "[Training Epoch 59] Batch 4301, Loss 0.2546912133693695\n",
            "[Training Epoch 59] Batch 4302, Loss 0.2715873420238495\n",
            "[Training Epoch 59] Batch 4303, Loss 0.24659986793994904\n",
            "[Training Epoch 59] Batch 4304, Loss 0.23896732926368713\n",
            "[Training Epoch 59] Batch 4305, Loss 0.24610811471939087\n",
            "[Training Epoch 59] Batch 4306, Loss 0.24120652675628662\n",
            "[Training Epoch 59] Batch 4307, Loss 0.2897361218929291\n",
            "[Training Epoch 59] Batch 4308, Loss 0.2613999843597412\n",
            "[Training Epoch 59] Batch 4309, Loss 0.26019564270973206\n",
            "[Training Epoch 59] Batch 4310, Loss 0.2358866184949875\n",
            "[Training Epoch 59] Batch 4311, Loss 0.3041200637817383\n",
            "[Training Epoch 59] Batch 4312, Loss 0.27855074405670166\n",
            "[Training Epoch 59] Batch 4313, Loss 0.2973891496658325\n",
            "[Training Epoch 59] Batch 4314, Loss 0.2675478756427765\n",
            "[Training Epoch 59] Batch 4315, Loss 0.28719577193260193\n",
            "[Training Epoch 59] Batch 4316, Loss 0.2614102065563202\n",
            "[Training Epoch 59] Batch 4317, Loss 0.2631869912147522\n",
            "[Training Epoch 59] Batch 4318, Loss 0.26338210701942444\n",
            "[Training Epoch 59] Batch 4319, Loss 0.25714215636253357\n",
            "[Training Epoch 59] Batch 4320, Loss 0.24753187596797943\n",
            "[Training Epoch 59] Batch 4321, Loss 0.23406393826007843\n",
            "[Training Epoch 59] Batch 4322, Loss 0.2629677951335907\n",
            "[Training Epoch 59] Batch 4323, Loss 0.2735711932182312\n",
            "[Training Epoch 59] Batch 4324, Loss 0.2639752924442291\n",
            "[Training Epoch 59] Batch 4325, Loss 0.297760009765625\n",
            "[Training Epoch 59] Batch 4326, Loss 0.23537947237491608\n",
            "[Training Epoch 59] Batch 4327, Loss 0.2837243676185608\n",
            "[Training Epoch 59] Batch 4328, Loss 0.2410443276166916\n",
            "[Training Epoch 59] Batch 4329, Loss 0.2799091637134552\n",
            "[Training Epoch 59] Batch 4330, Loss 0.2986740469932556\n",
            "[Training Epoch 59] Batch 4331, Loss 0.2604541480541229\n",
            "[Training Epoch 59] Batch 4332, Loss 0.25969064235687256\n",
            "[Training Epoch 59] Batch 4333, Loss 0.2814723253250122\n",
            "[Training Epoch 59] Batch 4334, Loss 0.24720321595668793\n",
            "[Training Epoch 59] Batch 4335, Loss 0.27812713384628296\n",
            "[Training Epoch 59] Batch 4336, Loss 0.24058187007904053\n",
            "[Training Epoch 59] Batch 4337, Loss 0.26932862401008606\n",
            "[Training Epoch 59] Batch 4338, Loss 0.25422361493110657\n",
            "[Training Epoch 59] Batch 4339, Loss 0.2545599341392517\n",
            "[Training Epoch 59] Batch 4340, Loss 0.2393854856491089\n",
            "[Training Epoch 59] Batch 4341, Loss 0.2739723324775696\n",
            "[Training Epoch 59] Batch 4342, Loss 0.24837151169776917\n",
            "[Training Epoch 59] Batch 4343, Loss 0.2597452402114868\n",
            "[Training Epoch 59] Batch 4344, Loss 0.26372724771499634\n",
            "[Training Epoch 59] Batch 4345, Loss 0.2748132348060608\n",
            "[Training Epoch 59] Batch 4346, Loss 0.24998480081558228\n",
            "[Training Epoch 59] Batch 4347, Loss 0.2561838924884796\n",
            "[Training Epoch 59] Batch 4348, Loss 0.26342958211898804\n",
            "[Training Epoch 59] Batch 4349, Loss 0.2400261014699936\n",
            "[Training Epoch 59] Batch 4350, Loss 0.2787551283836365\n",
            "[Training Epoch 59] Batch 4351, Loss 0.2572041451931\n",
            "[Training Epoch 59] Batch 4352, Loss 0.2725907266139984\n",
            "[Training Epoch 59] Batch 4353, Loss 0.2604130804538727\n",
            "[Training Epoch 59] Batch 4354, Loss 0.2746775150299072\n",
            "[Training Epoch 59] Batch 4355, Loss 0.2726583778858185\n",
            "[Training Epoch 59] Batch 4356, Loss 0.2759891152381897\n",
            "[Training Epoch 59] Batch 4357, Loss 0.2838086187839508\n",
            "[Training Epoch 59] Batch 4358, Loss 0.2468366175889969\n",
            "[Training Epoch 59] Batch 4359, Loss 0.24208204448223114\n",
            "[Training Epoch 59] Batch 4360, Loss 0.2545241415500641\n",
            "[Training Epoch 59] Batch 4361, Loss 0.27471408247947693\n",
            "[Training Epoch 59] Batch 4362, Loss 0.2854427993297577\n",
            "[Training Epoch 59] Batch 4363, Loss 0.28212589025497437\n",
            "[Training Epoch 59] Batch 4364, Loss 0.27561527490615845\n",
            "[Training Epoch 59] Batch 4365, Loss 0.27149444818496704\n",
            "[Training Epoch 59] Batch 4366, Loss 0.26619192957878113\n",
            "[Training Epoch 59] Batch 4367, Loss 0.2627852261066437\n",
            "[Training Epoch 59] Batch 4368, Loss 0.25729310512542725\n",
            "[Training Epoch 59] Batch 4369, Loss 0.25426438450813293\n",
            "[Training Epoch 59] Batch 4370, Loss 0.2569321095943451\n",
            "[Training Epoch 59] Batch 4371, Loss 0.2591051161289215\n",
            "[Training Epoch 59] Batch 4372, Loss 0.2661488950252533\n",
            "[Training Epoch 59] Batch 4373, Loss 0.24274414777755737\n",
            "[Training Epoch 59] Batch 4374, Loss 0.2611366808414459\n",
            "[Training Epoch 59] Batch 4375, Loss 0.2665507197380066\n",
            "[Training Epoch 59] Batch 4376, Loss 0.24203254282474518\n",
            "[Training Epoch 59] Batch 4377, Loss 0.28614264726638794\n",
            "[Training Epoch 59] Batch 4378, Loss 0.2708425223827362\n",
            "[Training Epoch 59] Batch 4379, Loss 0.25298547744750977\n",
            "[Training Epoch 59] Batch 4380, Loss 0.22226867079734802\n",
            "[Training Epoch 59] Batch 4381, Loss 0.247223362326622\n",
            "[Training Epoch 59] Batch 4382, Loss 0.2871788442134857\n",
            "[Training Epoch 59] Batch 4383, Loss 0.26677021384239197\n",
            "[Training Epoch 59] Batch 4384, Loss 0.2882784605026245\n",
            "[Training Epoch 59] Batch 4385, Loss 0.301859587430954\n",
            "[Training Epoch 59] Batch 4386, Loss 0.2507970333099365\n",
            "[Training Epoch 59] Batch 4387, Loss 0.2860110104084015\n",
            "[Training Epoch 59] Batch 4388, Loss 0.24998018145561218\n",
            "[Training Epoch 59] Batch 4389, Loss 0.2643145024776459\n",
            "[Training Epoch 59] Batch 4390, Loss 0.25744733214378357\n",
            "[Training Epoch 59] Batch 4391, Loss 0.2614729404449463\n",
            "[Training Epoch 59] Batch 4392, Loss 0.2593802809715271\n",
            "[Training Epoch 59] Batch 4393, Loss 0.2882862687110901\n",
            "[Training Epoch 59] Batch 4394, Loss 0.25277915596961975\n",
            "[Training Epoch 59] Batch 4395, Loss 0.2695971131324768\n",
            "[Training Epoch 59] Batch 4396, Loss 0.2578635811805725\n",
            "[Training Epoch 59] Batch 4397, Loss 0.26250022649765015\n",
            "[Training Epoch 59] Batch 4398, Loss 0.2768988311290741\n",
            "[Training Epoch 59] Batch 4399, Loss 0.30516281723976135\n",
            "[Training Epoch 59] Batch 4400, Loss 0.2876148521900177\n",
            "[Training Epoch 59] Batch 4401, Loss 0.27110230922698975\n",
            "[Training Epoch 59] Batch 4402, Loss 0.25883936882019043\n",
            "[Training Epoch 59] Batch 4403, Loss 0.2767093777656555\n",
            "[Training Epoch 59] Batch 4404, Loss 0.26124054193496704\n",
            "[Training Epoch 59] Batch 4405, Loss 0.28328463435173035\n",
            "[Training Epoch 59] Batch 4406, Loss 0.2503165006637573\n",
            "[Training Epoch 59] Batch 4407, Loss 0.2527090907096863\n",
            "[Training Epoch 59] Batch 4408, Loss 0.273592084646225\n",
            "[Training Epoch 59] Batch 4409, Loss 0.23388460278511047\n",
            "[Training Epoch 59] Batch 4410, Loss 0.259795218706131\n",
            "[Training Epoch 59] Batch 4411, Loss 0.2637147009372711\n",
            "[Training Epoch 59] Batch 4412, Loss 0.24585846066474915\n",
            "[Training Epoch 59] Batch 4413, Loss 0.24428898096084595\n",
            "[Training Epoch 59] Batch 4414, Loss 0.2669999301433563\n",
            "[Training Epoch 59] Batch 4415, Loss 0.2599748969078064\n",
            "[Training Epoch 59] Batch 4416, Loss 0.2549499571323395\n",
            "[Training Epoch 59] Batch 4417, Loss 0.2943458557128906\n",
            "[Training Epoch 59] Batch 4418, Loss 0.2965346872806549\n",
            "[Training Epoch 59] Batch 4419, Loss 0.2585134506225586\n",
            "[Training Epoch 59] Batch 4420, Loss 0.30449312925338745\n",
            "[Training Epoch 59] Batch 4421, Loss 0.2716263234615326\n",
            "[Training Epoch 59] Batch 4422, Loss 0.26019638776779175\n",
            "[Training Epoch 59] Batch 4423, Loss 0.2533772885799408\n",
            "[Training Epoch 59] Batch 4424, Loss 0.24770046770572662\n",
            "[Training Epoch 59] Batch 4425, Loss 0.24993246793746948\n",
            "[Training Epoch 59] Batch 4426, Loss 0.2452428638935089\n",
            "[Training Epoch 59] Batch 4427, Loss 0.2677776515483856\n",
            "[Training Epoch 59] Batch 4428, Loss 0.2757711112499237\n",
            "[Training Epoch 59] Batch 4429, Loss 0.27666792273521423\n",
            "[Training Epoch 59] Batch 4430, Loss 0.2437746226787567\n",
            "[Training Epoch 59] Batch 4431, Loss 0.26202839612960815\n",
            "[Training Epoch 59] Batch 4432, Loss 0.29687976837158203\n",
            "[Training Epoch 59] Batch 4433, Loss 0.23996751010417938\n",
            "[Training Epoch 59] Batch 4434, Loss 0.26177915930747986\n",
            "[Training Epoch 59] Batch 4435, Loss 0.274444043636322\n",
            "[Training Epoch 59] Batch 4436, Loss 0.263804167509079\n",
            "[Training Epoch 59] Batch 4437, Loss 0.24327990412712097\n",
            "[Training Epoch 59] Batch 4438, Loss 0.25277695059776306\n",
            "[Training Epoch 59] Batch 4439, Loss 0.26911669969558716\n",
            "[Training Epoch 59] Batch 4440, Loss 0.2748889625072479\n",
            "[Training Epoch 59] Batch 4441, Loss 0.29124122858047485\n",
            "[Training Epoch 59] Batch 4442, Loss 0.2663891613483429\n",
            "[Training Epoch 59] Batch 4443, Loss 0.2680172026157379\n",
            "[Training Epoch 59] Batch 4444, Loss 0.25726932287216187\n",
            "[Training Epoch 59] Batch 4445, Loss 0.22291627526283264\n",
            "[Training Epoch 59] Batch 4446, Loss 0.2955125570297241\n",
            "[Training Epoch 59] Batch 4447, Loss 0.2601583003997803\n",
            "[Training Epoch 59] Batch 4448, Loss 0.2825580835342407\n",
            "[Training Epoch 59] Batch 4449, Loss 0.2666811943054199\n",
            "[Training Epoch 59] Batch 4450, Loss 0.2447660267353058\n",
            "[Training Epoch 59] Batch 4451, Loss 0.23192721605300903\n",
            "[Training Epoch 59] Batch 4452, Loss 0.2593431770801544\n",
            "[Training Epoch 59] Batch 4453, Loss 0.2739337682723999\n",
            "[Training Epoch 59] Batch 4454, Loss 0.2531590461730957\n",
            "[Training Epoch 59] Batch 4455, Loss 0.2555937170982361\n",
            "[Training Epoch 59] Batch 4456, Loss 0.3151915669441223\n",
            "[Training Epoch 59] Batch 4457, Loss 0.2561213970184326\n",
            "[Training Epoch 59] Batch 4458, Loss 0.2654256522655487\n",
            "[Training Epoch 59] Batch 4459, Loss 0.2648610472679138\n",
            "[Training Epoch 59] Batch 4460, Loss 0.2566215991973877\n",
            "[Training Epoch 59] Batch 4461, Loss 0.28005266189575195\n",
            "[Training Epoch 59] Batch 4462, Loss 0.3119397759437561\n",
            "[Training Epoch 59] Batch 4463, Loss 0.24119403958320618\n",
            "[Training Epoch 59] Batch 4464, Loss 0.23811016976833344\n",
            "[Training Epoch 59] Batch 4465, Loss 0.2674238979816437\n",
            "[Training Epoch 59] Batch 4466, Loss 0.26535314321517944\n",
            "[Training Epoch 59] Batch 4467, Loss 0.2549774944782257\n",
            "[Training Epoch 59] Batch 4468, Loss 0.2569894790649414\n",
            "[Training Epoch 59] Batch 4469, Loss 0.2841796875\n",
            "[Training Epoch 59] Batch 4470, Loss 0.28354182839393616\n",
            "[Training Epoch 59] Batch 4471, Loss 0.26581844687461853\n",
            "[Training Epoch 59] Batch 4472, Loss 0.23669126629829407\n",
            "[Training Epoch 59] Batch 4473, Loss 0.2738780975341797\n",
            "[Training Epoch 59] Batch 4474, Loss 0.2666216194629669\n",
            "[Training Epoch 59] Batch 4475, Loss 0.2577507197856903\n",
            "[Training Epoch 59] Batch 4476, Loss 0.23328536748886108\n",
            "[Training Epoch 59] Batch 4477, Loss 0.25049281120300293\n",
            "[Training Epoch 59] Batch 4478, Loss 0.24692851305007935\n",
            "[Training Epoch 59] Batch 4479, Loss 0.2676210105419159\n",
            "[Training Epoch 59] Batch 4480, Loss 0.24817948043346405\n",
            "[Training Epoch 59] Batch 4481, Loss 0.2829172909259796\n",
            "[Training Epoch 59] Batch 4482, Loss 0.2550768554210663\n",
            "[Training Epoch 59] Batch 4483, Loss 0.2496977597475052\n",
            "[Training Epoch 59] Batch 4484, Loss 0.24349722266197205\n",
            "[Training Epoch 59] Batch 4485, Loss 0.31776291131973267\n",
            "[Training Epoch 59] Batch 4486, Loss 0.2496679276227951\n",
            "[Training Epoch 59] Batch 4487, Loss 0.2327871322631836\n",
            "[Training Epoch 59] Batch 4488, Loss 0.2894445061683655\n",
            "[Training Epoch 59] Batch 4489, Loss 0.2631061375141144\n",
            "[Training Epoch 59] Batch 4490, Loss 0.2721411883831024\n",
            "[Training Epoch 59] Batch 4491, Loss 0.26871582865715027\n",
            "[Training Epoch 59] Batch 4492, Loss 0.2829590141773224\n",
            "[Training Epoch 59] Batch 4493, Loss 0.24654673039913177\n",
            "[Training Epoch 59] Batch 4494, Loss 0.2582789957523346\n",
            "[Training Epoch 59] Batch 4495, Loss 0.2395835965871811\n",
            "[Training Epoch 59] Batch 4496, Loss 0.24839116632938385\n",
            "[Training Epoch 59] Batch 4497, Loss 0.2584920823574066\n",
            "[Training Epoch 59] Batch 4498, Loss 0.248186394572258\n",
            "[Training Epoch 59] Batch 4499, Loss 0.22554215788841248\n",
            "[Training Epoch 59] Batch 4500, Loss 0.2939651906490326\n",
            "[Training Epoch 59] Batch 4501, Loss 0.2906370162963867\n",
            "[Training Epoch 59] Batch 4502, Loss 0.26298344135284424\n",
            "[Training Epoch 59] Batch 4503, Loss 0.28246432542800903\n",
            "[Training Epoch 59] Batch 4504, Loss 0.27426227927207947\n",
            "[Training Epoch 59] Batch 4505, Loss 0.27545446157455444\n",
            "[Training Epoch 59] Batch 4506, Loss 0.27141547203063965\n",
            "[Training Epoch 59] Batch 4507, Loss 0.2524797320365906\n",
            "[Training Epoch 59] Batch 4508, Loss 0.2596336603164673\n",
            "[Training Epoch 59] Batch 4509, Loss 0.25444191694259644\n",
            "[Training Epoch 59] Batch 4510, Loss 0.2737196683883667\n",
            "[Training Epoch 59] Batch 4511, Loss 0.2461402863264084\n",
            "[Training Epoch 59] Batch 4512, Loss 0.26365768909454346\n",
            "[Training Epoch 59] Batch 4513, Loss 0.26276329159736633\n",
            "[Training Epoch 59] Batch 4514, Loss 0.2655002772808075\n",
            "[Training Epoch 59] Batch 4515, Loss 0.24420368671417236\n",
            "[Training Epoch 59] Batch 4516, Loss 0.2445014864206314\n",
            "[Training Epoch 59] Batch 4517, Loss 0.2793607711791992\n",
            "[Training Epoch 59] Batch 4518, Loss 0.25629767775535583\n",
            "[Training Epoch 59] Batch 4519, Loss 0.25496795773506165\n",
            "[Training Epoch 59] Batch 4520, Loss 0.2755052447319031\n",
            "[Training Epoch 59] Batch 4521, Loss 0.27441516518592834\n",
            "[Training Epoch 59] Batch 4522, Loss 0.2502250671386719\n",
            "[Training Epoch 59] Batch 4523, Loss 0.2546628713607788\n",
            "[Training Epoch 59] Batch 4524, Loss 0.2750703692436218\n",
            "[Training Epoch 59] Batch 4525, Loss 0.265802800655365\n",
            "[Training Epoch 59] Batch 4526, Loss 0.2761670649051666\n",
            "[Training Epoch 59] Batch 4527, Loss 0.2595022916793823\n",
            "[Training Epoch 59] Batch 4528, Loss 0.2803482711315155\n",
            "[Training Epoch 59] Batch 4529, Loss 0.2698328197002411\n",
            "[Training Epoch 59] Batch 4530, Loss 0.24258792400360107\n",
            "[Training Epoch 59] Batch 4531, Loss 0.2996542751789093\n",
            "[Training Epoch 59] Batch 4532, Loss 0.27983617782592773\n",
            "[Training Epoch 59] Batch 4533, Loss 0.2809101343154907\n",
            "[Training Epoch 59] Batch 4534, Loss 0.2555668354034424\n",
            "[Training Epoch 59] Batch 4535, Loss 0.2701908051967621\n",
            "[Training Epoch 59] Batch 4536, Loss 0.23714594542980194\n",
            "[Training Epoch 59] Batch 4537, Loss 0.26196765899658203\n",
            "[Training Epoch 59] Batch 4538, Loss 0.2450648695230484\n",
            "[Training Epoch 59] Batch 4539, Loss 0.2842593193054199\n",
            "[Training Epoch 59] Batch 4540, Loss 0.2629989683628082\n",
            "[Training Epoch 59] Batch 4541, Loss 0.2688486576080322\n",
            "[Training Epoch 59] Batch 4542, Loss 0.3052956759929657\n",
            "[Training Epoch 59] Batch 4543, Loss 0.2499978393316269\n",
            "[Training Epoch 59] Batch 4544, Loss 0.2533717751502991\n",
            "[Training Epoch 59] Batch 4545, Loss 0.2882762849330902\n",
            "[Training Epoch 59] Batch 4546, Loss 0.23524920642375946\n",
            "[Training Epoch 59] Batch 4547, Loss 0.26521024107933044\n",
            "[Training Epoch 59] Batch 4548, Loss 0.28921109437942505\n",
            "[Training Epoch 59] Batch 4549, Loss 0.25747838616371155\n",
            "[Training Epoch 59] Batch 4550, Loss 0.27247875928878784\n",
            "[Training Epoch 59] Batch 4551, Loss 0.28786927461624146\n",
            "[Training Epoch 59] Batch 4552, Loss 0.26180800795555115\n",
            "[Training Epoch 59] Batch 4553, Loss 0.26243212819099426\n",
            "[Training Epoch 59] Batch 4554, Loss 0.2728613317012787\n",
            "[Training Epoch 59] Batch 4555, Loss 0.2040921300649643\n",
            "[Training Epoch 59] Batch 4556, Loss 0.27274441719055176\n",
            "[Training Epoch 59] Batch 4557, Loss 0.25907236337661743\n",
            "[Training Epoch 59] Batch 4558, Loss 0.2701488137245178\n",
            "[Training Epoch 59] Batch 4559, Loss 0.2692163586616516\n",
            "[Training Epoch 59] Batch 4560, Loss 0.24241507053375244\n",
            "[Training Epoch 59] Batch 4561, Loss 0.29006338119506836\n",
            "[Training Epoch 59] Batch 4562, Loss 0.2595865726470947\n",
            "[Training Epoch 59] Batch 4563, Loss 0.2839457392692566\n",
            "[Training Epoch 59] Batch 4564, Loss 0.278568297624588\n",
            "[Training Epoch 59] Batch 4565, Loss 0.26652705669403076\n",
            "[Training Epoch 59] Batch 4566, Loss 0.2860870957374573\n",
            "[Training Epoch 59] Batch 4567, Loss 0.22784581780433655\n",
            "[Training Epoch 59] Batch 4568, Loss 0.2638404667377472\n",
            "[Training Epoch 59] Batch 4569, Loss 0.24319744110107422\n",
            "[Training Epoch 59] Batch 4570, Loss 0.267450213432312\n",
            "[Training Epoch 59] Batch 4571, Loss 0.28954216837882996\n",
            "[Training Epoch 59] Batch 4572, Loss 0.2599451541900635\n",
            "[Training Epoch 59] Batch 4573, Loss 0.2685505151748657\n",
            "[Training Epoch 59] Batch 4574, Loss 0.2766890823841095\n",
            "[Training Epoch 59] Batch 4575, Loss 0.2620787024497986\n",
            "[Training Epoch 59] Batch 4576, Loss 0.2724316716194153\n",
            "[Training Epoch 59] Batch 4577, Loss 0.24457362294197083\n",
            "[Training Epoch 59] Batch 4578, Loss 0.2923842668533325\n",
            "[Training Epoch 59] Batch 4579, Loss 0.2651948928833008\n",
            "[Training Epoch 59] Batch 4580, Loss 0.25170332193374634\n",
            "[Training Epoch 59] Batch 4581, Loss 0.2615804374217987\n",
            "[Training Epoch 59] Batch 4582, Loss 0.27029114961624146\n",
            "[Training Epoch 59] Batch 4583, Loss 0.2591146230697632\n",
            "[Training Epoch 59] Batch 4584, Loss 0.2590586841106415\n",
            "[Training Epoch 59] Batch 4585, Loss 0.3057249188423157\n",
            "[Training Epoch 59] Batch 4586, Loss 0.2629739046096802\n",
            "[Training Epoch 59] Batch 4587, Loss 0.274647980928421\n",
            "[Training Epoch 59] Batch 4588, Loss 0.249966561794281\n",
            "[Training Epoch 59] Batch 4589, Loss 0.26029542088508606\n",
            "[Training Epoch 59] Batch 4590, Loss 0.2564140558242798\n",
            "[Training Epoch 59] Batch 4591, Loss 0.2853434979915619\n",
            "[Training Epoch 59] Batch 4592, Loss 0.25929465889930725\n",
            "[Training Epoch 59] Batch 4593, Loss 0.2576615810394287\n",
            "[Training Epoch 59] Batch 4594, Loss 0.24781477451324463\n",
            "[Training Epoch 59] Batch 4595, Loss 0.24551209807395935\n",
            "[Training Epoch 59] Batch 4596, Loss 0.2756451964378357\n",
            "[Training Epoch 59] Batch 4597, Loss 0.27195173501968384\n",
            "[Training Epoch 59] Batch 4598, Loss 0.24345433712005615\n",
            "[Training Epoch 59] Batch 4599, Loss 0.24404823780059814\n",
            "[Training Epoch 59] Batch 4600, Loss 0.26608043909072876\n",
            "[Training Epoch 59] Batch 4601, Loss 0.24255160987377167\n",
            "[Training Epoch 59] Batch 4602, Loss 0.24859619140625\n",
            "[Training Epoch 59] Batch 4603, Loss 0.2711886763572693\n",
            "[Training Epoch 59] Batch 4604, Loss 0.28932732343673706\n",
            "[Training Epoch 59] Batch 4605, Loss 0.28012654185295105\n",
            "[Training Epoch 59] Batch 4606, Loss 0.2593982219696045\n",
            "[Training Epoch 59] Batch 4607, Loss 0.25478124618530273\n",
            "[Training Epoch 59] Batch 4608, Loss 0.24579578638076782\n",
            "[Training Epoch 59] Batch 4609, Loss 0.25237321853637695\n",
            "[Training Epoch 59] Batch 4610, Loss 0.2562229037284851\n",
            "[Training Epoch 59] Batch 4611, Loss 0.24719923734664917\n",
            "[Training Epoch 59] Batch 4612, Loss 0.253071129322052\n",
            "[Training Epoch 59] Batch 4613, Loss 0.2630791664123535\n",
            "[Training Epoch 59] Batch 4614, Loss 0.24901556968688965\n",
            "[Training Epoch 59] Batch 4615, Loss 0.28320205211639404\n",
            "[Training Epoch 59] Batch 4616, Loss 0.29232341051101685\n",
            "[Training Epoch 59] Batch 4617, Loss 0.27740129828453064\n",
            "[Training Epoch 59] Batch 4618, Loss 0.2932905852794647\n",
            "[Training Epoch 59] Batch 4619, Loss 0.23119521141052246\n",
            "[Training Epoch 59] Batch 4620, Loss 0.25047552585601807\n",
            "[Training Epoch 59] Batch 4621, Loss 0.2484275996685028\n",
            "[Training Epoch 59] Batch 4622, Loss 0.26768767833709717\n",
            "[Training Epoch 59] Batch 4623, Loss 0.2227146327495575\n",
            "[Training Epoch 59] Batch 4624, Loss 0.2741856276988983\n",
            "[Training Epoch 59] Batch 4625, Loss 0.27912381291389465\n",
            "[Training Epoch 59] Batch 4626, Loss 0.24663612246513367\n",
            "[Training Epoch 59] Batch 4627, Loss 0.26515644788742065\n",
            "[Training Epoch 59] Batch 4628, Loss 0.2211439609527588\n",
            "[Training Epoch 59] Batch 4629, Loss 0.250902384519577\n",
            "[Training Epoch 59] Batch 4630, Loss 0.2573501169681549\n",
            "[Training Epoch 59] Batch 4631, Loss 0.27712178230285645\n",
            "[Training Epoch 59] Batch 4632, Loss 0.2835814356803894\n",
            "[Training Epoch 59] Batch 4633, Loss 0.2752307653427124\n",
            "[Training Epoch 59] Batch 4634, Loss 0.2659306228160858\n",
            "[Training Epoch 59] Batch 4635, Loss 0.26511749625205994\n",
            "[Training Epoch 59] Batch 4636, Loss 0.26281535625457764\n",
            "[Training Epoch 59] Batch 4637, Loss 0.2681155502796173\n",
            "[Training Epoch 59] Batch 4638, Loss 0.24981369078159332\n",
            "[Training Epoch 59] Batch 4639, Loss 0.24726879596710205\n",
            "[Training Epoch 59] Batch 4640, Loss 0.2797318994998932\n",
            "[Training Epoch 59] Batch 4641, Loss 0.27568578720092773\n",
            "[Training Epoch 59] Batch 4642, Loss 0.27173930406570435\n",
            "[Training Epoch 59] Batch 4643, Loss 0.23600679636001587\n",
            "[Training Epoch 59] Batch 4644, Loss 0.24261942505836487\n",
            "[Training Epoch 59] Batch 4645, Loss 0.2603134214878082\n",
            "[Training Epoch 59] Batch 4646, Loss 0.2877015471458435\n",
            "[Training Epoch 59] Batch 4647, Loss 0.2648543417453766\n",
            "[Training Epoch 59] Batch 4648, Loss 0.2571839988231659\n",
            "[Training Epoch 59] Batch 4649, Loss 0.2571163773536682\n",
            "[Training Epoch 59] Batch 4650, Loss 0.27134573459625244\n",
            "[Training Epoch 59] Batch 4651, Loss 0.25438231229782104\n",
            "[Training Epoch 59] Batch 4652, Loss 0.27792099118232727\n",
            "[Training Epoch 59] Batch 4653, Loss 0.2566954493522644\n",
            "[Training Epoch 59] Batch 4654, Loss 0.26462310552597046\n",
            "[Training Epoch 59] Batch 4655, Loss 0.289777547121048\n",
            "[Training Epoch 59] Batch 4656, Loss 0.2725130021572113\n",
            "[Training Epoch 59] Batch 4657, Loss 0.2840394079685211\n",
            "[Training Epoch 59] Batch 4658, Loss 0.2793557345867157\n",
            "[Training Epoch 59] Batch 4659, Loss 0.27083081007003784\n",
            "[Training Epoch 59] Batch 4660, Loss 0.2787795662879944\n",
            "[Training Epoch 59] Batch 4661, Loss 0.2651180624961853\n",
            "[Training Epoch 59] Batch 4662, Loss 0.2702696919441223\n",
            "[Training Epoch 59] Batch 4663, Loss 0.28194981813430786\n",
            "[Training Epoch 59] Batch 4664, Loss 0.24555549025535583\n",
            "[Training Epoch 59] Batch 4665, Loss 0.2662598490715027\n",
            "[Training Epoch 59] Batch 4666, Loss 0.25312739610671997\n",
            "[Training Epoch 59] Batch 4667, Loss 0.2906407415866852\n",
            "[Training Epoch 59] Batch 4668, Loss 0.2853041887283325\n",
            "[Training Epoch 59] Batch 4669, Loss 0.2713516354560852\n",
            "[Training Epoch 59] Batch 4670, Loss 0.2729383409023285\n",
            "[Training Epoch 59] Batch 4671, Loss 0.28267529606819153\n",
            "[Training Epoch 59] Batch 4672, Loss 0.281381756067276\n",
            "[Training Epoch 59] Batch 4673, Loss 0.2509467303752899\n",
            "[Training Epoch 59] Batch 4674, Loss 0.2826177775859833\n",
            "[Training Epoch 59] Batch 4675, Loss 0.3074658215045929\n",
            "[Training Epoch 59] Batch 4676, Loss 0.2641274034976959\n",
            "[Training Epoch 59] Batch 4677, Loss 0.26087644696235657\n",
            "[Training Epoch 59] Batch 4678, Loss 0.2737778127193451\n",
            "[Training Epoch 59] Batch 4679, Loss 0.24965953826904297\n",
            "[Training Epoch 59] Batch 4680, Loss 0.2735540568828583\n",
            "[Training Epoch 59] Batch 4681, Loss 0.2994759976863861\n",
            "[Training Epoch 59] Batch 4682, Loss 0.23227235674858093\n",
            "[Training Epoch 59] Batch 4683, Loss 0.23880837857723236\n",
            "[Training Epoch 59] Batch 4684, Loss 0.27282702922821045\n",
            "[Training Epoch 59] Batch 4685, Loss 0.2610049545764923\n",
            "[Training Epoch 59] Batch 4686, Loss 0.2501479387283325\n",
            "[Training Epoch 59] Batch 4687, Loss 0.2634424567222595\n",
            "[Training Epoch 59] Batch 4688, Loss 0.26021474599838257\n",
            "[Training Epoch 59] Batch 4689, Loss 0.2750256657600403\n",
            "[Training Epoch 59] Batch 4690, Loss 0.25830358266830444\n",
            "[Training Epoch 59] Batch 4691, Loss 0.24826407432556152\n",
            "[Training Epoch 59] Batch 4692, Loss 0.23612886667251587\n",
            "[Training Epoch 59] Batch 4693, Loss 0.2978334426879883\n",
            "[Training Epoch 59] Batch 4694, Loss 0.27304303646087646\n",
            "[Training Epoch 59] Batch 4695, Loss 0.2315017282962799\n",
            "[Training Epoch 59] Batch 4696, Loss 0.24157916009426117\n",
            "[Training Epoch 59] Batch 4697, Loss 0.24007493257522583\n",
            "[Training Epoch 59] Batch 4698, Loss 0.24455660581588745\n",
            "[Training Epoch 59] Batch 4699, Loss 0.2883562743663788\n",
            "[Training Epoch 59] Batch 4700, Loss 0.25414204597473145\n",
            "[Training Epoch 59] Batch 4701, Loss 0.24262550473213196\n",
            "[Training Epoch 59] Batch 4702, Loss 0.26449963450431824\n",
            "[Training Epoch 59] Batch 4703, Loss 0.24878355860710144\n",
            "[Training Epoch 59] Batch 4704, Loss 0.2671411335468292\n",
            "[Training Epoch 59] Batch 4705, Loss 0.26015156507492065\n",
            "[Training Epoch 59] Batch 4706, Loss 0.2622905373573303\n",
            "[Training Epoch 59] Batch 4707, Loss 0.259091317653656\n",
            "[Training Epoch 59] Batch 4708, Loss 0.27884015440940857\n",
            "[Training Epoch 59] Batch 4709, Loss 0.27912822365760803\n",
            "[Training Epoch 59] Batch 4710, Loss 0.2515413165092468\n",
            "[Training Epoch 59] Batch 4711, Loss 0.268587589263916\n",
            "[Training Epoch 59] Batch 4712, Loss 0.23094645142555237\n",
            "[Training Epoch 59] Batch 4713, Loss 0.27117857336997986\n",
            "[Training Epoch 59] Batch 4714, Loss 0.25603368878364563\n",
            "[Training Epoch 59] Batch 4715, Loss 0.27018260955810547\n",
            "[Training Epoch 59] Batch 4716, Loss 0.22948725521564484\n",
            "[Training Epoch 59] Batch 4717, Loss 0.2705494463443756\n",
            "[Training Epoch 59] Batch 4718, Loss 0.27045735716819763\n",
            "[Training Epoch 59] Batch 4719, Loss 0.2582629919052124\n",
            "[Training Epoch 59] Batch 4720, Loss 0.29072293639183044\n",
            "[Training Epoch 59] Batch 4721, Loss 0.28198638558387756\n",
            "[Training Epoch 59] Batch 4722, Loss 0.2574463486671448\n",
            "[Training Epoch 59] Batch 4723, Loss 0.30650031566619873\n",
            "[Training Epoch 59] Batch 4724, Loss 0.27964097261428833\n",
            "[Training Epoch 59] Batch 4725, Loss 0.27453088760375977\n",
            "[Training Epoch 59] Batch 4726, Loss 0.29182708263397217\n",
            "[Training Epoch 59] Batch 4727, Loss 0.2600586414337158\n",
            "[Training Epoch 59] Batch 4728, Loss 0.25970375537872314\n",
            "[Training Epoch 59] Batch 4729, Loss 0.26102057099342346\n",
            "[Training Epoch 59] Batch 4730, Loss 0.280139684677124\n",
            "[Training Epoch 59] Batch 4731, Loss 0.25397050380706787\n",
            "[Training Epoch 59] Batch 4732, Loss 0.2561899423599243\n",
            "[Training Epoch 59] Batch 4733, Loss 0.27700936794281006\n",
            "[Training Epoch 59] Batch 4734, Loss 0.2960374355316162\n",
            "[Training Epoch 59] Batch 4735, Loss 0.25279903411865234\n",
            "[Training Epoch 59] Batch 4736, Loss 0.2618822455406189\n",
            "[Training Epoch 59] Batch 4737, Loss 0.2601768374443054\n",
            "[Training Epoch 59] Batch 4738, Loss 0.24845381081104279\n",
            "[Training Epoch 59] Batch 4739, Loss 0.2566903531551361\n",
            "[Training Epoch 59] Batch 4740, Loss 0.25672999024391174\n",
            "[Training Epoch 59] Batch 4741, Loss 0.2644326090812683\n",
            "[Training Epoch 59] Batch 4742, Loss 0.26824209094047546\n",
            "[Training Epoch 59] Batch 4743, Loss 0.28992024064064026\n",
            "[Training Epoch 59] Batch 4744, Loss 0.2653205692768097\n",
            "[Training Epoch 59] Batch 4745, Loss 0.2618693709373474\n",
            "[Training Epoch 59] Batch 4746, Loss 0.29453393816947937\n",
            "[Training Epoch 59] Batch 4747, Loss 0.26110097765922546\n",
            "[Training Epoch 59] Batch 4748, Loss 0.266899049282074\n",
            "[Training Epoch 59] Batch 4749, Loss 0.27828100323677063\n",
            "[Training Epoch 59] Batch 4750, Loss 0.2806754410266876\n",
            "[Training Epoch 59] Batch 4751, Loss 0.2277640849351883\n",
            "[Training Epoch 59] Batch 4752, Loss 0.24534666538238525\n",
            "[Training Epoch 59] Batch 4753, Loss 0.27373936772346497\n",
            "[Training Epoch 59] Batch 4754, Loss 0.23242220282554626\n",
            "[Training Epoch 59] Batch 4755, Loss 0.2806844115257263\n",
            "[Training Epoch 59] Batch 4756, Loss 0.2480885237455368\n",
            "[Training Epoch 59] Batch 4757, Loss 0.28656071424484253\n",
            "[Training Epoch 59] Batch 4758, Loss 0.28162384033203125\n",
            "[Training Epoch 59] Batch 4759, Loss 0.2606854736804962\n",
            "[Training Epoch 59] Batch 4760, Loss 0.26320892572402954\n",
            "[Training Epoch 59] Batch 4761, Loss 0.2592298984527588\n",
            "[Training Epoch 59] Batch 4762, Loss 0.24915838241577148\n",
            "[Training Epoch 59] Batch 4763, Loss 0.2725616991519928\n",
            "[Training Epoch 59] Batch 4764, Loss 0.2900843024253845\n",
            "[Training Epoch 59] Batch 4765, Loss 0.2677590548992157\n",
            "[Training Epoch 59] Batch 4766, Loss 0.27696117758750916\n",
            "[Training Epoch 59] Batch 4767, Loss 0.281054824590683\n",
            "[Training Epoch 59] Batch 4768, Loss 0.2427811324596405\n",
            "[Training Epoch 59] Batch 4769, Loss 0.27168458700180054\n",
            "[Training Epoch 59] Batch 4770, Loss 0.26369768381118774\n",
            "[Training Epoch 59] Batch 4771, Loss 0.2728727161884308\n",
            "[Training Epoch 59] Batch 4772, Loss 0.2684921324253082\n",
            "[Training Epoch 59] Batch 4773, Loss 0.2615840435028076\n",
            "[Training Epoch 59] Batch 4774, Loss 0.29026132822036743\n",
            "[Training Epoch 59] Batch 4775, Loss 0.26532724499702454\n",
            "[Training Epoch 59] Batch 4776, Loss 0.29747194051742554\n",
            "[Training Epoch 59] Batch 4777, Loss 0.2682075798511505\n",
            "[Training Epoch 59] Batch 4778, Loss 0.25222593545913696\n",
            "[Training Epoch 59] Batch 4779, Loss 0.25536495447158813\n",
            "[Training Epoch 59] Batch 4780, Loss 0.25717082619667053\n",
            "[Training Epoch 59] Batch 4781, Loss 0.2623460292816162\n",
            "[Training Epoch 59] Batch 4782, Loss 0.2846660614013672\n",
            "[Training Epoch 59] Batch 4783, Loss 0.27981072664260864\n",
            "[Training Epoch 59] Batch 4784, Loss 0.25273898243904114\n",
            "[Training Epoch 59] Batch 4785, Loss 0.2731829285621643\n",
            "[Training Epoch 59] Batch 4786, Loss 0.28408190608024597\n",
            "[Training Epoch 59] Batch 4787, Loss 0.26484477519989014\n",
            "[Training Epoch 59] Batch 4788, Loss 0.26292648911476135\n",
            "[Training Epoch 59] Batch 4789, Loss 0.2761968672275543\n",
            "[Training Epoch 59] Batch 4790, Loss 0.2521772086620331\n",
            "[Training Epoch 59] Batch 4791, Loss 0.2829831838607788\n",
            "[Training Epoch 59] Batch 4792, Loss 0.23743431270122528\n",
            "[Training Epoch 59] Batch 4793, Loss 0.2511349320411682\n",
            "[Training Epoch 59] Batch 4794, Loss 0.26866209506988525\n",
            "[Training Epoch 59] Batch 4795, Loss 0.24709606170654297\n",
            "[Training Epoch 59] Batch 4796, Loss 0.2868318259716034\n",
            "[Training Epoch 59] Batch 4797, Loss 0.28400638699531555\n",
            "[Training Epoch 59] Batch 4798, Loss 0.24857035279273987\n",
            "[Training Epoch 59] Batch 4799, Loss 0.256673663854599\n",
            "[Training Epoch 59] Batch 4800, Loss 0.276959091424942\n",
            "[Training Epoch 59] Batch 4801, Loss 0.26127317547798157\n",
            "[Training Epoch 59] Batch 4802, Loss 0.25679919123649597\n",
            "[Training Epoch 59] Batch 4803, Loss 0.3359558880329132\n",
            "[Training Epoch 59] Batch 4804, Loss 0.2916942238807678\n",
            "[Training Epoch 59] Batch 4805, Loss 0.22758205235004425\n",
            "[Training Epoch 59] Batch 4806, Loss 0.2707103490829468\n",
            "[Training Epoch 59] Batch 4807, Loss 0.2341194897890091\n",
            "[Training Epoch 59] Batch 4808, Loss 0.2548738121986389\n",
            "[Training Epoch 59] Batch 4809, Loss 0.273715078830719\n",
            "[Training Epoch 59] Batch 4810, Loss 0.24696208536624908\n",
            "[Training Epoch 59] Batch 4811, Loss 0.25138092041015625\n",
            "[Training Epoch 59] Batch 4812, Loss 0.2603380084037781\n",
            "[Training Epoch 59] Batch 4813, Loss 0.28689271211624146\n",
            "[Training Epoch 59] Batch 4814, Loss 0.2551387548446655\n",
            "[Training Epoch 59] Batch 4815, Loss 0.27936404943466187\n",
            "[Training Epoch 59] Batch 4816, Loss 0.28009355068206787\n",
            "[Training Epoch 59] Batch 4817, Loss 0.24991503357887268\n",
            "[Training Epoch 59] Batch 4818, Loss 0.2537761628627777\n",
            "[Training Epoch 59] Batch 4819, Loss 0.2896301746368408\n",
            "[Training Epoch 59] Batch 4820, Loss 0.2707878649234772\n",
            "[Training Epoch 59] Batch 4821, Loss 0.25938642024993896\n",
            "[Training Epoch 59] Batch 4822, Loss 0.24120308458805084\n",
            "[Training Epoch 59] Batch 4823, Loss 0.2843014597892761\n",
            "[Training Epoch 59] Batch 4824, Loss 0.26923051476478577\n",
            "[Training Epoch 59] Batch 4825, Loss 0.27029046416282654\n",
            "[Training Epoch 59] Batch 4826, Loss 0.2563956677913666\n",
            "[Training Epoch 59] Batch 4827, Loss 0.2610893249511719\n",
            "[Training Epoch 59] Batch 4828, Loss 0.27111297845840454\n",
            "[Training Epoch 59] Batch 4829, Loss 0.25862759351730347\n",
            "[Training Epoch 59] Batch 4830, Loss 0.25585615634918213\n",
            "[Training Epoch 59] Batch 4831, Loss 0.24876047670841217\n",
            "[Training Epoch 59] Batch 4832, Loss 0.2738695740699768\n",
            "[Training Epoch 59] Batch 4833, Loss 0.2368684560060501\n",
            "[Training Epoch 59] Batch 4834, Loss 0.27681294083595276\n",
            "[Training Epoch 59] Batch 4835, Loss 0.27481749653816223\n",
            "[Training Epoch 59] Batch 4836, Loss 0.2779095768928528\n",
            "[Training Epoch 59] Batch 4837, Loss 0.25270262360572815\n",
            "[Training Epoch 59] Batch 4838, Loss 0.28307414054870605\n",
            "[Training Epoch 59] Batch 4839, Loss 0.27835026383399963\n",
            "[Training Epoch 59] Batch 4840, Loss 0.24785850942134857\n",
            "[Training Epoch 59] Batch 4841, Loss 0.2620270252227783\n",
            "[Training Epoch 59] Batch 4842, Loss 0.2731379270553589\n",
            "[Training Epoch 59] Batch 4843, Loss 0.2754518687725067\n",
            "[Training Epoch 59] Batch 4844, Loss 0.27459967136383057\n",
            "[Training Epoch 59] Batch 4845, Loss 0.28097158670425415\n",
            "[Training Epoch 59] Batch 4846, Loss 0.28178584575653076\n",
            "[Training Epoch 59] Batch 4847, Loss 0.25718748569488525\n",
            "[Training Epoch 59] Batch 4848, Loss 0.2743948698043823\n",
            "[Training Epoch 59] Batch 4849, Loss 0.2725153863430023\n",
            "[Training Epoch 59] Batch 4850, Loss 0.2698928713798523\n",
            "[Training Epoch 59] Batch 4851, Loss 0.28227147459983826\n",
            "[Training Epoch 59] Batch 4852, Loss 0.2591009736061096\n",
            "[Training Epoch 59] Batch 4853, Loss 0.27535244822502136\n",
            "[Training Epoch 59] Batch 4854, Loss 0.25234660506248474\n",
            "/content/neural-collaborative-filtering/src/metrics.py:56: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n",
            "[Evluating Epoch 59] HR = 0.6454, NDCG = 0.3710\n",
            "Epoch 60 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 60] Batch 0, Loss 0.28155967593193054\n",
            "[Training Epoch 60] Batch 1, Loss 0.24662889540195465\n",
            "[Training Epoch 60] Batch 2, Loss 0.2358129769563675\n",
            "[Training Epoch 60] Batch 3, Loss 0.26745954155921936\n",
            "[Training Epoch 60] Batch 4, Loss 0.2617338001728058\n",
            "[Training Epoch 60] Batch 5, Loss 0.2705056369304657\n",
            "[Training Epoch 60] Batch 6, Loss 0.24532249569892883\n",
            "[Training Epoch 60] Batch 7, Loss 0.24868233501911163\n",
            "[Training Epoch 60] Batch 8, Loss 0.2346941977739334\n",
            "[Training Epoch 60] Batch 9, Loss 0.26187825202941895\n",
            "[Training Epoch 60] Batch 10, Loss 0.25529927015304565\n",
            "[Training Epoch 60] Batch 11, Loss 0.274005651473999\n",
            "[Training Epoch 60] Batch 12, Loss 0.2534720003604889\n",
            "[Training Epoch 60] Batch 13, Loss 0.26888519525527954\n",
            "[Training Epoch 60] Batch 14, Loss 0.3182179033756256\n",
            "[Training Epoch 60] Batch 15, Loss 0.2615690231323242\n",
            "[Training Epoch 60] Batch 16, Loss 0.2541455030441284\n",
            "[Training Epoch 60] Batch 17, Loss 0.2878854274749756\n",
            "[Training Epoch 60] Batch 18, Loss 0.26487016677856445\n",
            "[Training Epoch 60] Batch 19, Loss 0.2795828580856323\n",
            "[Training Epoch 60] Batch 20, Loss 0.25389543175697327\n",
            "[Training Epoch 60] Batch 21, Loss 0.261181116104126\n",
            "[Training Epoch 60] Batch 22, Loss 0.2780888080596924\n",
            "[Training Epoch 60] Batch 23, Loss 0.2781132757663727\n",
            "[Training Epoch 60] Batch 24, Loss 0.24517978727817535\n",
            "[Training Epoch 60] Batch 25, Loss 0.2501491904258728\n",
            "[Training Epoch 60] Batch 26, Loss 0.26402485370635986\n",
            "[Training Epoch 60] Batch 27, Loss 0.26790982484817505\n",
            "[Training Epoch 60] Batch 28, Loss 0.2613413631916046\n",
            "[Training Epoch 60] Batch 29, Loss 0.24585770070552826\n",
            "[Training Epoch 60] Batch 30, Loss 0.27302286028862\n",
            "[Training Epoch 60] Batch 31, Loss 0.26673439145088196\n",
            "[Training Epoch 60] Batch 32, Loss 0.26247701048851013\n",
            "[Training Epoch 60] Batch 33, Loss 0.2592875361442566\n",
            "[Training Epoch 60] Batch 34, Loss 0.2647053897380829\n",
            "[Training Epoch 60] Batch 35, Loss 0.24414771795272827\n",
            "[Training Epoch 60] Batch 36, Loss 0.2750919461250305\n",
            "[Training Epoch 60] Batch 37, Loss 0.2752821743488312\n",
            "[Training Epoch 60] Batch 38, Loss 0.26853227615356445\n",
            "[Training Epoch 60] Batch 39, Loss 0.2542495131492615\n",
            "[Training Epoch 60] Batch 40, Loss 0.25099411606788635\n",
            "[Training Epoch 60] Batch 41, Loss 0.2507059574127197\n",
            "[Training Epoch 60] Batch 42, Loss 0.261581689119339\n",
            "[Training Epoch 60] Batch 43, Loss 0.24240408837795258\n",
            "[Training Epoch 60] Batch 44, Loss 0.27676525712013245\n",
            "[Training Epoch 60] Batch 45, Loss 0.25815439224243164\n",
            "[Training Epoch 60] Batch 46, Loss 0.27437180280685425\n",
            "[Training Epoch 60] Batch 47, Loss 0.2624501585960388\n",
            "[Training Epoch 60] Batch 48, Loss 0.2471860945224762\n",
            "[Training Epoch 60] Batch 49, Loss 0.26660555601119995\n",
            "[Training Epoch 60] Batch 50, Loss 0.29034990072250366\n",
            "[Training Epoch 60] Batch 51, Loss 0.26227879524230957\n",
            "[Training Epoch 60] Batch 52, Loss 0.27439773082733154\n",
            "[Training Epoch 60] Batch 53, Loss 0.29407086968421936\n",
            "[Training Epoch 60] Batch 54, Loss 0.31167638301849365\n",
            "[Training Epoch 60] Batch 55, Loss 0.265459805727005\n",
            "[Training Epoch 60] Batch 56, Loss 0.24850064516067505\n",
            "[Training Epoch 60] Batch 57, Loss 0.25895121693611145\n",
            "[Training Epoch 60] Batch 58, Loss 0.26958340406417847\n",
            "[Training Epoch 60] Batch 59, Loss 0.30011531710624695\n",
            "[Training Epoch 60] Batch 60, Loss 0.28798502683639526\n",
            "[Training Epoch 60] Batch 61, Loss 0.3117872178554535\n",
            "[Training Epoch 60] Batch 62, Loss 0.25813084840774536\n",
            "[Training Epoch 60] Batch 63, Loss 0.29232800006866455\n",
            "[Training Epoch 60] Batch 64, Loss 0.23632803559303284\n",
            "[Training Epoch 60] Batch 65, Loss 0.234279602766037\n",
            "[Training Epoch 60] Batch 66, Loss 0.24022656679153442\n",
            "[Training Epoch 60] Batch 67, Loss 0.24575595557689667\n",
            "[Training Epoch 60] Batch 68, Loss 0.24880863726139069\n",
            "[Training Epoch 60] Batch 69, Loss 0.23789918422698975\n",
            "[Training Epoch 60] Batch 70, Loss 0.273593932390213\n",
            "[Training Epoch 60] Batch 71, Loss 0.25211095809936523\n",
            "[Training Epoch 60] Batch 72, Loss 0.2398935854434967\n",
            "[Training Epoch 60] Batch 73, Loss 0.2666724920272827\n",
            "[Training Epoch 60] Batch 74, Loss 0.2754724323749542\n",
            "[Training Epoch 60] Batch 75, Loss 0.24592213332653046\n",
            "[Training Epoch 60] Batch 76, Loss 0.2780633568763733\n",
            "[Training Epoch 60] Batch 77, Loss 0.2519926428794861\n",
            "[Training Epoch 60] Batch 78, Loss 0.2617241442203522\n",
            "[Training Epoch 60] Batch 79, Loss 0.2649223804473877\n",
            "[Training Epoch 60] Batch 80, Loss 0.24608787894248962\n",
            "[Training Epoch 60] Batch 81, Loss 0.25922444462776184\n",
            "[Training Epoch 60] Batch 82, Loss 0.2775092124938965\n",
            "[Training Epoch 60] Batch 83, Loss 0.2624177634716034\n",
            "[Training Epoch 60] Batch 84, Loss 0.26943206787109375\n",
            "[Training Epoch 60] Batch 85, Loss 0.24119770526885986\n",
            "[Training Epoch 60] Batch 86, Loss 0.23664632439613342\n",
            "[Training Epoch 60] Batch 87, Loss 0.28911763429641724\n",
            "[Training Epoch 60] Batch 88, Loss 0.2839840054512024\n",
            "[Training Epoch 60] Batch 89, Loss 0.2517620623111725\n",
            "[Training Epoch 60] Batch 90, Loss 0.2699631154537201\n",
            "[Training Epoch 60] Batch 91, Loss 0.2680652141571045\n",
            "[Training Epoch 60] Batch 92, Loss 0.26356232166290283\n",
            "[Training Epoch 60] Batch 93, Loss 0.2756970524787903\n",
            "[Training Epoch 60] Batch 94, Loss 0.23584073781967163\n",
            "[Training Epoch 60] Batch 95, Loss 0.2571292221546173\n",
            "[Training Epoch 60] Batch 96, Loss 0.2591266632080078\n",
            "[Training Epoch 60] Batch 97, Loss 0.2394254058599472\n",
            "[Training Epoch 60] Batch 98, Loss 0.271566778421402\n",
            "[Training Epoch 60] Batch 99, Loss 0.27764031291007996\n",
            "[Training Epoch 60] Batch 100, Loss 0.2964048683643341\n",
            "[Training Epoch 60] Batch 101, Loss 0.27685102820396423\n",
            "[Training Epoch 60] Batch 102, Loss 0.27647489309310913\n",
            "[Training Epoch 60] Batch 103, Loss 0.24030913412570953\n",
            "[Training Epoch 60] Batch 104, Loss 0.2579819858074188\n",
            "[Training Epoch 60] Batch 105, Loss 0.24265283346176147\n",
            "[Training Epoch 60] Batch 106, Loss 0.24539145827293396\n",
            "[Training Epoch 60] Batch 107, Loss 0.30158525705337524\n",
            "[Training Epoch 60] Batch 108, Loss 0.25813090801239014\n",
            "[Training Epoch 60] Batch 109, Loss 0.29565808176994324\n",
            "[Training Epoch 60] Batch 110, Loss 0.2626989483833313\n",
            "[Training Epoch 60] Batch 111, Loss 0.2831244170665741\n",
            "[Training Epoch 60] Batch 112, Loss 0.25149211287498474\n",
            "[Training Epoch 60] Batch 113, Loss 0.2624044418334961\n",
            "[Training Epoch 60] Batch 114, Loss 0.28742045164108276\n",
            "[Training Epoch 60] Batch 115, Loss 0.28698262572288513\n",
            "[Training Epoch 60] Batch 116, Loss 0.28028950095176697\n",
            "[Training Epoch 60] Batch 117, Loss 0.25994330644607544\n",
            "[Training Epoch 60] Batch 118, Loss 0.28010591864585876\n",
            "[Training Epoch 60] Batch 119, Loss 0.24473178386688232\n",
            "[Training Epoch 60] Batch 120, Loss 0.23863235116004944\n",
            "[Training Epoch 60] Batch 121, Loss 0.2747287154197693\n",
            "[Training Epoch 60] Batch 122, Loss 0.24955187737941742\n",
            "[Training Epoch 60] Batch 123, Loss 0.26502925157546997\n",
            "[Training Epoch 60] Batch 124, Loss 0.23265376687049866\n",
            "[Training Epoch 60] Batch 125, Loss 0.28944694995880127\n",
            "[Training Epoch 60] Batch 126, Loss 0.2475692480802536\n",
            "[Training Epoch 60] Batch 127, Loss 0.2746126651763916\n",
            "[Training Epoch 60] Batch 128, Loss 0.26407676935195923\n",
            "[Training Epoch 60] Batch 129, Loss 0.24423925578594208\n",
            "[Training Epoch 60] Batch 130, Loss 0.3029753267765045\n",
            "[Training Epoch 60] Batch 131, Loss 0.2694157361984253\n",
            "[Training Epoch 60] Batch 132, Loss 0.2627837657928467\n",
            "[Training Epoch 60] Batch 133, Loss 0.27083346247673035\n",
            "[Training Epoch 60] Batch 134, Loss 0.24484559893608093\n",
            "[Training Epoch 60] Batch 135, Loss 0.2657870054244995\n",
            "[Training Epoch 60] Batch 136, Loss 0.25346529483795166\n",
            "[Training Epoch 60] Batch 137, Loss 0.2827380299568176\n",
            "[Training Epoch 60] Batch 138, Loss 0.2517054080963135\n",
            "[Training Epoch 60] Batch 139, Loss 0.25808534026145935\n",
            "[Training Epoch 60] Batch 140, Loss 0.24462443590164185\n",
            "[Training Epoch 60] Batch 141, Loss 0.25209352374076843\n",
            "[Training Epoch 60] Batch 142, Loss 0.2961195707321167\n",
            "[Training Epoch 60] Batch 143, Loss 0.27401548624038696\n",
            "[Training Epoch 60] Batch 144, Loss 0.2619017958641052\n",
            "[Training Epoch 60] Batch 145, Loss 0.25920432806015015\n",
            "[Training Epoch 60] Batch 146, Loss 0.25449174642562866\n",
            "[Training Epoch 60] Batch 147, Loss 0.27479153871536255\n",
            "[Training Epoch 60] Batch 148, Loss 0.2450634241104126\n",
            "[Training Epoch 60] Batch 149, Loss 0.255586713552475\n",
            "[Training Epoch 60] Batch 150, Loss 0.2834232449531555\n",
            "[Training Epoch 60] Batch 151, Loss 0.26047831773757935\n",
            "[Training Epoch 60] Batch 152, Loss 0.26491832733154297\n",
            "[Training Epoch 60] Batch 153, Loss 0.2614414691925049\n",
            "[Training Epoch 60] Batch 154, Loss 0.24169188737869263\n",
            "[Training Epoch 60] Batch 155, Loss 0.2574874460697174\n",
            "[Training Epoch 60] Batch 156, Loss 0.2746952474117279\n",
            "[Training Epoch 60] Batch 157, Loss 0.2514050006866455\n",
            "[Training Epoch 60] Batch 158, Loss 0.2573046088218689\n",
            "[Training Epoch 60] Batch 159, Loss 0.2406022995710373\n",
            "[Training Epoch 60] Batch 160, Loss 0.27510330080986023\n",
            "[Training Epoch 60] Batch 161, Loss 0.2636715769767761\n",
            "[Training Epoch 60] Batch 162, Loss 0.30253636837005615\n",
            "[Training Epoch 60] Batch 163, Loss 0.264570951461792\n",
            "[Training Epoch 60] Batch 164, Loss 0.2967248260974884\n",
            "[Training Epoch 60] Batch 165, Loss 0.27278393507003784\n",
            "[Training Epoch 60] Batch 166, Loss 0.28623849153518677\n",
            "[Training Epoch 60] Batch 167, Loss 0.28165024518966675\n",
            "[Training Epoch 60] Batch 168, Loss 0.27210140228271484\n",
            "[Training Epoch 60] Batch 169, Loss 0.2265734076499939\n",
            "[Training Epoch 60] Batch 170, Loss 0.29196131229400635\n",
            "[Training Epoch 60] Batch 171, Loss 0.2575823664665222\n",
            "[Training Epoch 60] Batch 172, Loss 0.2681639492511749\n",
            "[Training Epoch 60] Batch 173, Loss 0.26126551628112793\n",
            "[Training Epoch 60] Batch 174, Loss 0.24956268072128296\n",
            "[Training Epoch 60] Batch 175, Loss 0.28598281741142273\n",
            "[Training Epoch 60] Batch 176, Loss 0.27072811126708984\n",
            "[Training Epoch 60] Batch 177, Loss 0.2629960775375366\n",
            "[Training Epoch 60] Batch 178, Loss 0.25819486379623413\n",
            "[Training Epoch 60] Batch 179, Loss 0.2372855842113495\n",
            "[Training Epoch 60] Batch 180, Loss 0.2635645866394043\n",
            "[Training Epoch 60] Batch 181, Loss 0.2937365174293518\n",
            "[Training Epoch 60] Batch 182, Loss 0.2576172947883606\n",
            "[Training Epoch 60] Batch 183, Loss 0.2591894268989563\n",
            "[Training Epoch 60] Batch 184, Loss 0.2653689682483673\n",
            "[Training Epoch 60] Batch 185, Loss 0.26009711623191833\n",
            "[Training Epoch 60] Batch 186, Loss 0.29132458567619324\n",
            "[Training Epoch 60] Batch 187, Loss 0.262668639421463\n",
            "[Training Epoch 60] Batch 188, Loss 0.28113457560539246\n",
            "[Training Epoch 60] Batch 189, Loss 0.2674424946308136\n",
            "[Training Epoch 60] Batch 190, Loss 0.26012423634529114\n",
            "[Training Epoch 60] Batch 191, Loss 0.2648645341396332\n",
            "[Training Epoch 60] Batch 192, Loss 0.253549724817276\n",
            "[Training Epoch 60] Batch 193, Loss 0.2227558195590973\n",
            "[Training Epoch 60] Batch 194, Loss 0.24700868129730225\n",
            "[Training Epoch 60] Batch 195, Loss 0.26008981466293335\n",
            "[Training Epoch 60] Batch 196, Loss 0.26672878861427307\n",
            "[Training Epoch 60] Batch 197, Loss 0.2657220959663391\n",
            "[Training Epoch 60] Batch 198, Loss 0.2509036064147949\n",
            "[Training Epoch 60] Batch 199, Loss 0.24331168830394745\n",
            "[Training Epoch 60] Batch 200, Loss 0.2491791546344757\n",
            "[Training Epoch 60] Batch 201, Loss 0.2609860897064209\n",
            "[Training Epoch 60] Batch 202, Loss 0.27479231357574463\n",
            "[Training Epoch 60] Batch 203, Loss 0.286710262298584\n",
            "[Training Epoch 60] Batch 204, Loss 0.25805437564849854\n",
            "[Training Epoch 60] Batch 205, Loss 0.2926383912563324\n",
            "[Training Epoch 60] Batch 206, Loss 0.26629847288131714\n",
            "[Training Epoch 60] Batch 207, Loss 0.2592097222805023\n",
            "[Training Epoch 60] Batch 208, Loss 0.24530433118343353\n",
            "[Training Epoch 60] Batch 209, Loss 0.2691698968410492\n",
            "[Training Epoch 60] Batch 210, Loss 0.2512897253036499\n",
            "[Training Epoch 60] Batch 211, Loss 0.28230607509613037\n",
            "[Training Epoch 60] Batch 212, Loss 0.27868467569351196\n",
            "[Training Epoch 60] Batch 213, Loss 0.28001394867897034\n",
            "[Training Epoch 60] Batch 214, Loss 0.2721833884716034\n",
            "[Training Epoch 60] Batch 215, Loss 0.2729560434818268\n",
            "[Training Epoch 60] Batch 216, Loss 0.25417760014533997\n",
            "[Training Epoch 60] Batch 217, Loss 0.2707447409629822\n",
            "[Training Epoch 60] Batch 218, Loss 0.2777329683303833\n",
            "[Training Epoch 60] Batch 219, Loss 0.2812425494194031\n",
            "[Training Epoch 60] Batch 220, Loss 0.25077834725379944\n",
            "[Training Epoch 60] Batch 221, Loss 0.26439157128334045\n",
            "[Training Epoch 60] Batch 222, Loss 0.28214094042778015\n",
            "[Training Epoch 60] Batch 223, Loss 0.29220402240753174\n",
            "[Training Epoch 60] Batch 224, Loss 0.23828426003456116\n",
            "[Training Epoch 60] Batch 225, Loss 0.27552226185798645\n",
            "[Training Epoch 60] Batch 226, Loss 0.23611465096473694\n",
            "[Training Epoch 60] Batch 227, Loss 0.2452925741672516\n",
            "[Training Epoch 60] Batch 228, Loss 0.25854426622390747\n",
            "[Training Epoch 60] Batch 229, Loss 0.24011200666427612\n",
            "[Training Epoch 60] Batch 230, Loss 0.26924580335617065\n",
            "[Training Epoch 60] Batch 231, Loss 0.2868416905403137\n",
            "[Training Epoch 60] Batch 232, Loss 0.2729509174823761\n",
            "[Training Epoch 60] Batch 233, Loss 0.268296480178833\n",
            "[Training Epoch 60] Batch 234, Loss 0.2529562711715698\n",
            "[Training Epoch 60] Batch 235, Loss 0.24255573749542236\n",
            "[Training Epoch 60] Batch 236, Loss 0.24438150227069855\n",
            "[Training Epoch 60] Batch 237, Loss 0.2846285104751587\n",
            "[Training Epoch 60] Batch 238, Loss 0.29940563440322876\n",
            "[Training Epoch 60] Batch 239, Loss 0.2906009554862976\n",
            "[Training Epoch 60] Batch 240, Loss 0.3024468421936035\n",
            "[Training Epoch 60] Batch 241, Loss 0.25913000106811523\n",
            "[Training Epoch 60] Batch 242, Loss 0.22785109281539917\n",
            "[Training Epoch 60] Batch 243, Loss 0.2601234018802643\n",
            "[Training Epoch 60] Batch 244, Loss 0.23777896165847778\n",
            "[Training Epoch 60] Batch 245, Loss 0.2655431628227234\n",
            "[Training Epoch 60] Batch 246, Loss 0.26981720328330994\n",
            "[Training Epoch 60] Batch 247, Loss 0.27143481373786926\n",
            "[Training Epoch 60] Batch 248, Loss 0.24677345156669617\n",
            "[Training Epoch 60] Batch 249, Loss 0.2533426582813263\n",
            "[Training Epoch 60] Batch 250, Loss 0.23658891022205353\n",
            "[Training Epoch 60] Batch 251, Loss 0.23769748210906982\n",
            "[Training Epoch 60] Batch 252, Loss 0.2680326998233795\n",
            "[Training Epoch 60] Batch 253, Loss 0.27715539932250977\n",
            "[Training Epoch 60] Batch 254, Loss 0.23336806893348694\n",
            "[Training Epoch 60] Batch 255, Loss 0.25120702385902405\n",
            "[Training Epoch 60] Batch 256, Loss 0.2667967975139618\n",
            "[Training Epoch 60] Batch 257, Loss 0.21907570958137512\n",
            "[Training Epoch 60] Batch 258, Loss 0.26628199219703674\n",
            "[Training Epoch 60] Batch 259, Loss 0.301935076713562\n",
            "[Training Epoch 60] Batch 260, Loss 0.2539610266685486\n",
            "[Training Epoch 60] Batch 261, Loss 0.3015879988670349\n",
            "[Training Epoch 60] Batch 262, Loss 0.25189539790153503\n",
            "[Training Epoch 60] Batch 263, Loss 0.2467268407344818\n",
            "[Training Epoch 60] Batch 264, Loss 0.26065564155578613\n",
            "[Training Epoch 60] Batch 265, Loss 0.25791001319885254\n",
            "[Training Epoch 60] Batch 266, Loss 0.271791934967041\n",
            "[Training Epoch 60] Batch 267, Loss 0.2479625940322876\n",
            "[Training Epoch 60] Batch 268, Loss 0.24796202778816223\n",
            "[Training Epoch 60] Batch 269, Loss 0.28615593910217285\n",
            "[Training Epoch 60] Batch 270, Loss 0.277421236038208\n",
            "[Training Epoch 60] Batch 271, Loss 0.23882220685482025\n",
            "[Training Epoch 60] Batch 272, Loss 0.26381915807724\n",
            "[Training Epoch 60] Batch 273, Loss 0.2546135485172272\n",
            "[Training Epoch 60] Batch 274, Loss 0.24623532593250275\n",
            "[Training Epoch 60] Batch 275, Loss 0.23539626598358154\n",
            "[Training Epoch 60] Batch 276, Loss 0.26795274019241333\n",
            "[Training Epoch 60] Batch 277, Loss 0.2732281982898712\n",
            "[Training Epoch 60] Batch 278, Loss 0.27054426074028015\n",
            "[Training Epoch 60] Batch 279, Loss 0.2674608826637268\n",
            "[Training Epoch 60] Batch 280, Loss 0.22420580685138702\n",
            "[Training Epoch 60] Batch 281, Loss 0.25583770871162415\n",
            "[Training Epoch 60] Batch 282, Loss 0.2527060806751251\n",
            "[Training Epoch 60] Batch 283, Loss 0.2611635625362396\n",
            "[Training Epoch 60] Batch 284, Loss 0.26713085174560547\n",
            "[Training Epoch 60] Batch 285, Loss 0.2552545666694641\n",
            "[Training Epoch 60] Batch 286, Loss 0.22706493735313416\n",
            "[Training Epoch 60] Batch 287, Loss 0.2924778461456299\n",
            "[Training Epoch 60] Batch 288, Loss 0.26212266087532043\n",
            "[Training Epoch 60] Batch 289, Loss 0.2501947581768036\n",
            "[Training Epoch 60] Batch 290, Loss 0.2584185004234314\n",
            "[Training Epoch 60] Batch 291, Loss 0.2710995376110077\n",
            "[Training Epoch 60] Batch 292, Loss 0.28600919246673584\n",
            "[Training Epoch 60] Batch 293, Loss 0.2744525671005249\n",
            "[Training Epoch 60] Batch 294, Loss 0.2686012089252472\n",
            "[Training Epoch 60] Batch 295, Loss 0.2664177119731903\n",
            "[Training Epoch 60] Batch 296, Loss 0.2647182047367096\n",
            "[Training Epoch 60] Batch 297, Loss 0.2794741988182068\n",
            "[Training Epoch 60] Batch 298, Loss 0.24417181313037872\n",
            "[Training Epoch 60] Batch 299, Loss 0.27811524271965027\n",
            "[Training Epoch 60] Batch 300, Loss 0.2507059574127197\n",
            "[Training Epoch 60] Batch 301, Loss 0.26711899042129517\n",
            "[Training Epoch 60] Batch 302, Loss 0.25064730644226074\n",
            "[Training Epoch 60] Batch 303, Loss 0.30225294828414917\n",
            "[Training Epoch 60] Batch 304, Loss 0.29926782846450806\n",
            "[Training Epoch 60] Batch 305, Loss 0.28930699825286865\n",
            "[Training Epoch 60] Batch 306, Loss 0.2504098415374756\n",
            "[Training Epoch 60] Batch 307, Loss 0.2945440113544464\n",
            "[Training Epoch 60] Batch 308, Loss 0.25504782795906067\n",
            "[Training Epoch 60] Batch 309, Loss 0.27151742577552795\n",
            "[Training Epoch 60] Batch 310, Loss 0.27954697608947754\n",
            "[Training Epoch 60] Batch 311, Loss 0.2667505443096161\n",
            "[Training Epoch 60] Batch 312, Loss 0.2540852427482605\n",
            "[Training Epoch 60] Batch 313, Loss 0.268557071685791\n",
            "[Training Epoch 60] Batch 314, Loss 0.2580823302268982\n",
            "[Training Epoch 60] Batch 315, Loss 0.2722073197364807\n",
            "[Training Epoch 60] Batch 316, Loss 0.27100783586502075\n",
            "[Training Epoch 60] Batch 317, Loss 0.29732146859169006\n",
            "[Training Epoch 60] Batch 318, Loss 0.26058346033096313\n",
            "[Training Epoch 60] Batch 319, Loss 0.2676989734172821\n",
            "[Training Epoch 60] Batch 320, Loss 0.26316216588020325\n",
            "[Training Epoch 60] Batch 321, Loss 0.2577586770057678\n",
            "[Training Epoch 60] Batch 322, Loss 0.25845813751220703\n",
            "[Training Epoch 60] Batch 323, Loss 0.2631677985191345\n",
            "[Training Epoch 60] Batch 324, Loss 0.24967707693576813\n",
            "[Training Epoch 60] Batch 325, Loss 0.2798377573490143\n",
            "[Training Epoch 60] Batch 326, Loss 0.256402850151062\n",
            "[Training Epoch 60] Batch 327, Loss 0.2548965513706207\n",
            "[Training Epoch 60] Batch 328, Loss 0.2779631018638611\n",
            "[Training Epoch 60] Batch 329, Loss 0.27303388714790344\n",
            "[Training Epoch 60] Batch 330, Loss 0.2820301651954651\n",
            "[Training Epoch 60] Batch 331, Loss 0.2581506073474884\n",
            "[Training Epoch 60] Batch 332, Loss 0.23812517523765564\n",
            "[Training Epoch 60] Batch 333, Loss 0.25639307498931885\n",
            "[Training Epoch 60] Batch 334, Loss 0.27231982350349426\n",
            "[Training Epoch 60] Batch 335, Loss 0.23165324330329895\n",
            "[Training Epoch 60] Batch 336, Loss 0.29195383191108704\n",
            "[Training Epoch 60] Batch 337, Loss 0.25506293773651123\n",
            "[Training Epoch 60] Batch 338, Loss 0.23691311478614807\n",
            "[Training Epoch 60] Batch 339, Loss 0.26176896691322327\n",
            "[Training Epoch 60] Batch 340, Loss 0.25980255007743835\n",
            "[Training Epoch 60] Batch 341, Loss 0.24691055715084076\n",
            "[Training Epoch 60] Batch 342, Loss 0.24456335604190826\n",
            "[Training Epoch 60] Batch 343, Loss 0.26847726106643677\n",
            "[Training Epoch 60] Batch 344, Loss 0.2619834244251251\n",
            "[Training Epoch 60] Batch 345, Loss 0.24651066958904266\n",
            "[Training Epoch 60] Batch 346, Loss 0.25906407833099365\n",
            "[Training Epoch 60] Batch 347, Loss 0.25539153814315796\n",
            "[Training Epoch 60] Batch 348, Loss 0.263771653175354\n",
            "[Training Epoch 60] Batch 349, Loss 0.26096826791763306\n",
            "[Training Epoch 60] Batch 350, Loss 0.2608390152454376\n",
            "[Training Epoch 60] Batch 351, Loss 0.2582140862941742\n",
            "[Training Epoch 60] Batch 352, Loss 0.27543556690216064\n",
            "[Training Epoch 60] Batch 353, Loss 0.2642977237701416\n",
            "[Training Epoch 60] Batch 354, Loss 0.29791855812072754\n",
            "[Training Epoch 60] Batch 355, Loss 0.27268004417419434\n",
            "[Training Epoch 60] Batch 356, Loss 0.27193009853363037\n",
            "[Training Epoch 60] Batch 357, Loss 0.2837010324001312\n",
            "[Training Epoch 60] Batch 358, Loss 0.2730009853839874\n",
            "[Training Epoch 60] Batch 359, Loss 0.2727583646774292\n",
            "[Training Epoch 60] Batch 360, Loss 0.2716764509677887\n",
            "[Training Epoch 60] Batch 361, Loss 0.25484558939933777\n",
            "[Training Epoch 60] Batch 362, Loss 0.25380170345306396\n",
            "[Training Epoch 60] Batch 363, Loss 0.250169575214386\n",
            "[Training Epoch 60] Batch 364, Loss 0.2560822367668152\n",
            "[Training Epoch 60] Batch 365, Loss 0.24299083650112152\n",
            "[Training Epoch 60] Batch 366, Loss 0.2762260138988495\n",
            "[Training Epoch 60] Batch 367, Loss 0.2473697066307068\n",
            "[Training Epoch 60] Batch 368, Loss 0.2818549871444702\n",
            "[Training Epoch 60] Batch 369, Loss 0.28758227825164795\n",
            "[Training Epoch 60] Batch 370, Loss 0.2884465157985687\n",
            "[Training Epoch 60] Batch 371, Loss 0.2593735456466675\n",
            "[Training Epoch 60] Batch 372, Loss 0.2626732289791107\n",
            "[Training Epoch 60] Batch 373, Loss 0.23549099266529083\n",
            "[Training Epoch 60] Batch 374, Loss 0.2911662459373474\n",
            "[Training Epoch 60] Batch 375, Loss 0.24414128065109253\n",
            "[Training Epoch 60] Batch 376, Loss 0.25022369623184204\n",
            "[Training Epoch 60] Batch 377, Loss 0.2684973180294037\n",
            "[Training Epoch 60] Batch 378, Loss 0.27604159712791443\n",
            "[Training Epoch 60] Batch 379, Loss 0.2575472891330719\n",
            "[Training Epoch 60] Batch 380, Loss 0.26388269662857056\n",
            "[Training Epoch 60] Batch 381, Loss 0.2379601001739502\n",
            "[Training Epoch 60] Batch 382, Loss 0.30847859382629395\n",
            "[Training Epoch 60] Batch 383, Loss 0.2511783540248871\n",
            "[Training Epoch 60] Batch 384, Loss 0.277325838804245\n",
            "[Training Epoch 60] Batch 385, Loss 0.2254958301782608\n",
            "[Training Epoch 60] Batch 386, Loss 0.27248477935791016\n",
            "[Training Epoch 60] Batch 387, Loss 0.2700653076171875\n",
            "[Training Epoch 60] Batch 388, Loss 0.28471237421035767\n",
            "[Training Epoch 60] Batch 389, Loss 0.2481141984462738\n",
            "[Training Epoch 60] Batch 390, Loss 0.28841477632522583\n",
            "[Training Epoch 60] Batch 391, Loss 0.25634855031967163\n",
            "[Training Epoch 60] Batch 392, Loss 0.2595731019973755\n",
            "[Training Epoch 60] Batch 393, Loss 0.30218714475631714\n",
            "[Training Epoch 60] Batch 394, Loss 0.25022995471954346\n",
            "[Training Epoch 60] Batch 395, Loss 0.26142778992652893\n",
            "[Training Epoch 60] Batch 396, Loss 0.25338879227638245\n",
            "[Training Epoch 60] Batch 397, Loss 0.2822389602661133\n",
            "[Training Epoch 60] Batch 398, Loss 0.2858552932739258\n",
            "[Training Epoch 60] Batch 399, Loss 0.27828851342201233\n",
            "[Training Epoch 60] Batch 400, Loss 0.2510819137096405\n",
            "[Training Epoch 60] Batch 401, Loss 0.2659551501274109\n",
            "[Training Epoch 60] Batch 402, Loss 0.26372841000556946\n",
            "[Training Epoch 60] Batch 403, Loss 0.2648767828941345\n",
            "[Training Epoch 60] Batch 404, Loss 0.23431043326854706\n",
            "[Training Epoch 60] Batch 405, Loss 0.25557151436805725\n",
            "[Training Epoch 60] Batch 406, Loss 0.2631407678127289\n",
            "[Training Epoch 60] Batch 407, Loss 0.2409680038690567\n",
            "[Training Epoch 60] Batch 408, Loss 0.2561854422092438\n",
            "[Training Epoch 60] Batch 409, Loss 0.2968008518218994\n",
            "[Training Epoch 60] Batch 410, Loss 0.2651141583919525\n",
            "[Training Epoch 60] Batch 411, Loss 0.26235443353652954\n",
            "[Training Epoch 60] Batch 412, Loss 0.27575623989105225\n",
            "[Training Epoch 60] Batch 413, Loss 0.26015761494636536\n",
            "[Training Epoch 60] Batch 414, Loss 0.2791864275932312\n",
            "[Training Epoch 60] Batch 415, Loss 0.24741360545158386\n",
            "[Training Epoch 60] Batch 416, Loss 0.28163135051727295\n",
            "[Training Epoch 60] Batch 417, Loss 0.24152898788452148\n",
            "[Training Epoch 60] Batch 418, Loss 0.27315253019332886\n",
            "[Training Epoch 60] Batch 419, Loss 0.3098728060722351\n",
            "[Training Epoch 60] Batch 420, Loss 0.27435797452926636\n",
            "[Training Epoch 60] Batch 421, Loss 0.27159383893013\n",
            "[Training Epoch 60] Batch 422, Loss 0.26580697298049927\n",
            "[Training Epoch 60] Batch 423, Loss 0.2493516206741333\n",
            "[Training Epoch 60] Batch 424, Loss 0.28364694118499756\n",
            "[Training Epoch 60] Batch 425, Loss 0.2835133671760559\n",
            "[Training Epoch 60] Batch 426, Loss 0.2542111873626709\n",
            "[Training Epoch 60] Batch 427, Loss 0.26674914360046387\n",
            "[Training Epoch 60] Batch 428, Loss 0.24592843651771545\n",
            "[Training Epoch 60] Batch 429, Loss 0.25556400418281555\n",
            "[Training Epoch 60] Batch 430, Loss 0.2948828339576721\n",
            "[Training Epoch 60] Batch 431, Loss 0.26458412408828735\n",
            "[Training Epoch 60] Batch 432, Loss 0.24573791027069092\n",
            "[Training Epoch 60] Batch 433, Loss 0.27051690220832825\n",
            "[Training Epoch 60] Batch 434, Loss 0.280158132314682\n",
            "[Training Epoch 60] Batch 435, Loss 0.26170048117637634\n",
            "[Training Epoch 60] Batch 436, Loss 0.26487401127815247\n",
            "[Training Epoch 60] Batch 437, Loss 0.269069641828537\n",
            "[Training Epoch 60] Batch 438, Loss 0.22389067709445953\n",
            "[Training Epoch 60] Batch 439, Loss 0.2728428244590759\n",
            "[Training Epoch 60] Batch 440, Loss 0.2566831707954407\n",
            "[Training Epoch 60] Batch 441, Loss 0.2709982097148895\n",
            "[Training Epoch 60] Batch 442, Loss 0.2698863446712494\n",
            "[Training Epoch 60] Batch 443, Loss 0.2684718072414398\n",
            "[Training Epoch 60] Batch 444, Loss 0.25602424144744873\n",
            "[Training Epoch 60] Batch 445, Loss 0.26365774869918823\n",
            "[Training Epoch 60] Batch 446, Loss 0.27225831151008606\n",
            "[Training Epoch 60] Batch 447, Loss 0.2703218162059784\n",
            "[Training Epoch 60] Batch 448, Loss 0.2879217565059662\n",
            "[Training Epoch 60] Batch 449, Loss 0.2549740672111511\n",
            "[Training Epoch 60] Batch 450, Loss 0.2536352276802063\n",
            "[Training Epoch 60] Batch 451, Loss 0.253044992685318\n",
            "[Training Epoch 60] Batch 452, Loss 0.24178603291511536\n",
            "[Training Epoch 60] Batch 453, Loss 0.29287630319595337\n",
            "[Training Epoch 60] Batch 454, Loss 0.3206651210784912\n",
            "[Training Epoch 60] Batch 455, Loss 0.24639524519443512\n",
            "[Training Epoch 60] Batch 456, Loss 0.2562515139579773\n",
            "[Training Epoch 60] Batch 457, Loss 0.2545000910758972\n",
            "[Training Epoch 60] Batch 458, Loss 0.23791569471359253\n",
            "[Training Epoch 60] Batch 459, Loss 0.2787959575653076\n",
            "[Training Epoch 60] Batch 460, Loss 0.2382379025220871\n",
            "[Training Epoch 60] Batch 461, Loss 0.25634583830833435\n",
            "[Training Epoch 60] Batch 462, Loss 0.24995429813861847\n",
            "[Training Epoch 60] Batch 463, Loss 0.2657237946987152\n",
            "[Training Epoch 60] Batch 464, Loss 0.25343436002731323\n",
            "[Training Epoch 60] Batch 465, Loss 0.25708067417144775\n",
            "[Training Epoch 60] Batch 466, Loss 0.2607685327529907\n",
            "[Training Epoch 60] Batch 467, Loss 0.23918253183364868\n",
            "[Training Epoch 60] Batch 468, Loss 0.2297954261302948\n",
            "[Training Epoch 60] Batch 469, Loss 0.26260364055633545\n",
            "[Training Epoch 60] Batch 470, Loss 0.2504563629627228\n",
            "[Training Epoch 60] Batch 471, Loss 0.2864559292793274\n",
            "[Training Epoch 60] Batch 472, Loss 0.264964759349823\n",
            "[Training Epoch 60] Batch 473, Loss 0.27430233359336853\n",
            "[Training Epoch 60] Batch 474, Loss 0.2636885643005371\n",
            "[Training Epoch 60] Batch 475, Loss 0.2621989846229553\n",
            "[Training Epoch 60] Batch 476, Loss 0.26372796297073364\n",
            "[Training Epoch 60] Batch 477, Loss 0.2596810758113861\n",
            "[Training Epoch 60] Batch 478, Loss 0.2896571159362793\n",
            "[Training Epoch 60] Batch 479, Loss 0.26848241686820984\n",
            "[Training Epoch 60] Batch 480, Loss 0.23251000046730042\n",
            "[Training Epoch 60] Batch 481, Loss 0.2832464873790741\n",
            "[Training Epoch 60] Batch 482, Loss 0.27605271339416504\n",
            "[Training Epoch 60] Batch 483, Loss 0.2487289011478424\n",
            "[Training Epoch 60] Batch 484, Loss 0.2395360767841339\n",
            "[Training Epoch 60] Batch 485, Loss 0.28066492080688477\n",
            "[Training Epoch 60] Batch 486, Loss 0.2659444212913513\n",
            "[Training Epoch 60] Batch 487, Loss 0.24704515933990479\n",
            "[Training Epoch 60] Batch 488, Loss 0.2799673080444336\n",
            "[Training Epoch 60] Batch 489, Loss 0.2703687846660614\n",
            "[Training Epoch 60] Batch 490, Loss 0.2419392317533493\n",
            "[Training Epoch 60] Batch 491, Loss 0.26879391074180603\n",
            "[Training Epoch 60] Batch 492, Loss 0.25527143478393555\n",
            "[Training Epoch 60] Batch 493, Loss 0.24214200675487518\n",
            "[Training Epoch 60] Batch 494, Loss 0.2626194953918457\n",
            "[Training Epoch 60] Batch 495, Loss 0.22923220694065094\n",
            "[Training Epoch 60] Batch 496, Loss 0.2643194794654846\n",
            "[Training Epoch 60] Batch 497, Loss 0.29524850845336914\n",
            "[Training Epoch 60] Batch 498, Loss 0.2746867537498474\n",
            "[Training Epoch 60] Batch 499, Loss 0.2410675436258316\n",
            "[Training Epoch 60] Batch 500, Loss 0.2791673243045807\n",
            "[Training Epoch 60] Batch 501, Loss 0.26360365748405457\n",
            "[Training Epoch 60] Batch 502, Loss 0.28979289531707764\n",
            "[Training Epoch 60] Batch 503, Loss 0.25795862078666687\n",
            "[Training Epoch 60] Batch 504, Loss 0.2513447403907776\n",
            "[Training Epoch 60] Batch 505, Loss 0.24752043187618256\n",
            "[Training Epoch 60] Batch 506, Loss 0.25693419575691223\n",
            "[Training Epoch 60] Batch 507, Loss 0.27736544609069824\n",
            "[Training Epoch 60] Batch 508, Loss 0.26192063093185425\n",
            "[Training Epoch 60] Batch 509, Loss 0.26526889204978943\n",
            "[Training Epoch 60] Batch 510, Loss 0.24796365201473236\n",
            "[Training Epoch 60] Batch 511, Loss 0.28974950313568115\n",
            "[Training Epoch 60] Batch 512, Loss 0.24692216515541077\n",
            "[Training Epoch 60] Batch 513, Loss 0.27203792333602905\n",
            "[Training Epoch 60] Batch 514, Loss 0.25399693846702576\n",
            "[Training Epoch 60] Batch 515, Loss 0.2688000202178955\n",
            "[Training Epoch 60] Batch 516, Loss 0.24330425262451172\n",
            "[Training Epoch 60] Batch 517, Loss 0.21767142415046692\n",
            "[Training Epoch 60] Batch 518, Loss 0.26214027404785156\n",
            "[Training Epoch 60] Batch 519, Loss 0.27457231283187866\n",
            "[Training Epoch 60] Batch 520, Loss 0.2641894221305847\n",
            "[Training Epoch 60] Batch 521, Loss 0.27636992931365967\n",
            "[Training Epoch 60] Batch 522, Loss 0.25997599959373474\n",
            "[Training Epoch 60] Batch 523, Loss 0.2646689713001251\n",
            "[Training Epoch 60] Batch 524, Loss 0.29479867219924927\n",
            "[Training Epoch 60] Batch 525, Loss 0.28820669651031494\n",
            "[Training Epoch 60] Batch 526, Loss 0.2422945499420166\n",
            "[Training Epoch 60] Batch 527, Loss 0.2945071756839752\n",
            "[Training Epoch 60] Batch 528, Loss 0.255948930978775\n",
            "[Training Epoch 60] Batch 529, Loss 0.2436935305595398\n",
            "[Training Epoch 60] Batch 530, Loss 0.2537914514541626\n",
            "[Training Epoch 60] Batch 531, Loss 0.2637380361557007\n",
            "[Training Epoch 60] Batch 532, Loss 0.26679012179374695\n",
            "[Training Epoch 60] Batch 533, Loss 0.24629315733909607\n",
            "[Training Epoch 60] Batch 534, Loss 0.23947465419769287\n",
            "[Training Epoch 60] Batch 535, Loss 0.290697306394577\n",
            "[Training Epoch 60] Batch 536, Loss 0.238614022731781\n",
            "[Training Epoch 60] Batch 537, Loss 0.24082162976264954\n",
            "[Training Epoch 60] Batch 538, Loss 0.27605298161506653\n",
            "[Training Epoch 60] Batch 539, Loss 0.2382674515247345\n",
            "[Training Epoch 60] Batch 540, Loss 0.25773221254348755\n",
            "[Training Epoch 60] Batch 541, Loss 0.2852019965648651\n",
            "[Training Epoch 60] Batch 542, Loss 0.25958797335624695\n",
            "[Training Epoch 60] Batch 543, Loss 0.2643616795539856\n",
            "[Training Epoch 60] Batch 544, Loss 0.25813350081443787\n",
            "[Training Epoch 60] Batch 545, Loss 0.25874248147010803\n",
            "[Training Epoch 60] Batch 546, Loss 0.27014434337615967\n",
            "[Training Epoch 60] Batch 547, Loss 0.2775873839855194\n",
            "[Training Epoch 60] Batch 548, Loss 0.27774831652641296\n",
            "[Training Epoch 60] Batch 549, Loss 0.25622785091400146\n",
            "[Training Epoch 60] Batch 550, Loss 0.2686942517757416\n",
            "[Training Epoch 60] Batch 551, Loss 0.2755250632762909\n",
            "[Training Epoch 60] Batch 552, Loss 0.2688975930213928\n",
            "[Training Epoch 60] Batch 553, Loss 0.25776466727256775\n",
            "[Training Epoch 60] Batch 554, Loss 0.26591047644615173\n",
            "[Training Epoch 60] Batch 555, Loss 0.2972436547279358\n",
            "[Training Epoch 60] Batch 556, Loss 0.2657126188278198\n",
            "[Training Epoch 60] Batch 557, Loss 0.2665683329105377\n",
            "[Training Epoch 60] Batch 558, Loss 0.2571831941604614\n",
            "[Training Epoch 60] Batch 559, Loss 0.28498461842536926\n",
            "[Training Epoch 60] Batch 560, Loss 0.2841566801071167\n",
            "[Training Epoch 60] Batch 561, Loss 0.27399885654449463\n",
            "[Training Epoch 60] Batch 562, Loss 0.24503910541534424\n",
            "[Training Epoch 60] Batch 563, Loss 0.25374627113342285\n",
            "[Training Epoch 60] Batch 564, Loss 0.25265294313430786\n",
            "[Training Epoch 60] Batch 565, Loss 0.2795744240283966\n",
            "[Training Epoch 60] Batch 566, Loss 0.24890567362308502\n",
            "[Training Epoch 60] Batch 567, Loss 0.25432008504867554\n",
            "[Training Epoch 60] Batch 568, Loss 0.2788907289505005\n",
            "[Training Epoch 60] Batch 569, Loss 0.22668838500976562\n",
            "[Training Epoch 60] Batch 570, Loss 0.2593281865119934\n",
            "[Training Epoch 60] Batch 571, Loss 0.2588185667991638\n",
            "[Training Epoch 60] Batch 572, Loss 0.26151949167251587\n",
            "[Training Epoch 60] Batch 573, Loss 0.2728493809700012\n",
            "[Training Epoch 60] Batch 574, Loss 0.26780515909194946\n",
            "[Training Epoch 60] Batch 575, Loss 0.2543313503265381\n",
            "[Training Epoch 60] Batch 576, Loss 0.28998902440071106\n",
            "[Training Epoch 60] Batch 577, Loss 0.25693953037261963\n",
            "[Training Epoch 60] Batch 578, Loss 0.26582732796669006\n",
            "[Training Epoch 60] Batch 579, Loss 0.273755818605423\n",
            "[Training Epoch 60] Batch 580, Loss 0.29762914776802063\n",
            "[Training Epoch 60] Batch 581, Loss 0.27804723381996155\n",
            "[Training Epoch 60] Batch 582, Loss 0.2325027585029602\n",
            "[Training Epoch 60] Batch 583, Loss 0.253736674785614\n",
            "[Training Epoch 60] Batch 584, Loss 0.28851762413978577\n",
            "[Training Epoch 60] Batch 585, Loss 0.28760531544685364\n",
            "[Training Epoch 60] Batch 586, Loss 0.2767699658870697\n",
            "[Training Epoch 60] Batch 587, Loss 0.2599087953567505\n",
            "[Training Epoch 60] Batch 588, Loss 0.2784616947174072\n",
            "[Training Epoch 60] Batch 589, Loss 0.2773827016353607\n",
            "[Training Epoch 60] Batch 590, Loss 0.26676636934280396\n",
            "[Training Epoch 60] Batch 591, Loss 0.2525865435600281\n",
            "[Training Epoch 60] Batch 592, Loss 0.2603074908256531\n",
            "[Training Epoch 60] Batch 593, Loss 0.25345379114151\n",
            "[Training Epoch 60] Batch 594, Loss 0.24752581119537354\n",
            "[Training Epoch 60] Batch 595, Loss 0.2493080496788025\n",
            "[Training Epoch 60] Batch 596, Loss 0.26822710037231445\n",
            "[Training Epoch 60] Batch 597, Loss 0.23069816827774048\n",
            "[Training Epoch 60] Batch 598, Loss 0.2810799479484558\n",
            "[Training Epoch 60] Batch 599, Loss 0.2551453411579132\n",
            "[Training Epoch 60] Batch 600, Loss 0.26853427290916443\n",
            "[Training Epoch 60] Batch 601, Loss 0.28082308173179626\n",
            "[Training Epoch 60] Batch 602, Loss 0.24581709504127502\n",
            "[Training Epoch 60] Batch 603, Loss 0.2604987919330597\n",
            "[Training Epoch 60] Batch 604, Loss 0.23745080828666687\n",
            "[Training Epoch 60] Batch 605, Loss 0.2997875213623047\n",
            "[Training Epoch 60] Batch 606, Loss 0.2561745047569275\n",
            "[Training Epoch 60] Batch 607, Loss 0.2654399573802948\n",
            "[Training Epoch 60] Batch 608, Loss 0.2630820572376251\n",
            "[Training Epoch 60] Batch 609, Loss 0.2676824927330017\n",
            "[Training Epoch 60] Batch 610, Loss 0.2642602026462555\n",
            "[Training Epoch 60] Batch 611, Loss 0.2513957619667053\n",
            "[Training Epoch 60] Batch 612, Loss 0.2702980935573578\n",
            "[Training Epoch 60] Batch 613, Loss 0.2513492703437805\n",
            "[Training Epoch 60] Batch 614, Loss 0.24219416081905365\n",
            "[Training Epoch 60] Batch 615, Loss 0.2506522536277771\n",
            "[Training Epoch 60] Batch 616, Loss 0.2698568105697632\n",
            "[Training Epoch 60] Batch 617, Loss 0.27108949422836304\n",
            "[Training Epoch 60] Batch 618, Loss 0.26793545484542847\n",
            "[Training Epoch 60] Batch 619, Loss 0.2582598924636841\n",
            "[Training Epoch 60] Batch 620, Loss 0.23775069415569305\n",
            "[Training Epoch 60] Batch 621, Loss 0.2554778456687927\n",
            "[Training Epoch 60] Batch 622, Loss 0.27466827630996704\n",
            "[Training Epoch 60] Batch 623, Loss 0.2913096249103546\n",
            "[Training Epoch 60] Batch 624, Loss 0.2671492397785187\n",
            "[Training Epoch 60] Batch 625, Loss 0.26182588934898376\n",
            "[Training Epoch 60] Batch 626, Loss 0.2827882170677185\n",
            "[Training Epoch 60] Batch 627, Loss 0.27619668841362\n",
            "[Training Epoch 60] Batch 628, Loss 0.22024527192115784\n",
            "[Training Epoch 60] Batch 629, Loss 0.2893195152282715\n",
            "[Training Epoch 60] Batch 630, Loss 0.23154467344284058\n",
            "[Training Epoch 60] Batch 631, Loss 0.2600932717323303\n",
            "[Training Epoch 60] Batch 632, Loss 0.283250093460083\n",
            "[Training Epoch 60] Batch 633, Loss 0.2499902993440628\n",
            "[Training Epoch 60] Batch 634, Loss 0.2584736943244934\n",
            "[Training Epoch 60] Batch 635, Loss 0.24953016638755798\n",
            "[Training Epoch 60] Batch 636, Loss 0.2716042697429657\n",
            "[Training Epoch 60] Batch 637, Loss 0.23305252194404602\n",
            "[Training Epoch 60] Batch 638, Loss 0.25648248195648193\n",
            "[Training Epoch 60] Batch 639, Loss 0.2931484878063202\n",
            "[Training Epoch 60] Batch 640, Loss 0.2762638032436371\n",
            "[Training Epoch 60] Batch 641, Loss 0.27076810598373413\n",
            "[Training Epoch 60] Batch 642, Loss 0.23823398351669312\n",
            "[Training Epoch 60] Batch 643, Loss 0.25147104263305664\n",
            "[Training Epoch 60] Batch 644, Loss 0.2937285006046295\n",
            "[Training Epoch 60] Batch 645, Loss 0.2864396274089813\n",
            "[Training Epoch 60] Batch 646, Loss 0.2742828130722046\n",
            "[Training Epoch 60] Batch 647, Loss 0.257328063249588\n",
            "[Training Epoch 60] Batch 648, Loss 0.2673151195049286\n",
            "[Training Epoch 60] Batch 649, Loss 0.2655905485153198\n",
            "[Training Epoch 60] Batch 650, Loss 0.25300315022468567\n",
            "[Training Epoch 60] Batch 651, Loss 0.2762075662612915\n",
            "[Training Epoch 60] Batch 652, Loss 0.25335395336151123\n",
            "[Training Epoch 60] Batch 653, Loss 0.27912983298301697\n",
            "[Training Epoch 60] Batch 654, Loss 0.28037238121032715\n",
            "[Training Epoch 60] Batch 655, Loss 0.27390366792678833\n",
            "[Training Epoch 60] Batch 656, Loss 0.25412535667419434\n",
            "[Training Epoch 60] Batch 657, Loss 0.2629789113998413\n",
            "[Training Epoch 60] Batch 658, Loss 0.29986873269081116\n",
            "[Training Epoch 60] Batch 659, Loss 0.2502232789993286\n",
            "[Training Epoch 60] Batch 660, Loss 0.2744275629520416\n",
            "[Training Epoch 60] Batch 661, Loss 0.26963746547698975\n",
            "[Training Epoch 60] Batch 662, Loss 0.26987433433532715\n",
            "[Training Epoch 60] Batch 663, Loss 0.28618571162223816\n",
            "[Training Epoch 60] Batch 664, Loss 0.2745758295059204\n",
            "[Training Epoch 60] Batch 665, Loss 0.26720762252807617\n",
            "[Training Epoch 60] Batch 666, Loss 0.2473779022693634\n",
            "[Training Epoch 60] Batch 667, Loss 0.2810916602611542\n",
            "[Training Epoch 60] Batch 668, Loss 0.24090629816055298\n",
            "[Training Epoch 60] Batch 669, Loss 0.27856311202049255\n",
            "[Training Epoch 60] Batch 670, Loss 0.29052865505218506\n",
            "[Training Epoch 60] Batch 671, Loss 0.27067476511001587\n",
            "[Training Epoch 60] Batch 672, Loss 0.2848049998283386\n",
            "[Training Epoch 60] Batch 673, Loss 0.2642202377319336\n",
            "[Training Epoch 60] Batch 674, Loss 0.26855549216270447\n",
            "[Training Epoch 60] Batch 675, Loss 0.26118171215057373\n",
            "[Training Epoch 60] Batch 676, Loss 0.2538585662841797\n",
            "[Training Epoch 60] Batch 677, Loss 0.26602548360824585\n",
            "[Training Epoch 60] Batch 678, Loss 0.2706950902938843\n",
            "[Training Epoch 60] Batch 679, Loss 0.2862096428871155\n",
            "[Training Epoch 60] Batch 680, Loss 0.2615576982498169\n",
            "[Training Epoch 60] Batch 681, Loss 0.26489830017089844\n",
            "[Training Epoch 60] Batch 682, Loss 0.30008941888809204\n",
            "[Training Epoch 60] Batch 683, Loss 0.2810717225074768\n",
            "[Training Epoch 60] Batch 684, Loss 0.2504512071609497\n",
            "[Training Epoch 60] Batch 685, Loss 0.26359328627586365\n",
            "[Training Epoch 60] Batch 686, Loss 0.28560659289360046\n",
            "[Training Epoch 60] Batch 687, Loss 0.26249030232429504\n",
            "[Training Epoch 60] Batch 688, Loss 0.2669095993041992\n",
            "[Training Epoch 60] Batch 689, Loss 0.23413297533988953\n",
            "[Training Epoch 60] Batch 690, Loss 0.24612772464752197\n",
            "[Training Epoch 60] Batch 691, Loss 0.25861606001853943\n",
            "[Training Epoch 60] Batch 692, Loss 0.2767515480518341\n",
            "[Training Epoch 60] Batch 693, Loss 0.2559301555156708\n",
            "[Training Epoch 60] Batch 694, Loss 0.302272766828537\n",
            "[Training Epoch 60] Batch 695, Loss 0.25109827518463135\n",
            "[Training Epoch 60] Batch 696, Loss 0.2873019278049469\n",
            "[Training Epoch 60] Batch 697, Loss 0.2690150737762451\n",
            "[Training Epoch 60] Batch 698, Loss 0.26863908767700195\n",
            "[Training Epoch 60] Batch 699, Loss 0.26842939853668213\n",
            "[Training Epoch 60] Batch 700, Loss 0.2909694015979767\n",
            "[Training Epoch 60] Batch 701, Loss 0.2267431765794754\n",
            "[Training Epoch 60] Batch 702, Loss 0.2634870111942291\n",
            "[Training Epoch 60] Batch 703, Loss 0.2577103078365326\n",
            "[Training Epoch 60] Batch 704, Loss 0.2500898838043213\n",
            "[Training Epoch 60] Batch 705, Loss 0.2859886586666107\n",
            "[Training Epoch 60] Batch 706, Loss 0.2395784556865692\n",
            "[Training Epoch 60] Batch 707, Loss 0.2901405692100525\n",
            "[Training Epoch 60] Batch 708, Loss 0.24606108665466309\n",
            "[Training Epoch 60] Batch 709, Loss 0.2714611291885376\n",
            "[Training Epoch 60] Batch 710, Loss 0.250200480222702\n",
            "[Training Epoch 60] Batch 711, Loss 0.2775343358516693\n",
            "[Training Epoch 60] Batch 712, Loss 0.26202622056007385\n",
            "[Training Epoch 60] Batch 713, Loss 0.2534641921520233\n",
            "[Training Epoch 60] Batch 714, Loss 0.2690405249595642\n",
            "[Training Epoch 60] Batch 715, Loss 0.2625645399093628\n",
            "[Training Epoch 60] Batch 716, Loss 0.2589253783226013\n",
            "[Training Epoch 60] Batch 717, Loss 0.26550769805908203\n",
            "[Training Epoch 60] Batch 718, Loss 0.27558615803718567\n",
            "[Training Epoch 60] Batch 719, Loss 0.27430784702301025\n",
            "[Training Epoch 60] Batch 720, Loss 0.27027979493141174\n",
            "[Training Epoch 60] Batch 721, Loss 0.276040256023407\n",
            "[Training Epoch 60] Batch 722, Loss 0.24918779730796814\n",
            "[Training Epoch 60] Batch 723, Loss 0.27034398913383484\n",
            "[Training Epoch 60] Batch 724, Loss 0.25861266255378723\n",
            "[Training Epoch 60] Batch 725, Loss 0.25107717514038086\n",
            "[Training Epoch 60] Batch 726, Loss 0.2544418275356293\n",
            "[Training Epoch 60] Batch 727, Loss 0.25974538922309875\n",
            "[Training Epoch 60] Batch 728, Loss 0.24702775478363037\n",
            "[Training Epoch 60] Batch 729, Loss 0.2876330614089966\n",
            "[Training Epoch 60] Batch 730, Loss 0.2481497973203659\n",
            "[Training Epoch 60] Batch 731, Loss 0.25862130522727966\n",
            "[Training Epoch 60] Batch 732, Loss 0.24588114023208618\n",
            "[Training Epoch 60] Batch 733, Loss 0.30005425214767456\n",
            "[Training Epoch 60] Batch 734, Loss 0.2813967764377594\n",
            "[Training Epoch 60] Batch 735, Loss 0.2755643427371979\n",
            "[Training Epoch 60] Batch 736, Loss 0.2777653634548187\n",
            "[Training Epoch 60] Batch 737, Loss 0.28645309805870056\n",
            "[Training Epoch 60] Batch 738, Loss 0.26620280742645264\n",
            "[Training Epoch 60] Batch 739, Loss 0.23707062005996704\n",
            "[Training Epoch 60] Batch 740, Loss 0.2840321362018585\n",
            "[Training Epoch 60] Batch 741, Loss 0.27350953221321106\n",
            "[Training Epoch 60] Batch 742, Loss 0.2422231286764145\n",
            "[Training Epoch 60] Batch 743, Loss 0.2917374074459076\n",
            "[Training Epoch 60] Batch 744, Loss 0.23153182864189148\n",
            "[Training Epoch 60] Batch 745, Loss 0.243604376912117\n",
            "[Training Epoch 60] Batch 746, Loss 0.2328375279903412\n",
            "[Training Epoch 60] Batch 747, Loss 0.2682584226131439\n",
            "[Training Epoch 60] Batch 748, Loss 0.25233447551727295\n",
            "[Training Epoch 60] Batch 749, Loss 0.2585754692554474\n",
            "[Training Epoch 60] Batch 750, Loss 0.26575565338134766\n",
            "[Training Epoch 60] Batch 751, Loss 0.28316783905029297\n",
            "[Training Epoch 60] Batch 752, Loss 0.2772896885871887\n",
            "[Training Epoch 60] Batch 753, Loss 0.25179311633110046\n",
            "[Training Epoch 60] Batch 754, Loss 0.23699694871902466\n",
            "[Training Epoch 60] Batch 755, Loss 0.26025283336639404\n",
            "[Training Epoch 60] Batch 756, Loss 0.25409412384033203\n",
            "[Training Epoch 60] Batch 757, Loss 0.24957098066806793\n",
            "[Training Epoch 60] Batch 758, Loss 0.2783757150173187\n",
            "[Training Epoch 60] Batch 759, Loss 0.25185123085975647\n",
            "[Training Epoch 60] Batch 760, Loss 0.24300798773765564\n",
            "[Training Epoch 60] Batch 761, Loss 0.26816651225090027\n",
            "[Training Epoch 60] Batch 762, Loss 0.26584920287132263\n",
            "[Training Epoch 60] Batch 763, Loss 0.2651767432689667\n",
            "[Training Epoch 60] Batch 764, Loss 0.27719855308532715\n",
            "[Training Epoch 60] Batch 765, Loss 0.2615988552570343\n",
            "[Training Epoch 60] Batch 766, Loss 0.2404763251543045\n",
            "[Training Epoch 60] Batch 767, Loss 0.24744604527950287\n",
            "[Training Epoch 60] Batch 768, Loss 0.2644917070865631\n",
            "[Training Epoch 60] Batch 769, Loss 0.27343541383743286\n",
            "[Training Epoch 60] Batch 770, Loss 0.27954012155532837\n",
            "[Training Epoch 60] Batch 771, Loss 0.2642683982849121\n",
            "[Training Epoch 60] Batch 772, Loss 0.24798384308815002\n",
            "[Training Epoch 60] Batch 773, Loss 0.28006768226623535\n",
            "[Training Epoch 60] Batch 774, Loss 0.24631039798259735\n",
            "[Training Epoch 60] Batch 775, Loss 0.2592771053314209\n",
            "[Training Epoch 60] Batch 776, Loss 0.2590477168560028\n",
            "[Training Epoch 60] Batch 777, Loss 0.2484365850687027\n",
            "[Training Epoch 60] Batch 778, Loss 0.2593492269515991\n",
            "[Training Epoch 60] Batch 779, Loss 0.25147444009780884\n",
            "[Training Epoch 60] Batch 780, Loss 0.2469140738248825\n",
            "[Training Epoch 60] Batch 781, Loss 0.2509540021419525\n",
            "[Training Epoch 60] Batch 782, Loss 0.26474684476852417\n",
            "[Training Epoch 60] Batch 783, Loss 0.27582088112831116\n",
            "[Training Epoch 60] Batch 784, Loss 0.27856236696243286\n",
            "[Training Epoch 60] Batch 785, Loss 0.23944160342216492\n",
            "[Training Epoch 60] Batch 786, Loss 0.29832303524017334\n",
            "[Training Epoch 60] Batch 787, Loss 0.2537386417388916\n",
            "[Training Epoch 60] Batch 788, Loss 0.26581281423568726\n",
            "[Training Epoch 60] Batch 789, Loss 0.2367973029613495\n",
            "[Training Epoch 60] Batch 790, Loss 0.2524208128452301\n",
            "[Training Epoch 60] Batch 791, Loss 0.2426900416612625\n",
            "[Training Epoch 60] Batch 792, Loss 0.2585999071598053\n",
            "[Training Epoch 60] Batch 793, Loss 0.301469087600708\n",
            "[Training Epoch 60] Batch 794, Loss 0.25954219698905945\n",
            "[Training Epoch 60] Batch 795, Loss 0.2789873480796814\n",
            "[Training Epoch 60] Batch 796, Loss 0.24546721577644348\n",
            "[Training Epoch 60] Batch 797, Loss 0.2568812668323517\n",
            "[Training Epoch 60] Batch 798, Loss 0.2560337781906128\n",
            "[Training Epoch 60] Batch 799, Loss 0.258041650056839\n",
            "[Training Epoch 60] Batch 800, Loss 0.2508085072040558\n",
            "[Training Epoch 60] Batch 801, Loss 0.29760682582855225\n",
            "[Training Epoch 60] Batch 802, Loss 0.2646067142486572\n",
            "[Training Epoch 60] Batch 803, Loss 0.2776772677898407\n",
            "[Training Epoch 60] Batch 804, Loss 0.24077461659908295\n",
            "[Training Epoch 60] Batch 805, Loss 0.2640361487865448\n",
            "[Training Epoch 60] Batch 806, Loss 0.26707637310028076\n",
            "[Training Epoch 60] Batch 807, Loss 0.27524057030677795\n",
            "[Training Epoch 60] Batch 808, Loss 0.26602911949157715\n",
            "[Training Epoch 60] Batch 809, Loss 0.2636549472808838\n",
            "[Training Epoch 60] Batch 810, Loss 0.27636417746543884\n",
            "[Training Epoch 60] Batch 811, Loss 0.24544084072113037\n",
            "[Training Epoch 60] Batch 812, Loss 0.22393324971199036\n",
            "[Training Epoch 60] Batch 813, Loss 0.2913556396961212\n",
            "[Training Epoch 60] Batch 814, Loss 0.2871094346046448\n",
            "[Training Epoch 60] Batch 815, Loss 0.26833584904670715\n",
            "[Training Epoch 60] Batch 816, Loss 0.2605370283126831\n",
            "[Training Epoch 60] Batch 817, Loss 0.274198979139328\n",
            "[Training Epoch 60] Batch 818, Loss 0.27541062235832214\n",
            "[Training Epoch 60] Batch 819, Loss 0.26936212182044983\n",
            "[Training Epoch 60] Batch 820, Loss 0.2560007870197296\n",
            "[Training Epoch 60] Batch 821, Loss 0.27022168040275574\n",
            "[Training Epoch 60] Batch 822, Loss 0.2860102951526642\n",
            "[Training Epoch 60] Batch 823, Loss 0.24588948488235474\n",
            "[Training Epoch 60] Batch 824, Loss 0.24985657632350922\n",
            "[Training Epoch 60] Batch 825, Loss 0.28438541293144226\n",
            "[Training Epoch 60] Batch 826, Loss 0.25954192876815796\n",
            "[Training Epoch 60] Batch 827, Loss 0.27457478642463684\n",
            "[Training Epoch 60] Batch 828, Loss 0.26483744382858276\n",
            "[Training Epoch 60] Batch 829, Loss 0.2542957067489624\n",
            "[Training Epoch 60] Batch 830, Loss 0.27791982889175415\n",
            "[Training Epoch 60] Batch 831, Loss 0.251964271068573\n",
            "[Training Epoch 60] Batch 832, Loss 0.28556010127067566\n",
            "[Training Epoch 60] Batch 833, Loss 0.24630075693130493\n",
            "[Training Epoch 60] Batch 834, Loss 0.2743831276893616\n",
            "[Training Epoch 60] Batch 835, Loss 0.22508464753627777\n",
            "[Training Epoch 60] Batch 836, Loss 0.27128005027770996\n",
            "[Training Epoch 60] Batch 837, Loss 0.278947114944458\n",
            "[Training Epoch 60] Batch 838, Loss 0.26407891511917114\n",
            "[Training Epoch 60] Batch 839, Loss 0.24892829358577728\n",
            "[Training Epoch 60] Batch 840, Loss 0.26694542169570923\n",
            "[Training Epoch 60] Batch 841, Loss 0.2461283653974533\n",
            "[Training Epoch 60] Batch 842, Loss 0.23310089111328125\n",
            "[Training Epoch 60] Batch 843, Loss 0.25132283568382263\n",
            "[Training Epoch 60] Batch 844, Loss 0.2510382831096649\n",
            "[Training Epoch 60] Batch 845, Loss 0.2667614817619324\n",
            "[Training Epoch 60] Batch 846, Loss 0.2705371379852295\n",
            "[Training Epoch 60] Batch 847, Loss 0.2611112892627716\n",
            "[Training Epoch 60] Batch 848, Loss 0.22758254408836365\n",
            "[Training Epoch 60] Batch 849, Loss 0.27860531210899353\n",
            "[Training Epoch 60] Batch 850, Loss 0.25765252113342285\n",
            "[Training Epoch 60] Batch 851, Loss 0.25411975383758545\n",
            "[Training Epoch 60] Batch 852, Loss 0.26964837312698364\n",
            "[Training Epoch 60] Batch 853, Loss 0.2390803098678589\n",
            "[Training Epoch 60] Batch 854, Loss 0.2631370723247528\n",
            "[Training Epoch 60] Batch 855, Loss 0.2459840625524521\n",
            "[Training Epoch 60] Batch 856, Loss 0.26471787691116333\n",
            "[Training Epoch 60] Batch 857, Loss 0.25192543864250183\n",
            "[Training Epoch 60] Batch 858, Loss 0.26205453276634216\n",
            "[Training Epoch 60] Batch 859, Loss 0.24796074628829956\n",
            "[Training Epoch 60] Batch 860, Loss 0.242047518491745\n",
            "[Training Epoch 60] Batch 861, Loss 0.26831358671188354\n",
            "[Training Epoch 60] Batch 862, Loss 0.3287297785282135\n",
            "[Training Epoch 60] Batch 863, Loss 0.248151496052742\n",
            "[Training Epoch 60] Batch 864, Loss 0.26079410314559937\n",
            "[Training Epoch 60] Batch 865, Loss 0.29920828342437744\n",
            "[Training Epoch 60] Batch 866, Loss 0.2620552182197571\n",
            "[Training Epoch 60] Batch 867, Loss 0.26667165756225586\n",
            "[Training Epoch 60] Batch 868, Loss 0.2234509140253067\n",
            "[Training Epoch 60] Batch 869, Loss 0.2395755648612976\n",
            "[Training Epoch 60] Batch 870, Loss 0.27743303775787354\n",
            "[Training Epoch 60] Batch 871, Loss 0.255154013633728\n",
            "[Training Epoch 60] Batch 872, Loss 0.2769674062728882\n",
            "[Training Epoch 60] Batch 873, Loss 0.2712394893169403\n",
            "[Training Epoch 60] Batch 874, Loss 0.2590232789516449\n",
            "[Training Epoch 60] Batch 875, Loss 0.26076093316078186\n",
            "[Training Epoch 60] Batch 876, Loss 0.2613552212715149\n",
            "[Training Epoch 60] Batch 877, Loss 0.24478168785572052\n",
            "[Training Epoch 60] Batch 878, Loss 0.2953449785709381\n",
            "[Training Epoch 60] Batch 879, Loss 0.22691471874713898\n",
            "[Training Epoch 60] Batch 880, Loss 0.2647261321544647\n",
            "[Training Epoch 60] Batch 881, Loss 0.26322078704833984\n",
            "[Training Epoch 60] Batch 882, Loss 0.25877875089645386\n",
            "[Training Epoch 60] Batch 883, Loss 0.25680801272392273\n",
            "[Training Epoch 60] Batch 884, Loss 0.26409733295440674\n",
            "[Training Epoch 60] Batch 885, Loss 0.2562275826931\n",
            "[Training Epoch 60] Batch 886, Loss 0.24103817343711853\n",
            "[Training Epoch 60] Batch 887, Loss 0.26062482595443726\n",
            "[Training Epoch 60] Batch 888, Loss 0.2675478458404541\n",
            "[Training Epoch 60] Batch 889, Loss 0.28183814883232117\n",
            "[Training Epoch 60] Batch 890, Loss 0.2568112909793854\n",
            "[Training Epoch 60] Batch 891, Loss 0.2847762703895569\n",
            "[Training Epoch 60] Batch 892, Loss 0.2362806797027588\n",
            "[Training Epoch 60] Batch 893, Loss 0.24963051080703735\n",
            "[Training Epoch 60] Batch 894, Loss 0.28767135739326477\n",
            "[Training Epoch 60] Batch 895, Loss 0.268537312746048\n",
            "[Training Epoch 60] Batch 896, Loss 0.24572473764419556\n",
            "[Training Epoch 60] Batch 897, Loss 0.2604439854621887\n",
            "[Training Epoch 60] Batch 898, Loss 0.2761836051940918\n",
            "[Training Epoch 60] Batch 899, Loss 0.2641540765762329\n",
            "[Training Epoch 60] Batch 900, Loss 0.2374349683523178\n",
            "[Training Epoch 60] Batch 901, Loss 0.23685835301876068\n",
            "[Training Epoch 60] Batch 902, Loss 0.26234519481658936\n",
            "[Training Epoch 60] Batch 903, Loss 0.27733856439590454\n",
            "[Training Epoch 60] Batch 904, Loss 0.27551454305648804\n",
            "[Training Epoch 60] Batch 905, Loss 0.26348206400871277\n",
            "[Training Epoch 60] Batch 906, Loss 0.23854556679725647\n",
            "[Training Epoch 60] Batch 907, Loss 0.2776887118816376\n",
            "[Training Epoch 60] Batch 908, Loss 0.2440251111984253\n",
            "[Training Epoch 60] Batch 909, Loss 0.24624483287334442\n",
            "[Training Epoch 60] Batch 910, Loss 0.28108328580856323\n",
            "[Training Epoch 60] Batch 911, Loss 0.27141261100769043\n",
            "[Training Epoch 60] Batch 912, Loss 0.2737521827220917\n",
            "[Training Epoch 60] Batch 913, Loss 0.2715127468109131\n",
            "[Training Epoch 60] Batch 914, Loss 0.2786203920841217\n",
            "[Training Epoch 60] Batch 915, Loss 0.2542981505393982\n",
            "[Training Epoch 60] Batch 916, Loss 0.2754630148410797\n",
            "[Training Epoch 60] Batch 917, Loss 0.2468774914741516\n",
            "[Training Epoch 60] Batch 918, Loss 0.2587376832962036\n",
            "[Training Epoch 60] Batch 919, Loss 0.2615021765232086\n",
            "[Training Epoch 60] Batch 920, Loss 0.24921846389770508\n",
            "[Training Epoch 60] Batch 921, Loss 0.24984358251094818\n",
            "[Training Epoch 60] Batch 922, Loss 0.2735775113105774\n",
            "[Training Epoch 60] Batch 923, Loss 0.2448231279850006\n",
            "[Training Epoch 60] Batch 924, Loss 0.2793562114238739\n",
            "[Training Epoch 60] Batch 925, Loss 0.2657029628753662\n",
            "[Training Epoch 60] Batch 926, Loss 0.2342851758003235\n",
            "[Training Epoch 60] Batch 927, Loss 0.25514593720436096\n",
            "[Training Epoch 60] Batch 928, Loss 0.23620694875717163\n",
            "[Training Epoch 60] Batch 929, Loss 0.2981552481651306\n",
            "[Training Epoch 60] Batch 930, Loss 0.2746936082839966\n",
            "[Training Epoch 60] Batch 931, Loss 0.2558513879776001\n",
            "[Training Epoch 60] Batch 932, Loss 0.28285062313079834\n",
            "[Training Epoch 60] Batch 933, Loss 0.2542935907840729\n",
            "[Training Epoch 60] Batch 934, Loss 0.27293357253074646\n",
            "[Training Epoch 60] Batch 935, Loss 0.2857835292816162\n",
            "[Training Epoch 60] Batch 936, Loss 0.26103541254997253\n",
            "[Training Epoch 60] Batch 937, Loss 0.28063690662384033\n",
            "[Training Epoch 60] Batch 938, Loss 0.28493231534957886\n",
            "[Training Epoch 60] Batch 939, Loss 0.2812615633010864\n",
            "[Training Epoch 60] Batch 940, Loss 0.2663198411464691\n",
            "[Training Epoch 60] Batch 941, Loss 0.2568472623825073\n",
            "[Training Epoch 60] Batch 942, Loss 0.26475197076797485\n",
            "[Training Epoch 60] Batch 943, Loss 0.26346731185913086\n",
            "[Training Epoch 60] Batch 944, Loss 0.2793101668357849\n",
            "[Training Epoch 60] Batch 945, Loss 0.2550814747810364\n",
            "[Training Epoch 60] Batch 946, Loss 0.26279696822166443\n",
            "[Training Epoch 60] Batch 947, Loss 0.2520396113395691\n",
            "[Training Epoch 60] Batch 948, Loss 0.2597266137599945\n",
            "[Training Epoch 60] Batch 949, Loss 0.2591216564178467\n",
            "[Training Epoch 60] Batch 950, Loss 0.3113962411880493\n",
            "[Training Epoch 60] Batch 951, Loss 0.2624974846839905\n",
            "[Training Epoch 60] Batch 952, Loss 0.2562900185585022\n",
            "[Training Epoch 60] Batch 953, Loss 0.25120943784713745\n",
            "[Training Epoch 60] Batch 954, Loss 0.27140364050865173\n",
            "[Training Epoch 60] Batch 955, Loss 0.22520580887794495\n",
            "[Training Epoch 60] Batch 956, Loss 0.2595410943031311\n",
            "[Training Epoch 60] Batch 957, Loss 0.2721986770629883\n",
            "[Training Epoch 60] Batch 958, Loss 0.2552476227283478\n",
            "[Training Epoch 60] Batch 959, Loss 0.2597039043903351\n",
            "[Training Epoch 60] Batch 960, Loss 0.24122509360313416\n",
            "[Training Epoch 60] Batch 961, Loss 0.28404727578163147\n",
            "[Training Epoch 60] Batch 962, Loss 0.24750928580760956\n",
            "[Training Epoch 60] Batch 963, Loss 0.2809267044067383\n",
            "[Training Epoch 60] Batch 964, Loss 0.247653529047966\n",
            "[Training Epoch 60] Batch 965, Loss 0.2530115842819214\n",
            "[Training Epoch 60] Batch 966, Loss 0.2893332242965698\n",
            "[Training Epoch 60] Batch 967, Loss 0.25286996364593506\n",
            "[Training Epoch 60] Batch 968, Loss 0.2438255250453949\n",
            "[Training Epoch 60] Batch 969, Loss 0.2667984664440155\n",
            "[Training Epoch 60] Batch 970, Loss 0.25058919191360474\n",
            "[Training Epoch 60] Batch 971, Loss 0.2613469660282135\n",
            "[Training Epoch 60] Batch 972, Loss 0.2768403887748718\n",
            "[Training Epoch 60] Batch 973, Loss 0.26832348108291626\n",
            "[Training Epoch 60] Batch 974, Loss 0.2507922053337097\n",
            "[Training Epoch 60] Batch 975, Loss 0.25484803318977356\n",
            "[Training Epoch 60] Batch 976, Loss 0.2598687708377838\n",
            "[Training Epoch 60] Batch 977, Loss 0.2811420261859894\n",
            "[Training Epoch 60] Batch 978, Loss 0.2552424669265747\n",
            "[Training Epoch 60] Batch 979, Loss 0.2537955641746521\n",
            "[Training Epoch 60] Batch 980, Loss 0.30814430117607117\n",
            "[Training Epoch 60] Batch 981, Loss 0.24765676259994507\n",
            "[Training Epoch 60] Batch 982, Loss 0.26708799600601196\n",
            "[Training Epoch 60] Batch 983, Loss 0.278917133808136\n",
            "[Training Epoch 60] Batch 984, Loss 0.25235968828201294\n",
            "[Training Epoch 60] Batch 985, Loss 0.2755492627620697\n",
            "[Training Epoch 60] Batch 986, Loss 0.2491910755634308\n",
            "[Training Epoch 60] Batch 987, Loss 0.27562838792800903\n",
            "[Training Epoch 60] Batch 988, Loss 0.2704479992389679\n",
            "[Training Epoch 60] Batch 989, Loss 0.2394847273826599\n",
            "[Training Epoch 60] Batch 990, Loss 0.2500379979610443\n",
            "[Training Epoch 60] Batch 991, Loss 0.2457716464996338\n",
            "[Training Epoch 60] Batch 992, Loss 0.2578931748867035\n",
            "[Training Epoch 60] Batch 993, Loss 0.2713530957698822\n",
            "[Training Epoch 60] Batch 994, Loss 0.28006982803344727\n",
            "[Training Epoch 60] Batch 995, Loss 0.2568168044090271\n",
            "[Training Epoch 60] Batch 996, Loss 0.28155040740966797\n",
            "[Training Epoch 60] Batch 997, Loss 0.23920917510986328\n",
            "[Training Epoch 60] Batch 998, Loss 0.2603188753128052\n",
            "[Training Epoch 60] Batch 999, Loss 0.26741716265678406\n",
            "[Training Epoch 60] Batch 1000, Loss 0.24500222504138947\n",
            "[Training Epoch 60] Batch 1001, Loss 0.2663314938545227\n",
            "[Training Epoch 60] Batch 1002, Loss 0.2549912929534912\n",
            "[Training Epoch 60] Batch 1003, Loss 0.2677198350429535\n",
            "[Training Epoch 60] Batch 1004, Loss 0.2677966356277466\n",
            "[Training Epoch 60] Batch 1005, Loss 0.24728165566921234\n",
            "[Training Epoch 60] Batch 1006, Loss 0.277712881565094\n",
            "[Training Epoch 60] Batch 1007, Loss 0.2559325098991394\n",
            "[Training Epoch 60] Batch 1008, Loss 0.2589520215988159\n",
            "[Training Epoch 60] Batch 1009, Loss 0.27815118432044983\n",
            "[Training Epoch 60] Batch 1010, Loss 0.25850024819374084\n",
            "[Training Epoch 60] Batch 1011, Loss 0.25107070803642273\n",
            "[Training Epoch 60] Batch 1012, Loss 0.254630982875824\n",
            "[Training Epoch 60] Batch 1013, Loss 0.23406106233596802\n",
            "[Training Epoch 60] Batch 1014, Loss 0.27157899737358093\n",
            "[Training Epoch 60] Batch 1015, Loss 0.25983095169067383\n",
            "[Training Epoch 60] Batch 1016, Loss 0.23602856695652008\n",
            "[Training Epoch 60] Batch 1017, Loss 0.25619715452194214\n",
            "[Training Epoch 60] Batch 1018, Loss 0.2222709059715271\n",
            "[Training Epoch 60] Batch 1019, Loss 0.23753879964351654\n",
            "[Training Epoch 60] Batch 1020, Loss 0.23669062554836273\n",
            "[Training Epoch 60] Batch 1021, Loss 0.26722899079322815\n",
            "[Training Epoch 60] Batch 1022, Loss 0.24010488390922546\n",
            "[Training Epoch 60] Batch 1023, Loss 0.244938924908638\n",
            "[Training Epoch 60] Batch 1024, Loss 0.248551607131958\n",
            "[Training Epoch 60] Batch 1025, Loss 0.24711094796657562\n",
            "[Training Epoch 60] Batch 1026, Loss 0.2733387053012848\n",
            "[Training Epoch 60] Batch 1027, Loss 0.23487603664398193\n",
            "[Training Epoch 60] Batch 1028, Loss 0.2734985649585724\n",
            "[Training Epoch 60] Batch 1029, Loss 0.2678755223751068\n",
            "[Training Epoch 60] Batch 1030, Loss 0.24904605746269226\n",
            "[Training Epoch 60] Batch 1031, Loss 0.2790892720222473\n",
            "[Training Epoch 60] Batch 1032, Loss 0.2490113526582718\n",
            "[Training Epoch 60] Batch 1033, Loss 0.26356032490730286\n",
            "[Training Epoch 60] Batch 1034, Loss 0.24875988066196442\n",
            "[Training Epoch 60] Batch 1035, Loss 0.28404027223587036\n",
            "[Training Epoch 60] Batch 1036, Loss 0.26891782879829407\n",
            "[Training Epoch 60] Batch 1037, Loss 0.27263668179512024\n",
            "[Training Epoch 60] Batch 1038, Loss 0.2661910057067871\n",
            "[Training Epoch 60] Batch 1039, Loss 0.2584408223628998\n",
            "[Training Epoch 60] Batch 1040, Loss 0.2514209449291229\n",
            "[Training Epoch 60] Batch 1041, Loss 0.2582392692565918\n",
            "[Training Epoch 60] Batch 1042, Loss 0.23693335056304932\n",
            "[Training Epoch 60] Batch 1043, Loss 0.25768378376960754\n",
            "[Training Epoch 60] Batch 1044, Loss 0.2541450262069702\n",
            "[Training Epoch 60] Batch 1045, Loss 0.30255043506622314\n",
            "[Training Epoch 60] Batch 1046, Loss 0.2538946270942688\n",
            "[Training Epoch 60] Batch 1047, Loss 0.3180273175239563\n",
            "[Training Epoch 60] Batch 1048, Loss 0.2551732361316681\n",
            "[Training Epoch 60] Batch 1049, Loss 0.26219528913497925\n",
            "[Training Epoch 60] Batch 1050, Loss 0.2740386128425598\n",
            "[Training Epoch 60] Batch 1051, Loss 0.2596972584724426\n",
            "[Training Epoch 60] Batch 1052, Loss 0.2667160928249359\n",
            "[Training Epoch 60] Batch 1053, Loss 0.25904083251953125\n",
            "[Training Epoch 60] Batch 1054, Loss 0.2726340591907501\n",
            "[Training Epoch 60] Batch 1055, Loss 0.25237253308296204\n",
            "[Training Epoch 60] Batch 1056, Loss 0.27281081676483154\n",
            "[Training Epoch 60] Batch 1057, Loss 0.2624543011188507\n",
            "[Training Epoch 60] Batch 1058, Loss 0.2709461450576782\n",
            "[Training Epoch 60] Batch 1059, Loss 0.23871192336082458\n",
            "[Training Epoch 60] Batch 1060, Loss 0.24513232707977295\n",
            "[Training Epoch 60] Batch 1061, Loss 0.2875441908836365\n",
            "[Training Epoch 60] Batch 1062, Loss 0.2647537887096405\n",
            "[Training Epoch 60] Batch 1063, Loss 0.26203134655952454\n",
            "[Training Epoch 60] Batch 1064, Loss 0.24957896769046783\n",
            "[Training Epoch 60] Batch 1065, Loss 0.26352551579475403\n",
            "[Training Epoch 60] Batch 1066, Loss 0.251969575881958\n",
            "[Training Epoch 60] Batch 1067, Loss 0.2532288730144501\n",
            "[Training Epoch 60] Batch 1068, Loss 0.26308637857437134\n",
            "[Training Epoch 60] Batch 1069, Loss 0.2717973291873932\n",
            "[Training Epoch 60] Batch 1070, Loss 0.28740596771240234\n",
            "[Training Epoch 60] Batch 1071, Loss 0.28407227993011475\n",
            "[Training Epoch 60] Batch 1072, Loss 0.2779167592525482\n",
            "[Training Epoch 60] Batch 1073, Loss 0.22664619982242584\n",
            "[Training Epoch 60] Batch 1074, Loss 0.2610069811344147\n",
            "[Training Epoch 60] Batch 1075, Loss 0.2508116364479065\n",
            "[Training Epoch 60] Batch 1076, Loss 0.2582979202270508\n",
            "[Training Epoch 60] Batch 1077, Loss 0.2890976667404175\n",
            "[Training Epoch 60] Batch 1078, Loss 0.24318471550941467\n",
            "[Training Epoch 60] Batch 1079, Loss 0.2423127442598343\n",
            "[Training Epoch 60] Batch 1080, Loss 0.27645057439804077\n",
            "[Training Epoch 60] Batch 1081, Loss 0.277345210313797\n",
            "[Training Epoch 60] Batch 1082, Loss 0.2726868689060211\n",
            "[Training Epoch 60] Batch 1083, Loss 0.2533107399940491\n",
            "[Training Epoch 60] Batch 1084, Loss 0.28366461396217346\n",
            "[Training Epoch 60] Batch 1085, Loss 0.24540507793426514\n",
            "[Training Epoch 60] Batch 1086, Loss 0.26930320262908936\n",
            "[Training Epoch 60] Batch 1087, Loss 0.2423027902841568\n",
            "[Training Epoch 60] Batch 1088, Loss 0.24029895663261414\n",
            "[Training Epoch 60] Batch 1089, Loss 0.27016398310661316\n",
            "[Training Epoch 60] Batch 1090, Loss 0.26947981119155884\n",
            "[Training Epoch 60] Batch 1091, Loss 0.2801331579685211\n",
            "[Training Epoch 60] Batch 1092, Loss 0.26004987955093384\n",
            "[Training Epoch 60] Batch 1093, Loss 0.26014095544815063\n",
            "[Training Epoch 60] Batch 1094, Loss 0.27097752690315247\n",
            "[Training Epoch 60] Batch 1095, Loss 0.26796063780784607\n",
            "[Training Epoch 60] Batch 1096, Loss 0.2813936471939087\n",
            "[Training Epoch 60] Batch 1097, Loss 0.26554304361343384\n",
            "[Training Epoch 60] Batch 1098, Loss 0.2946431040763855\n",
            "[Training Epoch 60] Batch 1099, Loss 0.2612273693084717\n",
            "[Training Epoch 60] Batch 1100, Loss 0.24233967065811157\n",
            "[Training Epoch 60] Batch 1101, Loss 0.25556811690330505\n",
            "[Training Epoch 60] Batch 1102, Loss 0.270929217338562\n",
            "[Training Epoch 60] Batch 1103, Loss 0.2614114582538605\n",
            "[Training Epoch 60] Batch 1104, Loss 0.25982218980789185\n",
            "[Training Epoch 60] Batch 1105, Loss 0.24948257207870483\n",
            "[Training Epoch 60] Batch 1106, Loss 0.2561611533164978\n",
            "[Training Epoch 60] Batch 1107, Loss 0.23157432675361633\n",
            "[Training Epoch 60] Batch 1108, Loss 0.26898014545440674\n",
            "[Training Epoch 60] Batch 1109, Loss 0.2776729166507721\n",
            "[Training Epoch 60] Batch 1110, Loss 0.24285975098609924\n",
            "[Training Epoch 60] Batch 1111, Loss 0.2764062285423279\n",
            "[Training Epoch 60] Batch 1112, Loss 0.23215536773204803\n",
            "[Training Epoch 60] Batch 1113, Loss 0.24990564584732056\n",
            "[Training Epoch 60] Batch 1114, Loss 0.24476978182792664\n",
            "[Training Epoch 60] Batch 1115, Loss 0.24818310141563416\n",
            "[Training Epoch 60] Batch 1116, Loss 0.27245160937309265\n",
            "[Training Epoch 60] Batch 1117, Loss 0.28423792123794556\n",
            "[Training Epoch 60] Batch 1118, Loss 0.2509463429450989\n",
            "[Training Epoch 60] Batch 1119, Loss 0.2588810920715332\n",
            "[Training Epoch 60] Batch 1120, Loss 0.24091655015945435\n",
            "[Training Epoch 60] Batch 1121, Loss 0.2451084852218628\n",
            "[Training Epoch 60] Batch 1122, Loss 0.27474069595336914\n",
            "[Training Epoch 60] Batch 1123, Loss 0.2819766402244568\n",
            "[Training Epoch 60] Batch 1124, Loss 0.24977868795394897\n",
            "[Training Epoch 60] Batch 1125, Loss 0.2695530652999878\n",
            "[Training Epoch 60] Batch 1126, Loss 0.2584052085876465\n",
            "[Training Epoch 60] Batch 1127, Loss 0.2646351754665375\n",
            "[Training Epoch 60] Batch 1128, Loss 0.24299341440200806\n",
            "[Training Epoch 60] Batch 1129, Loss 0.27162978053092957\n",
            "[Training Epoch 60] Batch 1130, Loss 0.27566757798194885\n",
            "[Training Epoch 60] Batch 1131, Loss 0.2374866008758545\n",
            "[Training Epoch 60] Batch 1132, Loss 0.28098800778388977\n",
            "[Training Epoch 60] Batch 1133, Loss 0.22025009989738464\n",
            "[Training Epoch 60] Batch 1134, Loss 0.27359825372695923\n",
            "[Training Epoch 60] Batch 1135, Loss 0.2733153700828552\n",
            "[Training Epoch 60] Batch 1136, Loss 0.2586129307746887\n",
            "[Training Epoch 60] Batch 1137, Loss 0.2531265616416931\n",
            "[Training Epoch 60] Batch 1138, Loss 0.25615960359573364\n",
            "[Training Epoch 60] Batch 1139, Loss 0.25921934843063354\n",
            "[Training Epoch 60] Batch 1140, Loss 0.27688083052635193\n",
            "[Training Epoch 60] Batch 1141, Loss 0.2622530162334442\n",
            "[Training Epoch 60] Batch 1142, Loss 0.24827668070793152\n",
            "[Training Epoch 60] Batch 1143, Loss 0.26047176122665405\n",
            "[Training Epoch 60] Batch 1144, Loss 0.24649740755558014\n",
            "[Training Epoch 60] Batch 1145, Loss 0.27521663904190063\n",
            "[Training Epoch 60] Batch 1146, Loss 0.2500867247581482\n",
            "[Training Epoch 60] Batch 1147, Loss 0.26344794034957886\n",
            "[Training Epoch 60] Batch 1148, Loss 0.24478521943092346\n",
            "[Training Epoch 60] Batch 1149, Loss 0.2434328943490982\n",
            "[Training Epoch 60] Batch 1150, Loss 0.2506808340549469\n",
            "[Training Epoch 60] Batch 1151, Loss 0.2723850607872009\n",
            "[Training Epoch 60] Batch 1152, Loss 0.2776761054992676\n",
            "[Training Epoch 60] Batch 1153, Loss 0.2541431188583374\n",
            "[Training Epoch 60] Batch 1154, Loss 0.26207607984542847\n",
            "[Training Epoch 60] Batch 1155, Loss 0.24391230940818787\n",
            "[Training Epoch 60] Batch 1156, Loss 0.23784002661705017\n",
            "[Training Epoch 60] Batch 1157, Loss 0.26623839139938354\n",
            "[Training Epoch 60] Batch 1158, Loss 0.2665719985961914\n",
            "[Training Epoch 60] Batch 1159, Loss 0.282000333070755\n",
            "[Training Epoch 60] Batch 1160, Loss 0.24848505854606628\n",
            "[Training Epoch 60] Batch 1161, Loss 0.25420209765434265\n",
            "[Training Epoch 60] Batch 1162, Loss 0.25033560395240784\n",
            "[Training Epoch 60] Batch 1163, Loss 0.2514028549194336\n",
            "[Training Epoch 60] Batch 1164, Loss 0.26764780282974243\n",
            "[Training Epoch 60] Batch 1165, Loss 0.3000156879425049\n",
            "[Training Epoch 60] Batch 1166, Loss 0.28098616003990173\n",
            "[Training Epoch 60] Batch 1167, Loss 0.25929561257362366\n",
            "[Training Epoch 60] Batch 1168, Loss 0.2407975047826767\n",
            "[Training Epoch 60] Batch 1169, Loss 0.2492048591375351\n",
            "[Training Epoch 60] Batch 1170, Loss 0.26058608293533325\n",
            "[Training Epoch 60] Batch 1171, Loss 0.2613641023635864\n",
            "[Training Epoch 60] Batch 1172, Loss 0.24662861227989197\n",
            "[Training Epoch 60] Batch 1173, Loss 0.28885143995285034\n",
            "[Training Epoch 60] Batch 1174, Loss 0.2522872984409332\n",
            "[Training Epoch 60] Batch 1175, Loss 0.2328043431043625\n",
            "[Training Epoch 60] Batch 1176, Loss 0.2608814835548401\n",
            "[Training Epoch 60] Batch 1177, Loss 0.2845444679260254\n",
            "[Training Epoch 60] Batch 1178, Loss 0.27661409974098206\n",
            "[Training Epoch 60] Batch 1179, Loss 0.251247763633728\n",
            "[Training Epoch 60] Batch 1180, Loss 0.2220464050769806\n",
            "[Training Epoch 60] Batch 1181, Loss 0.26495039463043213\n",
            "[Training Epoch 60] Batch 1182, Loss 0.27181220054626465\n",
            "[Training Epoch 60] Batch 1183, Loss 0.26736053824424744\n",
            "[Training Epoch 60] Batch 1184, Loss 0.2389354705810547\n",
            "[Training Epoch 60] Batch 1185, Loss 0.2615903317928314\n",
            "[Training Epoch 60] Batch 1186, Loss 0.2652429938316345\n",
            "[Training Epoch 60] Batch 1187, Loss 0.22695742547512054\n",
            "[Training Epoch 60] Batch 1188, Loss 0.2784656584262848\n",
            "[Training Epoch 60] Batch 1189, Loss 0.27777785062789917\n",
            "[Training Epoch 60] Batch 1190, Loss 0.26874178647994995\n",
            "[Training Epoch 60] Batch 1191, Loss 0.23126859962940216\n",
            "[Training Epoch 60] Batch 1192, Loss 0.2749224901199341\n",
            "[Training Epoch 60] Batch 1193, Loss 0.23793071508407593\n",
            "[Training Epoch 60] Batch 1194, Loss 0.27085357904434204\n",
            "[Training Epoch 60] Batch 1195, Loss 0.26510339975357056\n",
            "[Training Epoch 60] Batch 1196, Loss 0.2710961103439331\n",
            "[Training Epoch 60] Batch 1197, Loss 0.2666357159614563\n",
            "[Training Epoch 60] Batch 1198, Loss 0.2595444321632385\n",
            "[Training Epoch 60] Batch 1199, Loss 0.2428894340991974\n",
            "[Training Epoch 60] Batch 1200, Loss 0.2523050606250763\n",
            "[Training Epoch 60] Batch 1201, Loss 0.261738121509552\n",
            "[Training Epoch 60] Batch 1202, Loss 0.2416258603334427\n",
            "[Training Epoch 60] Batch 1203, Loss 0.2563890814781189\n",
            "[Training Epoch 60] Batch 1204, Loss 0.258195161819458\n",
            "[Training Epoch 60] Batch 1205, Loss 0.2563428580760956\n",
            "[Training Epoch 60] Batch 1206, Loss 0.25581881403923035\n",
            "[Training Epoch 60] Batch 1207, Loss 0.27481016516685486\n",
            "[Training Epoch 60] Batch 1208, Loss 0.27206191420555115\n",
            "[Training Epoch 60] Batch 1209, Loss 0.26383480429649353\n",
            "[Training Epoch 60] Batch 1210, Loss 0.23586025834083557\n",
            "[Training Epoch 60] Batch 1211, Loss 0.2549798786640167\n",
            "[Training Epoch 60] Batch 1212, Loss 0.2560960650444031\n",
            "[Training Epoch 60] Batch 1213, Loss 0.2940518856048584\n",
            "[Training Epoch 60] Batch 1214, Loss 0.28420406579971313\n",
            "[Training Epoch 60] Batch 1215, Loss 0.24549150466918945\n",
            "[Training Epoch 60] Batch 1216, Loss 0.27841290831565857\n",
            "[Training Epoch 60] Batch 1217, Loss 0.24365684390068054\n",
            "[Training Epoch 60] Batch 1218, Loss 0.2840936779975891\n",
            "[Training Epoch 60] Batch 1219, Loss 0.28546246886253357\n",
            "[Training Epoch 60] Batch 1220, Loss 0.25551483035087585\n",
            "[Training Epoch 60] Batch 1221, Loss 0.2768394947052002\n",
            "[Training Epoch 60] Batch 1222, Loss 0.25379684567451477\n",
            "[Training Epoch 60] Batch 1223, Loss 0.2390148639678955\n",
            "[Training Epoch 60] Batch 1224, Loss 0.2546215355396271\n",
            "[Training Epoch 60] Batch 1225, Loss 0.23838520050048828\n",
            "[Training Epoch 60] Batch 1226, Loss 0.3009084165096283\n",
            "[Training Epoch 60] Batch 1227, Loss 0.25930285453796387\n",
            "[Training Epoch 60] Batch 1228, Loss 0.26518160104751587\n",
            "[Training Epoch 60] Batch 1229, Loss 0.22343149781227112\n",
            "[Training Epoch 60] Batch 1230, Loss 0.24075016379356384\n",
            "[Training Epoch 60] Batch 1231, Loss 0.24254660308361053\n",
            "[Training Epoch 60] Batch 1232, Loss 0.2542072832584381\n",
            "[Training Epoch 60] Batch 1233, Loss 0.25632309913635254\n",
            "[Training Epoch 60] Batch 1234, Loss 0.2615054249763489\n",
            "[Training Epoch 60] Batch 1235, Loss 0.2757718563079834\n",
            "[Training Epoch 60] Batch 1236, Loss 0.24259209632873535\n",
            "[Training Epoch 60] Batch 1237, Loss 0.27789175510406494\n",
            "[Training Epoch 60] Batch 1238, Loss 0.2811981737613678\n",
            "[Training Epoch 60] Batch 1239, Loss 0.2567470967769623\n",
            "[Training Epoch 60] Batch 1240, Loss 0.2541775703430176\n",
            "[Training Epoch 60] Batch 1241, Loss 0.26606443524360657\n",
            "[Training Epoch 60] Batch 1242, Loss 0.2851637899875641\n",
            "[Training Epoch 60] Batch 1243, Loss 0.2619014084339142\n",
            "[Training Epoch 60] Batch 1244, Loss 0.2523338794708252\n",
            "[Training Epoch 60] Batch 1245, Loss 0.26325714588165283\n",
            "[Training Epoch 60] Batch 1246, Loss 0.2829340696334839\n",
            "[Training Epoch 60] Batch 1247, Loss 0.2467874139547348\n",
            "[Training Epoch 60] Batch 1248, Loss 0.27346402406692505\n",
            "[Training Epoch 60] Batch 1249, Loss 0.25229698419570923\n",
            "[Training Epoch 60] Batch 1250, Loss 0.27970626950263977\n",
            "[Training Epoch 60] Batch 1251, Loss 0.2549176812171936\n",
            "[Training Epoch 60] Batch 1252, Loss 0.28665435314178467\n",
            "[Training Epoch 60] Batch 1253, Loss 0.277665913105011\n",
            "[Training Epoch 60] Batch 1254, Loss 0.29557961225509644\n",
            "[Training Epoch 60] Batch 1255, Loss 0.25680825114250183\n",
            "[Training Epoch 60] Batch 1256, Loss 0.2863085865974426\n",
            "[Training Epoch 60] Batch 1257, Loss 0.25140032172203064\n",
            "[Training Epoch 60] Batch 1258, Loss 0.2811031639575958\n",
            "[Training Epoch 60] Batch 1259, Loss 0.2641286849975586\n",
            "[Training Epoch 60] Batch 1260, Loss 0.2504402995109558\n",
            "[Training Epoch 60] Batch 1261, Loss 0.2825266122817993\n",
            "[Training Epoch 60] Batch 1262, Loss 0.2589987814426422\n",
            "[Training Epoch 60] Batch 1263, Loss 0.2732636332511902\n",
            "[Training Epoch 60] Batch 1264, Loss 0.2513434886932373\n",
            "[Training Epoch 60] Batch 1265, Loss 0.2580534517765045\n",
            "[Training Epoch 60] Batch 1266, Loss 0.2796875238418579\n",
            "[Training Epoch 60] Batch 1267, Loss 0.2462458610534668\n",
            "[Training Epoch 60] Batch 1268, Loss 0.22763721644878387\n",
            "[Training Epoch 60] Batch 1269, Loss 0.2467653900384903\n",
            "[Training Epoch 60] Batch 1270, Loss 0.26069435477256775\n",
            "[Training Epoch 60] Batch 1271, Loss 0.2750180959701538\n",
            "[Training Epoch 60] Batch 1272, Loss 0.2595062255859375\n",
            "[Training Epoch 60] Batch 1273, Loss 0.26748019456863403\n",
            "[Training Epoch 60] Batch 1274, Loss 0.2599394917488098\n",
            "[Training Epoch 60] Batch 1275, Loss 0.2760685682296753\n",
            "[Training Epoch 60] Batch 1276, Loss 0.25577735900878906\n",
            "[Training Epoch 60] Batch 1277, Loss 0.26427608728408813\n",
            "[Training Epoch 60] Batch 1278, Loss 0.2800638675689697\n",
            "[Training Epoch 60] Batch 1279, Loss 0.2507871985435486\n",
            "[Training Epoch 60] Batch 1280, Loss 0.2785772979259491\n",
            "[Training Epoch 60] Batch 1281, Loss 0.2463904172182083\n",
            "[Training Epoch 60] Batch 1282, Loss 0.2633671462535858\n",
            "[Training Epoch 60] Batch 1283, Loss 0.2845500409603119\n",
            "[Training Epoch 60] Batch 1284, Loss 0.249506413936615\n",
            "[Training Epoch 60] Batch 1285, Loss 0.2921292781829834\n",
            "[Training Epoch 60] Batch 1286, Loss 0.23893378674983978\n",
            "[Training Epoch 60] Batch 1287, Loss 0.27740901708602905\n",
            "[Training Epoch 60] Batch 1288, Loss 0.26500558853149414\n",
            "[Training Epoch 60] Batch 1289, Loss 0.2773016095161438\n",
            "[Training Epoch 60] Batch 1290, Loss 0.26433899998664856\n",
            "[Training Epoch 60] Batch 1291, Loss 0.27759358286857605\n",
            "[Training Epoch 60] Batch 1292, Loss 0.24927286803722382\n",
            "[Training Epoch 60] Batch 1293, Loss 0.2894163131713867\n",
            "[Training Epoch 60] Batch 1294, Loss 0.2689184844493866\n",
            "[Training Epoch 60] Batch 1295, Loss 0.24418601393699646\n",
            "[Training Epoch 60] Batch 1296, Loss 0.2383662760257721\n",
            "[Training Epoch 60] Batch 1297, Loss 0.2644471526145935\n",
            "[Training Epoch 60] Batch 1298, Loss 0.2534552812576294\n",
            "[Training Epoch 60] Batch 1299, Loss 0.2549324631690979\n",
            "[Training Epoch 60] Batch 1300, Loss 0.28205007314682007\n",
            "[Training Epoch 60] Batch 1301, Loss 0.24306154251098633\n",
            "[Training Epoch 60] Batch 1302, Loss 0.2712163031101227\n",
            "[Training Epoch 60] Batch 1303, Loss 0.24852244555950165\n",
            "[Training Epoch 60] Batch 1304, Loss 0.2778480350971222\n",
            "[Training Epoch 60] Batch 1305, Loss 0.26810508966445923\n",
            "[Training Epoch 60] Batch 1306, Loss 0.26694196462631226\n",
            "[Training Epoch 60] Batch 1307, Loss 0.2695137858390808\n",
            "[Training Epoch 60] Batch 1308, Loss 0.2831602394580841\n",
            "[Training Epoch 60] Batch 1309, Loss 0.26466667652130127\n",
            "[Training Epoch 60] Batch 1310, Loss 0.28284022212028503\n",
            "[Training Epoch 60] Batch 1311, Loss 0.24806392192840576\n",
            "[Training Epoch 60] Batch 1312, Loss 0.2900361716747284\n",
            "[Training Epoch 60] Batch 1313, Loss 0.2715027928352356\n",
            "[Training Epoch 60] Batch 1314, Loss 0.2749084532260895\n",
            "[Training Epoch 60] Batch 1315, Loss 0.2720067501068115\n",
            "[Training Epoch 60] Batch 1316, Loss 0.23525086045265198\n",
            "[Training Epoch 60] Batch 1317, Loss 0.27163729071617126\n",
            "[Training Epoch 60] Batch 1318, Loss 0.2634517550468445\n",
            "[Training Epoch 60] Batch 1319, Loss 0.28439679741859436\n",
            "[Training Epoch 60] Batch 1320, Loss 0.24283137917518616\n",
            "[Training Epoch 60] Batch 1321, Loss 0.2674962282180786\n",
            "[Training Epoch 60] Batch 1322, Loss 0.2745570242404938\n",
            "[Training Epoch 60] Batch 1323, Loss 0.23199231922626495\n",
            "[Training Epoch 60] Batch 1324, Loss 0.24793288111686707\n",
            "[Training Epoch 60] Batch 1325, Loss 0.2605704665184021\n",
            "[Training Epoch 60] Batch 1326, Loss 0.27617090940475464\n",
            "[Training Epoch 60] Batch 1327, Loss 0.26848742365837097\n",
            "[Training Epoch 60] Batch 1328, Loss 0.2829320728778839\n",
            "[Training Epoch 60] Batch 1329, Loss 0.2630397081375122\n",
            "[Training Epoch 60] Batch 1330, Loss 0.23809556663036346\n",
            "[Training Epoch 60] Batch 1331, Loss 0.2579571008682251\n",
            "[Training Epoch 60] Batch 1332, Loss 0.2602359652519226\n",
            "[Training Epoch 60] Batch 1333, Loss 0.28501665592193604\n",
            "[Training Epoch 60] Batch 1334, Loss 0.28969576954841614\n",
            "[Training Epoch 60] Batch 1335, Loss 0.2692107856273651\n",
            "[Training Epoch 60] Batch 1336, Loss 0.25547465682029724\n",
            "[Training Epoch 60] Batch 1337, Loss 0.2893006205558777\n",
            "[Training Epoch 60] Batch 1338, Loss 0.261277973651886\n",
            "[Training Epoch 60] Batch 1339, Loss 0.2505105435848236\n",
            "[Training Epoch 60] Batch 1340, Loss 0.2758635878562927\n",
            "[Training Epoch 60] Batch 1341, Loss 0.24850121140480042\n",
            "[Training Epoch 60] Batch 1342, Loss 0.2794927656650543\n",
            "[Training Epoch 60] Batch 1343, Loss 0.26070430874824524\n",
            "[Training Epoch 60] Batch 1344, Loss 0.2712389826774597\n",
            "[Training Epoch 60] Batch 1345, Loss 0.2736976742744446\n",
            "[Training Epoch 60] Batch 1346, Loss 0.2710723876953125\n",
            "[Training Epoch 60] Batch 1347, Loss 0.2557530701160431\n",
            "[Training Epoch 60] Batch 1348, Loss 0.2632591128349304\n",
            "[Training Epoch 60] Batch 1349, Loss 0.29955580830574036\n",
            "[Training Epoch 60] Batch 1350, Loss 0.29254063963890076\n",
            "[Training Epoch 60] Batch 1351, Loss 0.25317203998565674\n",
            "[Training Epoch 60] Batch 1352, Loss 0.29555824398994446\n",
            "[Training Epoch 60] Batch 1353, Loss 0.2562410235404968\n",
            "[Training Epoch 60] Batch 1354, Loss 0.28121596574783325\n",
            "[Training Epoch 60] Batch 1355, Loss 0.2690286934375763\n",
            "[Training Epoch 60] Batch 1356, Loss 0.2384408563375473\n",
            "[Training Epoch 60] Batch 1357, Loss 0.25057244300842285\n",
            "[Training Epoch 60] Batch 1358, Loss 0.2654382586479187\n",
            "[Training Epoch 60] Batch 1359, Loss 0.23426394164562225\n",
            "[Training Epoch 60] Batch 1360, Loss 0.2500714063644409\n",
            "[Training Epoch 60] Batch 1361, Loss 0.2492765188217163\n",
            "[Training Epoch 60] Batch 1362, Loss 0.24243387579917908\n",
            "[Training Epoch 60] Batch 1363, Loss 0.26281338930130005\n",
            "[Training Epoch 60] Batch 1364, Loss 0.25652605295181274\n",
            "[Training Epoch 60] Batch 1365, Loss 0.23000869154930115\n",
            "[Training Epoch 60] Batch 1366, Loss 0.26410770416259766\n",
            "[Training Epoch 60] Batch 1367, Loss 0.27733156085014343\n",
            "[Training Epoch 60] Batch 1368, Loss 0.24388140439987183\n",
            "[Training Epoch 60] Batch 1369, Loss 0.3090532124042511\n",
            "[Training Epoch 60] Batch 1370, Loss 0.27655696868896484\n",
            "[Training Epoch 60] Batch 1371, Loss 0.2799210846424103\n",
            "[Training Epoch 60] Batch 1372, Loss 0.2888474464416504\n",
            "[Training Epoch 60] Batch 1373, Loss 0.2843018174171448\n",
            "[Training Epoch 60] Batch 1374, Loss 0.23322591185569763\n",
            "[Training Epoch 60] Batch 1375, Loss 0.24320945143699646\n",
            "[Training Epoch 60] Batch 1376, Loss 0.2643078863620758\n",
            "[Training Epoch 60] Batch 1377, Loss 0.26467397809028625\n",
            "[Training Epoch 60] Batch 1378, Loss 0.2618827223777771\n",
            "[Training Epoch 60] Batch 1379, Loss 0.2889467179775238\n",
            "[Training Epoch 60] Batch 1380, Loss 0.2693914771080017\n",
            "[Training Epoch 60] Batch 1381, Loss 0.28326216340065\n",
            "[Training Epoch 60] Batch 1382, Loss 0.2953181564807892\n",
            "[Training Epoch 60] Batch 1383, Loss 0.224863201379776\n",
            "[Training Epoch 60] Batch 1384, Loss 0.282043993473053\n",
            "[Training Epoch 60] Batch 1385, Loss 0.27171531319618225\n",
            "[Training Epoch 60] Batch 1386, Loss 0.2651047110557556\n",
            "[Training Epoch 60] Batch 1387, Loss 0.25388795137405396\n",
            "[Training Epoch 60] Batch 1388, Loss 0.26307615637779236\n",
            "[Training Epoch 60] Batch 1389, Loss 0.2608127295970917\n",
            "[Training Epoch 60] Batch 1390, Loss 0.2728497087955475\n",
            "[Training Epoch 60] Batch 1391, Loss 0.25932180881500244\n",
            "[Training Epoch 60] Batch 1392, Loss 0.27110639214515686\n",
            "[Training Epoch 60] Batch 1393, Loss 0.28750312328338623\n",
            "[Training Epoch 60] Batch 1394, Loss 0.2429632544517517\n",
            "[Training Epoch 60] Batch 1395, Loss 0.2696525752544403\n",
            "[Training Epoch 60] Batch 1396, Loss 0.2923549711704254\n",
            "[Training Epoch 60] Batch 1397, Loss 0.24392183125019073\n",
            "[Training Epoch 60] Batch 1398, Loss 0.27137142419815063\n",
            "[Training Epoch 60] Batch 1399, Loss 0.2686331868171692\n",
            "[Training Epoch 60] Batch 1400, Loss 0.2484622448682785\n",
            "[Training Epoch 60] Batch 1401, Loss 0.25047600269317627\n",
            "[Training Epoch 60] Batch 1402, Loss 0.2555500864982605\n",
            "[Training Epoch 60] Batch 1403, Loss 0.26411721110343933\n",
            "[Training Epoch 60] Batch 1404, Loss 0.2791237533092499\n",
            "[Training Epoch 60] Batch 1405, Loss 0.25120866298675537\n",
            "[Training Epoch 60] Batch 1406, Loss 0.29194021224975586\n",
            "[Training Epoch 60] Batch 1407, Loss 0.27598169445991516\n",
            "[Training Epoch 60] Batch 1408, Loss 0.2662893533706665\n",
            "[Training Epoch 60] Batch 1409, Loss 0.2674202024936676\n",
            "[Training Epoch 60] Batch 1410, Loss 0.24277597665786743\n",
            "[Training Epoch 60] Batch 1411, Loss 0.2856610417366028\n",
            "[Training Epoch 60] Batch 1412, Loss 0.24567314982414246\n",
            "[Training Epoch 60] Batch 1413, Loss 0.24447116255760193\n",
            "[Training Epoch 60] Batch 1414, Loss 0.26129141449928284\n",
            "[Training Epoch 60] Batch 1415, Loss 0.25270676612854004\n",
            "[Training Epoch 60] Batch 1416, Loss 0.22077301144599915\n",
            "[Training Epoch 60] Batch 1417, Loss 0.26987987756729126\n",
            "[Training Epoch 60] Batch 1418, Loss 0.2557181417942047\n",
            "[Training Epoch 60] Batch 1419, Loss 0.2718271017074585\n",
            "[Training Epoch 60] Batch 1420, Loss 0.27928784489631653\n",
            "[Training Epoch 60] Batch 1421, Loss 0.2581275403499603\n",
            "[Training Epoch 60] Batch 1422, Loss 0.2590724229812622\n",
            "[Training Epoch 60] Batch 1423, Loss 0.28879407048225403\n",
            "[Training Epoch 60] Batch 1424, Loss 0.2865489721298218\n",
            "[Training Epoch 60] Batch 1425, Loss 0.2764674723148346\n",
            "[Training Epoch 60] Batch 1426, Loss 0.27093955874443054\n",
            "[Training Epoch 60] Batch 1427, Loss 0.26997309923171997\n",
            "[Training Epoch 60] Batch 1428, Loss 0.2599394917488098\n",
            "[Training Epoch 60] Batch 1429, Loss 0.25882071256637573\n",
            "[Training Epoch 60] Batch 1430, Loss 0.258253812789917\n",
            "[Training Epoch 60] Batch 1431, Loss 0.2623031735420227\n",
            "[Training Epoch 60] Batch 1432, Loss 0.26976478099823\n",
            "[Training Epoch 60] Batch 1433, Loss 0.2645973265171051\n",
            "[Training Epoch 60] Batch 1434, Loss 0.28167328238487244\n",
            "[Training Epoch 60] Batch 1435, Loss 0.2821301221847534\n",
            "[Training Epoch 60] Batch 1436, Loss 0.26220595836639404\n",
            "[Training Epoch 60] Batch 1437, Loss 0.2904934287071228\n",
            "[Training Epoch 60] Batch 1438, Loss 0.2720547020435333\n",
            "[Training Epoch 60] Batch 1439, Loss 0.2497791200876236\n",
            "[Training Epoch 60] Batch 1440, Loss 0.2914620041847229\n",
            "[Training Epoch 60] Batch 1441, Loss 0.24651940166950226\n",
            "[Training Epoch 60] Batch 1442, Loss 0.25935471057891846\n",
            "[Training Epoch 60] Batch 1443, Loss 0.24191290140151978\n",
            "[Training Epoch 60] Batch 1444, Loss 0.2855871617794037\n",
            "[Training Epoch 60] Batch 1445, Loss 0.2825356423854828\n",
            "[Training Epoch 60] Batch 1446, Loss 0.2494371235370636\n",
            "[Training Epoch 60] Batch 1447, Loss 0.24064776301383972\n",
            "[Training Epoch 60] Batch 1448, Loss 0.27909570932388306\n",
            "[Training Epoch 60] Batch 1449, Loss 0.2665763795375824\n",
            "[Training Epoch 60] Batch 1450, Loss 0.28101861476898193\n",
            "[Training Epoch 60] Batch 1451, Loss 0.24266935884952545\n",
            "[Training Epoch 60] Batch 1452, Loss 0.28011009097099304\n",
            "[Training Epoch 60] Batch 1453, Loss 0.27566567063331604\n",
            "[Training Epoch 60] Batch 1454, Loss 0.29156494140625\n",
            "[Training Epoch 60] Batch 1455, Loss 0.25621116161346436\n",
            "[Training Epoch 60] Batch 1456, Loss 0.2522350549697876\n",
            "[Training Epoch 60] Batch 1457, Loss 0.22803087532520294\n",
            "[Training Epoch 60] Batch 1458, Loss 0.26030218601226807\n",
            "[Training Epoch 60] Batch 1459, Loss 0.23991884291172028\n",
            "[Training Epoch 60] Batch 1460, Loss 0.2794497013092041\n",
            "[Training Epoch 60] Batch 1461, Loss 0.2579905390739441\n",
            "[Training Epoch 60] Batch 1462, Loss 0.27610623836517334\n",
            "[Training Epoch 60] Batch 1463, Loss 0.25826704502105713\n",
            "[Training Epoch 60] Batch 1464, Loss 0.27453917264938354\n",
            "[Training Epoch 60] Batch 1465, Loss 0.253177672624588\n",
            "[Training Epoch 60] Batch 1466, Loss 0.2620903253555298\n",
            "[Training Epoch 60] Batch 1467, Loss 0.2747274339199066\n",
            "[Training Epoch 60] Batch 1468, Loss 0.2456098347902298\n",
            "[Training Epoch 60] Batch 1469, Loss 0.2523454427719116\n",
            "[Training Epoch 60] Batch 1470, Loss 0.2581317722797394\n",
            "[Training Epoch 60] Batch 1471, Loss 0.23544500768184662\n",
            "[Training Epoch 60] Batch 1472, Loss 0.2546286880970001\n",
            "[Training Epoch 60] Batch 1473, Loss 0.2680707573890686\n",
            "[Training Epoch 60] Batch 1474, Loss 0.26916348934173584\n",
            "[Training Epoch 60] Batch 1475, Loss 0.26135265827178955\n",
            "[Training Epoch 60] Batch 1476, Loss 0.2714439928531647\n",
            "[Training Epoch 60] Batch 1477, Loss 0.2660359740257263\n",
            "[Training Epoch 60] Batch 1478, Loss 0.28540295362472534\n",
            "[Training Epoch 60] Batch 1479, Loss 0.2713497281074524\n",
            "[Training Epoch 60] Batch 1480, Loss 0.26691576838493347\n",
            "[Training Epoch 60] Batch 1481, Loss 0.2693758010864258\n",
            "[Training Epoch 60] Batch 1482, Loss 0.23304668068885803\n",
            "[Training Epoch 60] Batch 1483, Loss 0.2772977650165558\n",
            "[Training Epoch 60] Batch 1484, Loss 0.2512839436531067\n",
            "[Training Epoch 60] Batch 1485, Loss 0.2554107904434204\n",
            "[Training Epoch 60] Batch 1486, Loss 0.2752228379249573\n",
            "[Training Epoch 60] Batch 1487, Loss 0.25829702615737915\n",
            "[Training Epoch 60] Batch 1488, Loss 0.27895650267601013\n",
            "[Training Epoch 60] Batch 1489, Loss 0.2540157437324524\n",
            "[Training Epoch 60] Batch 1490, Loss 0.2635798454284668\n",
            "[Training Epoch 60] Batch 1491, Loss 0.253024160861969\n",
            "[Training Epoch 60] Batch 1492, Loss 0.2811507284641266\n",
            "[Training Epoch 60] Batch 1493, Loss 0.2608430087566376\n",
            "[Training Epoch 60] Batch 1494, Loss 0.2613915801048279\n",
            "[Training Epoch 60] Batch 1495, Loss 0.2553444802761078\n",
            "[Training Epoch 60] Batch 1496, Loss 0.2679281532764435\n",
            "[Training Epoch 60] Batch 1497, Loss 0.28120675683021545\n",
            "[Training Epoch 60] Batch 1498, Loss 0.24727913737297058\n",
            "[Training Epoch 60] Batch 1499, Loss 0.2665964365005493\n",
            "[Training Epoch 60] Batch 1500, Loss 0.2670103907585144\n",
            "[Training Epoch 60] Batch 1501, Loss 0.2636633515357971\n",
            "[Training Epoch 60] Batch 1502, Loss 0.2570764422416687\n",
            "[Training Epoch 60] Batch 1503, Loss 0.2749737501144409\n",
            "[Training Epoch 60] Batch 1504, Loss 0.2463398277759552\n",
            "[Training Epoch 60] Batch 1505, Loss 0.255875825881958\n",
            "[Training Epoch 60] Batch 1506, Loss 0.2778266966342926\n",
            "[Training Epoch 60] Batch 1507, Loss 0.25867146253585815\n",
            "[Training Epoch 60] Batch 1508, Loss 0.27068138122558594\n",
            "[Training Epoch 60] Batch 1509, Loss 0.24772518873214722\n",
            "[Training Epoch 60] Batch 1510, Loss 0.24502286314964294\n",
            "[Training Epoch 60] Batch 1511, Loss 0.28518208861351013\n",
            "[Training Epoch 60] Batch 1512, Loss 0.24944397807121277\n",
            "[Training Epoch 60] Batch 1513, Loss 0.27896085381507874\n",
            "[Training Epoch 60] Batch 1514, Loss 0.2462596446275711\n",
            "[Training Epoch 60] Batch 1515, Loss 0.25884684920310974\n",
            "[Training Epoch 60] Batch 1516, Loss 0.26323458552360535\n",
            "[Training Epoch 60] Batch 1517, Loss 0.2641724646091461\n",
            "[Training Epoch 60] Batch 1518, Loss 0.2693033516407013\n",
            "[Training Epoch 60] Batch 1519, Loss 0.24499161541461945\n",
            "[Training Epoch 60] Batch 1520, Loss 0.26078611612319946\n",
            "[Training Epoch 60] Batch 1521, Loss 0.28800255060195923\n",
            "[Training Epoch 60] Batch 1522, Loss 0.26256000995635986\n",
            "[Training Epoch 60] Batch 1523, Loss 0.26457715034484863\n",
            "[Training Epoch 60] Batch 1524, Loss 0.24000900983810425\n",
            "[Training Epoch 60] Batch 1525, Loss 0.254630446434021\n",
            "[Training Epoch 60] Batch 1526, Loss 0.2293117791414261\n",
            "[Training Epoch 60] Batch 1527, Loss 0.2822209596633911\n",
            "[Training Epoch 60] Batch 1528, Loss 0.25912806391716003\n",
            "[Training Epoch 60] Batch 1529, Loss 0.26995784044265747\n",
            "[Training Epoch 60] Batch 1530, Loss 0.24285858869552612\n",
            "[Training Epoch 60] Batch 1531, Loss 0.27390432357788086\n",
            "[Training Epoch 60] Batch 1532, Loss 0.29808270931243896\n",
            "[Training Epoch 60] Batch 1533, Loss 0.2434276044368744\n",
            "[Training Epoch 60] Batch 1534, Loss 0.2755710184574127\n",
            "[Training Epoch 60] Batch 1535, Loss 0.2639864683151245\n",
            "[Training Epoch 60] Batch 1536, Loss 0.2576112449169159\n",
            "[Training Epoch 60] Batch 1537, Loss 0.2660265564918518\n",
            "[Training Epoch 60] Batch 1538, Loss 0.2770894169807434\n",
            "[Training Epoch 60] Batch 1539, Loss 0.2516108453273773\n",
            "[Training Epoch 60] Batch 1540, Loss 0.252703994512558\n",
            "[Training Epoch 60] Batch 1541, Loss 0.2562277019023895\n",
            "[Training Epoch 60] Batch 1542, Loss 0.2532060444355011\n",
            "[Training Epoch 60] Batch 1543, Loss 0.27625328302383423\n",
            "[Training Epoch 60] Batch 1544, Loss 0.24552832543849945\n",
            "[Training Epoch 60] Batch 1545, Loss 0.2729429602622986\n",
            "[Training Epoch 60] Batch 1546, Loss 0.27909499406814575\n",
            "[Training Epoch 60] Batch 1547, Loss 0.2766838073730469\n",
            "[Training Epoch 60] Batch 1548, Loss 0.23642593622207642\n",
            "[Training Epoch 60] Batch 1549, Loss 0.2592319846153259\n",
            "[Training Epoch 60] Batch 1550, Loss 0.24966081976890564\n",
            "[Training Epoch 60] Batch 1551, Loss 0.24141673743724823\n",
            "[Training Epoch 60] Batch 1552, Loss 0.2784249484539032\n",
            "[Training Epoch 60] Batch 1553, Loss 0.28093183040618896\n",
            "[Training Epoch 60] Batch 1554, Loss 0.23065811395645142\n",
            "[Training Epoch 60] Batch 1555, Loss 0.26256221532821655\n",
            "[Training Epoch 60] Batch 1556, Loss 0.2788214087486267\n",
            "[Training Epoch 60] Batch 1557, Loss 0.26483577489852905\n",
            "[Training Epoch 60] Batch 1558, Loss 0.2648610472679138\n",
            "[Training Epoch 60] Batch 1559, Loss 0.26155027747154236\n",
            "[Training Epoch 60] Batch 1560, Loss 0.2970544099807739\n",
            "[Training Epoch 60] Batch 1561, Loss 0.2795789837837219\n",
            "[Training Epoch 60] Batch 1562, Loss 0.27942463755607605\n",
            "[Training Epoch 60] Batch 1563, Loss 0.26797524094581604\n",
            "[Training Epoch 60] Batch 1564, Loss 0.2631366550922394\n",
            "[Training Epoch 60] Batch 1565, Loss 0.2584858536720276\n",
            "[Training Epoch 60] Batch 1566, Loss 0.30897465348243713\n",
            "[Training Epoch 60] Batch 1567, Loss 0.26170361042022705\n",
            "[Training Epoch 60] Batch 1568, Loss 0.2735729217529297\n",
            "[Training Epoch 60] Batch 1569, Loss 0.2574717104434967\n",
            "[Training Epoch 60] Batch 1570, Loss 0.2633090317249298\n",
            "[Training Epoch 60] Batch 1571, Loss 0.27538466453552246\n",
            "[Training Epoch 60] Batch 1572, Loss 0.29464414715766907\n",
            "[Training Epoch 60] Batch 1573, Loss 0.27157899737358093\n",
            "[Training Epoch 60] Batch 1574, Loss 0.24045106768608093\n",
            "[Training Epoch 60] Batch 1575, Loss 0.24904590845108032\n",
            "[Training Epoch 60] Batch 1576, Loss 0.2398281693458557\n",
            "[Training Epoch 60] Batch 1577, Loss 0.2619766891002655\n",
            "[Training Epoch 60] Batch 1578, Loss 0.276790052652359\n",
            "[Training Epoch 60] Batch 1579, Loss 0.2940201759338379\n",
            "[Training Epoch 60] Batch 1580, Loss 0.2620742619037628\n",
            "[Training Epoch 60] Batch 1581, Loss 0.2896410822868347\n",
            "[Training Epoch 60] Batch 1582, Loss 0.29273417592048645\n",
            "[Training Epoch 60] Batch 1583, Loss 0.2425985485315323\n",
            "[Training Epoch 60] Batch 1584, Loss 0.3000164031982422\n",
            "[Training Epoch 60] Batch 1585, Loss 0.280570924282074\n",
            "[Training Epoch 60] Batch 1586, Loss 0.2689448595046997\n",
            "[Training Epoch 60] Batch 1587, Loss 0.24830925464630127\n",
            "[Training Epoch 60] Batch 1588, Loss 0.2723102569580078\n",
            "[Training Epoch 60] Batch 1589, Loss 0.2852935791015625\n",
            "[Training Epoch 60] Batch 1590, Loss 0.2900603115558624\n",
            "[Training Epoch 60] Batch 1591, Loss 0.2609538733959198\n",
            "[Training Epoch 60] Batch 1592, Loss 0.25367629528045654\n",
            "[Training Epoch 60] Batch 1593, Loss 0.25786057114601135\n",
            "[Training Epoch 60] Batch 1594, Loss 0.2435167282819748\n",
            "[Training Epoch 60] Batch 1595, Loss 0.2661096155643463\n",
            "[Training Epoch 60] Batch 1596, Loss 0.25532835721969604\n",
            "[Training Epoch 60] Batch 1597, Loss 0.23378460109233856\n",
            "[Training Epoch 60] Batch 1598, Loss 0.2586892247200012\n",
            "[Training Epoch 60] Batch 1599, Loss 0.27116671204566956\n",
            "[Training Epoch 60] Batch 1600, Loss 0.26846617460250854\n",
            "[Training Epoch 60] Batch 1601, Loss 0.27652761340141296\n",
            "[Training Epoch 60] Batch 1602, Loss 0.26345857977867126\n",
            "[Training Epoch 60] Batch 1603, Loss 0.26600849628448486\n",
            "[Training Epoch 60] Batch 1604, Loss 0.2362254559993744\n",
            "[Training Epoch 60] Batch 1605, Loss 0.288818895816803\n",
            "[Training Epoch 60] Batch 1606, Loss 0.25444403290748596\n",
            "[Training Epoch 60] Batch 1607, Loss 0.2561785578727722\n",
            "[Training Epoch 60] Batch 1608, Loss 0.24263857305049896\n",
            "[Training Epoch 60] Batch 1609, Loss 0.287742555141449\n",
            "[Training Epoch 60] Batch 1610, Loss 0.27047964930534363\n",
            "[Training Epoch 60] Batch 1611, Loss 0.27935805916786194\n",
            "[Training Epoch 60] Batch 1612, Loss 0.2525325417518616\n",
            "[Training Epoch 60] Batch 1613, Loss 0.2786097526550293\n",
            "[Training Epoch 60] Batch 1614, Loss 0.2525903284549713\n",
            "[Training Epoch 60] Batch 1615, Loss 0.2658482789993286\n",
            "[Training Epoch 60] Batch 1616, Loss 0.2595565617084503\n",
            "[Training Epoch 60] Batch 1617, Loss 0.25856852531433105\n",
            "[Training Epoch 60] Batch 1618, Loss 0.253724604845047\n",
            "[Training Epoch 60] Batch 1619, Loss 0.29859673976898193\n",
            "[Training Epoch 60] Batch 1620, Loss 0.25519856810569763\n",
            "[Training Epoch 60] Batch 1621, Loss 0.25996968150138855\n",
            "[Training Epoch 60] Batch 1622, Loss 0.2866908311843872\n",
            "[Training Epoch 60] Batch 1623, Loss 0.2475336492061615\n",
            "[Training Epoch 60] Batch 1624, Loss 0.27242904901504517\n",
            "[Training Epoch 60] Batch 1625, Loss 0.29149314761161804\n",
            "[Training Epoch 60] Batch 1626, Loss 0.2335529327392578\n",
            "[Training Epoch 60] Batch 1627, Loss 0.255268394947052\n",
            "[Training Epoch 60] Batch 1628, Loss 0.24229881167411804\n",
            "[Training Epoch 60] Batch 1629, Loss 0.2766456604003906\n",
            "[Training Epoch 60] Batch 1630, Loss 0.2861344516277313\n",
            "[Training Epoch 60] Batch 1631, Loss 0.2628667950630188\n",
            "[Training Epoch 60] Batch 1632, Loss 0.29112374782562256\n",
            "[Training Epoch 60] Batch 1633, Loss 0.2520323395729065\n",
            "[Training Epoch 60] Batch 1634, Loss 0.29738613963127136\n",
            "[Training Epoch 60] Batch 1635, Loss 0.283655047416687\n",
            "[Training Epoch 60] Batch 1636, Loss 0.2623104453086853\n",
            "[Training Epoch 60] Batch 1637, Loss 0.27867284417152405\n",
            "[Training Epoch 60] Batch 1638, Loss 0.25195586681365967\n",
            "[Training Epoch 60] Batch 1639, Loss 0.25070029497146606\n",
            "[Training Epoch 60] Batch 1640, Loss 0.2764706313610077\n",
            "[Training Epoch 60] Batch 1641, Loss 0.28619831800460815\n",
            "[Training Epoch 60] Batch 1642, Loss 0.2588737905025482\n",
            "[Training Epoch 60] Batch 1643, Loss 0.27662819623947144\n",
            "[Training Epoch 60] Batch 1644, Loss 0.2385178655385971\n",
            "[Training Epoch 60] Batch 1645, Loss 0.27144208550453186\n",
            "[Training Epoch 60] Batch 1646, Loss 0.25866010785102844\n",
            "[Training Epoch 60] Batch 1647, Loss 0.26887792348861694\n",
            "[Training Epoch 60] Batch 1648, Loss 0.24851451814174652\n",
            "[Training Epoch 60] Batch 1649, Loss 0.26215946674346924\n",
            "[Training Epoch 60] Batch 1650, Loss 0.2684327960014343\n",
            "[Training Epoch 60] Batch 1651, Loss 0.24654094874858856\n",
            "[Training Epoch 60] Batch 1652, Loss 0.24257370829582214\n",
            "[Training Epoch 60] Batch 1653, Loss 0.2566288113594055\n",
            "[Training Epoch 60] Batch 1654, Loss 0.25267428159713745\n",
            "[Training Epoch 60] Batch 1655, Loss 0.2449445277452469\n",
            "[Training Epoch 60] Batch 1656, Loss 0.2646747827529907\n",
            "[Training Epoch 60] Batch 1657, Loss 0.2627803683280945\n",
            "[Training Epoch 60] Batch 1658, Loss 0.23847468197345734\n",
            "[Training Epoch 60] Batch 1659, Loss 0.2731146812438965\n",
            "[Training Epoch 60] Batch 1660, Loss 0.27667030692100525\n",
            "[Training Epoch 60] Batch 1661, Loss 0.2634115219116211\n",
            "[Training Epoch 60] Batch 1662, Loss 0.272177129983902\n",
            "[Training Epoch 60] Batch 1663, Loss 0.2657538056373596\n",
            "[Training Epoch 60] Batch 1664, Loss 0.2590796947479248\n",
            "[Training Epoch 60] Batch 1665, Loss 0.259720116853714\n",
            "[Training Epoch 60] Batch 1666, Loss 0.2519668936729431\n",
            "[Training Epoch 60] Batch 1667, Loss 0.2540314793586731\n",
            "[Training Epoch 60] Batch 1668, Loss 0.24311791360378265\n",
            "[Training Epoch 60] Batch 1669, Loss 0.24714483320713043\n",
            "[Training Epoch 60] Batch 1670, Loss 0.24987450242042542\n",
            "[Training Epoch 60] Batch 1671, Loss 0.29642099142074585\n",
            "[Training Epoch 60] Batch 1672, Loss 0.2533937096595764\n",
            "[Training Epoch 60] Batch 1673, Loss 0.24345679581165314\n",
            "[Training Epoch 60] Batch 1674, Loss 0.2772592008113861\n",
            "[Training Epoch 60] Batch 1675, Loss 0.2801448702812195\n",
            "[Training Epoch 60] Batch 1676, Loss 0.22790586948394775\n",
            "[Training Epoch 60] Batch 1677, Loss 0.23788896203041077\n",
            "[Training Epoch 60] Batch 1678, Loss 0.27752751111984253\n",
            "[Training Epoch 60] Batch 1679, Loss 0.25361117720603943\n",
            "[Training Epoch 60] Batch 1680, Loss 0.24862603843212128\n",
            "[Training Epoch 60] Batch 1681, Loss 0.29009053111076355\n",
            "[Training Epoch 60] Batch 1682, Loss 0.25729435682296753\n",
            "[Training Epoch 60] Batch 1683, Loss 0.2586899995803833\n",
            "[Training Epoch 60] Batch 1684, Loss 0.2772272229194641\n",
            "[Training Epoch 60] Batch 1685, Loss 0.2766055464744568\n",
            "[Training Epoch 60] Batch 1686, Loss 0.28940898180007935\n",
            "[Training Epoch 60] Batch 1687, Loss 0.24920980632305145\n",
            "[Training Epoch 60] Batch 1688, Loss 0.2477106899023056\n",
            "[Training Epoch 60] Batch 1689, Loss 0.25843971967697144\n",
            "[Training Epoch 60] Batch 1690, Loss 0.24183622002601624\n",
            "[Training Epoch 60] Batch 1691, Loss 0.254107803106308\n",
            "[Training Epoch 60] Batch 1692, Loss 0.24327433109283447\n",
            "[Training Epoch 60] Batch 1693, Loss 0.28618454933166504\n",
            "[Training Epoch 60] Batch 1694, Loss 0.2853548526763916\n",
            "[Training Epoch 60] Batch 1695, Loss 0.2825058102607727\n",
            "[Training Epoch 60] Batch 1696, Loss 0.24851340055465698\n",
            "[Training Epoch 60] Batch 1697, Loss 0.27977365255355835\n",
            "[Training Epoch 60] Batch 1698, Loss 0.2614867687225342\n",
            "[Training Epoch 60] Batch 1699, Loss 0.24454648792743683\n",
            "[Training Epoch 60] Batch 1700, Loss 0.2911202907562256\n",
            "[Training Epoch 60] Batch 1701, Loss 0.26091665029525757\n",
            "[Training Epoch 60] Batch 1702, Loss 0.25549277663230896\n",
            "[Training Epoch 60] Batch 1703, Loss 0.258246511220932\n",
            "[Training Epoch 60] Batch 1704, Loss 0.26264113187789917\n",
            "[Training Epoch 60] Batch 1705, Loss 0.24777916073799133\n",
            "[Training Epoch 60] Batch 1706, Loss 0.2751813530921936\n",
            "[Training Epoch 60] Batch 1707, Loss 0.2812356948852539\n",
            "[Training Epoch 60] Batch 1708, Loss 0.24934735894203186\n",
            "[Training Epoch 60] Batch 1709, Loss 0.2541852295398712\n",
            "[Training Epoch 60] Batch 1710, Loss 0.25713011622428894\n",
            "[Training Epoch 60] Batch 1711, Loss 0.262398362159729\n",
            "[Training Epoch 60] Batch 1712, Loss 0.26927170157432556\n",
            "[Training Epoch 60] Batch 1713, Loss 0.2560513913631439\n",
            "[Training Epoch 60] Batch 1714, Loss 0.2779965400695801\n",
            "[Training Epoch 60] Batch 1715, Loss 0.2595332860946655\n",
            "[Training Epoch 60] Batch 1716, Loss 0.28921136260032654\n",
            "[Training Epoch 60] Batch 1717, Loss 0.28326958417892456\n",
            "[Training Epoch 60] Batch 1718, Loss 0.26296234130859375\n",
            "[Training Epoch 60] Batch 1719, Loss 0.2817753851413727\n",
            "[Training Epoch 60] Batch 1720, Loss 0.24992120265960693\n",
            "[Training Epoch 60] Batch 1721, Loss 0.250274121761322\n",
            "[Training Epoch 60] Batch 1722, Loss 0.23534104228019714\n",
            "[Training Epoch 60] Batch 1723, Loss 0.2833348214626312\n",
            "[Training Epoch 60] Batch 1724, Loss 0.29349374771118164\n",
            "[Training Epoch 60] Batch 1725, Loss 0.25697803497314453\n",
            "[Training Epoch 60] Batch 1726, Loss 0.26292598247528076\n",
            "[Training Epoch 60] Batch 1727, Loss 0.26993924379348755\n",
            "[Training Epoch 60] Batch 1728, Loss 0.28271612524986267\n",
            "[Training Epoch 60] Batch 1729, Loss 0.25060129165649414\n",
            "[Training Epoch 60] Batch 1730, Loss 0.26856303215026855\n",
            "[Training Epoch 60] Batch 1731, Loss 0.2409645915031433\n",
            "[Training Epoch 60] Batch 1732, Loss 0.2827950417995453\n",
            "[Training Epoch 60] Batch 1733, Loss 0.2862918972969055\n",
            "[Training Epoch 60] Batch 1734, Loss 0.26834994554519653\n",
            "[Training Epoch 60] Batch 1735, Loss 0.26285678148269653\n",
            "[Training Epoch 60] Batch 1736, Loss 0.2936230003833771\n",
            "[Training Epoch 60] Batch 1737, Loss 0.30172017216682434\n",
            "[Training Epoch 60] Batch 1738, Loss 0.26700758934020996\n",
            "[Training Epoch 60] Batch 1739, Loss 0.2589075565338135\n",
            "[Training Epoch 60] Batch 1740, Loss 0.2427656054496765\n",
            "[Training Epoch 60] Batch 1741, Loss 0.2575584948062897\n",
            "[Training Epoch 60] Batch 1742, Loss 0.26974397897720337\n",
            "[Training Epoch 60] Batch 1743, Loss 0.2740557789802551\n",
            "[Training Epoch 60] Batch 1744, Loss 0.2579810917377472\n",
            "[Training Epoch 60] Batch 1745, Loss 0.2570412755012512\n",
            "[Training Epoch 60] Batch 1746, Loss 0.24690383672714233\n",
            "[Training Epoch 60] Batch 1747, Loss 0.3124927878379822\n",
            "[Training Epoch 60] Batch 1748, Loss 0.2602841258049011\n",
            "[Training Epoch 60] Batch 1749, Loss 0.24972304701805115\n",
            "[Training Epoch 60] Batch 1750, Loss 0.2767648994922638\n",
            "[Training Epoch 60] Batch 1751, Loss 0.26022714376449585\n",
            "[Training Epoch 60] Batch 1752, Loss 0.2522551715373993\n",
            "[Training Epoch 60] Batch 1753, Loss 0.2526633143424988\n",
            "[Training Epoch 60] Batch 1754, Loss 0.26144662499427795\n",
            "[Training Epoch 60] Batch 1755, Loss 0.29549744725227356\n",
            "[Training Epoch 60] Batch 1756, Loss 0.24664396047592163\n",
            "[Training Epoch 60] Batch 1757, Loss 0.24673201143741608\n",
            "[Training Epoch 60] Batch 1758, Loss 0.27390193939208984\n",
            "[Training Epoch 60] Batch 1759, Loss 0.267550528049469\n",
            "[Training Epoch 60] Batch 1760, Loss 0.2885514199733734\n",
            "[Training Epoch 60] Batch 1761, Loss 0.2834934592247009\n",
            "[Training Epoch 60] Batch 1762, Loss 0.26696842908859253\n",
            "[Training Epoch 60] Batch 1763, Loss 0.28478747606277466\n",
            "[Training Epoch 60] Batch 1764, Loss 0.27507561445236206\n",
            "[Training Epoch 60] Batch 1765, Loss 0.287678599357605\n",
            "[Training Epoch 60] Batch 1766, Loss 0.25499412417411804\n",
            "[Training Epoch 60] Batch 1767, Loss 0.26720064878463745\n",
            "[Training Epoch 60] Batch 1768, Loss 0.256435364484787\n",
            "[Training Epoch 60] Batch 1769, Loss 0.26108789443969727\n",
            "[Training Epoch 60] Batch 1770, Loss 0.24506716430187225\n",
            "[Training Epoch 60] Batch 1771, Loss 0.2818552255630493\n",
            "[Training Epoch 60] Batch 1772, Loss 0.2490670084953308\n",
            "[Training Epoch 60] Batch 1773, Loss 0.2725452780723572\n",
            "[Training Epoch 60] Batch 1774, Loss 0.2435602843761444\n",
            "[Training Epoch 60] Batch 1775, Loss 0.28206977248191833\n",
            "[Training Epoch 60] Batch 1776, Loss 0.2916368246078491\n",
            "[Training Epoch 60] Batch 1777, Loss 0.26863613724708557\n",
            "[Training Epoch 60] Batch 1778, Loss 0.2597791254520416\n",
            "[Training Epoch 60] Batch 1779, Loss 0.2555323541164398\n",
            "[Training Epoch 60] Batch 1780, Loss 0.28044456243515015\n",
            "[Training Epoch 60] Batch 1781, Loss 0.27117204666137695\n",
            "[Training Epoch 60] Batch 1782, Loss 0.27465689182281494\n",
            "[Training Epoch 60] Batch 1783, Loss 0.2647854685783386\n",
            "[Training Epoch 60] Batch 1784, Loss 0.2520553469657898\n",
            "[Training Epoch 60] Batch 1785, Loss 0.29332348704338074\n",
            "[Training Epoch 60] Batch 1786, Loss 0.27582594752311707\n",
            "[Training Epoch 60] Batch 1787, Loss 0.23701906204223633\n",
            "[Training Epoch 60] Batch 1788, Loss 0.29378610849380493\n",
            "[Training Epoch 60] Batch 1789, Loss 0.2652307450771332\n",
            "[Training Epoch 60] Batch 1790, Loss 0.25423556566238403\n",
            "[Training Epoch 60] Batch 1791, Loss 0.2566327452659607\n",
            "[Training Epoch 60] Batch 1792, Loss 0.2516852617263794\n",
            "[Training Epoch 60] Batch 1793, Loss 0.27419450879096985\n",
            "[Training Epoch 60] Batch 1794, Loss 0.2561604976654053\n",
            "[Training Epoch 60] Batch 1795, Loss 0.2920665144920349\n",
            "[Training Epoch 60] Batch 1796, Loss 0.2473369985818863\n",
            "[Training Epoch 60] Batch 1797, Loss 0.28244084119796753\n",
            "[Training Epoch 60] Batch 1798, Loss 0.2668716013431549\n",
            "[Training Epoch 60] Batch 1799, Loss 0.25224536657333374\n",
            "[Training Epoch 60] Batch 1800, Loss 0.27546370029449463\n",
            "[Training Epoch 60] Batch 1801, Loss 0.2608709931373596\n",
            "[Training Epoch 60] Batch 1802, Loss 0.2709215581417084\n",
            "[Training Epoch 60] Batch 1803, Loss 0.2595990002155304\n",
            "[Training Epoch 60] Batch 1804, Loss 0.2513910233974457\n",
            "[Training Epoch 60] Batch 1805, Loss 0.28884872794151306\n",
            "[Training Epoch 60] Batch 1806, Loss 0.25429287552833557\n",
            "[Training Epoch 60] Batch 1807, Loss 0.2605336606502533\n",
            "[Training Epoch 60] Batch 1808, Loss 0.28442227840423584\n",
            "[Training Epoch 60] Batch 1809, Loss 0.2950379550457001\n",
            "[Training Epoch 60] Batch 1810, Loss 0.2565241754055023\n",
            "[Training Epoch 60] Batch 1811, Loss 0.2707248032093048\n",
            "[Training Epoch 60] Batch 1812, Loss 0.2586296796798706\n",
            "[Training Epoch 60] Batch 1813, Loss 0.250912070274353\n",
            "[Training Epoch 60] Batch 1814, Loss 0.26358768343925476\n",
            "[Training Epoch 60] Batch 1815, Loss 0.27345770597457886\n",
            "[Training Epoch 60] Batch 1816, Loss 0.27014586329460144\n",
            "[Training Epoch 60] Batch 1817, Loss 0.29194989800453186\n",
            "[Training Epoch 60] Batch 1818, Loss 0.2862486243247986\n",
            "[Training Epoch 60] Batch 1819, Loss 0.2526738941669464\n",
            "[Training Epoch 60] Batch 1820, Loss 0.28355416655540466\n",
            "[Training Epoch 60] Batch 1821, Loss 0.2306535392999649\n",
            "[Training Epoch 60] Batch 1822, Loss 0.2654437720775604\n",
            "[Training Epoch 60] Batch 1823, Loss 0.283366858959198\n",
            "[Training Epoch 60] Batch 1824, Loss 0.2667033076286316\n",
            "[Training Epoch 60] Batch 1825, Loss 0.2640368640422821\n",
            "[Training Epoch 60] Batch 1826, Loss 0.27344220876693726\n",
            "[Training Epoch 60] Batch 1827, Loss 0.2438293844461441\n",
            "[Training Epoch 60] Batch 1828, Loss 0.24997034668922424\n",
            "[Training Epoch 60] Batch 1829, Loss 0.23287293314933777\n",
            "[Training Epoch 60] Batch 1830, Loss 0.24727071821689606\n",
            "[Training Epoch 60] Batch 1831, Loss 0.28440162539482117\n",
            "[Training Epoch 60] Batch 1832, Loss 0.27598246932029724\n",
            "[Training Epoch 60] Batch 1833, Loss 0.24099628627300262\n",
            "[Training Epoch 60] Batch 1834, Loss 0.24984057247638702\n",
            "[Training Epoch 60] Batch 1835, Loss 0.29893064498901367\n",
            "[Training Epoch 60] Batch 1836, Loss 0.2723902463912964\n",
            "[Training Epoch 60] Batch 1837, Loss 0.26387444138526917\n",
            "[Training Epoch 60] Batch 1838, Loss 0.26982593536376953\n",
            "[Training Epoch 60] Batch 1839, Loss 0.26780837774276733\n",
            "[Training Epoch 60] Batch 1840, Loss 0.2673317790031433\n",
            "[Training Epoch 60] Batch 1841, Loss 0.27122634649276733\n",
            "[Training Epoch 60] Batch 1842, Loss 0.27297475934028625\n",
            "[Training Epoch 60] Batch 1843, Loss 0.28030699491500854\n",
            "[Training Epoch 60] Batch 1844, Loss 0.29177412390708923\n",
            "[Training Epoch 60] Batch 1845, Loss 0.24858540296554565\n",
            "[Training Epoch 60] Batch 1846, Loss 0.30894675850868225\n",
            "[Training Epoch 60] Batch 1847, Loss 0.23823940753936768\n",
            "[Training Epoch 60] Batch 1848, Loss 0.27973809838294983\n",
            "[Training Epoch 60] Batch 1849, Loss 0.2770651578903198\n",
            "[Training Epoch 60] Batch 1850, Loss 0.2560061812400818\n",
            "[Training Epoch 60] Batch 1851, Loss 0.2940062880516052\n",
            "[Training Epoch 60] Batch 1852, Loss 0.2514192461967468\n",
            "[Training Epoch 60] Batch 1853, Loss 0.26976025104522705\n",
            "[Training Epoch 60] Batch 1854, Loss 0.24067509174346924\n",
            "[Training Epoch 60] Batch 1855, Loss 0.2519178092479706\n",
            "[Training Epoch 60] Batch 1856, Loss 0.27771031856536865\n",
            "[Training Epoch 60] Batch 1857, Loss 0.25962358713150024\n",
            "[Training Epoch 60] Batch 1858, Loss 0.2567674219608307\n",
            "[Training Epoch 60] Batch 1859, Loss 0.26558730006217957\n",
            "[Training Epoch 60] Batch 1860, Loss 0.24779976904392242\n",
            "[Training Epoch 60] Batch 1861, Loss 0.2649179697036743\n",
            "[Training Epoch 60] Batch 1862, Loss 0.25925031304359436\n",
            "[Training Epoch 60] Batch 1863, Loss 0.27899742126464844\n",
            "[Training Epoch 60] Batch 1864, Loss 0.25872522592544556\n",
            "[Training Epoch 60] Batch 1865, Loss 0.266857773065567\n",
            "[Training Epoch 60] Batch 1866, Loss 0.2873072624206543\n",
            "[Training Epoch 60] Batch 1867, Loss 0.26414573192596436\n",
            "[Training Epoch 60] Batch 1868, Loss 0.25142067670822144\n",
            "[Training Epoch 60] Batch 1869, Loss 0.25338611006736755\n",
            "[Training Epoch 60] Batch 1870, Loss 0.27565813064575195\n",
            "[Training Epoch 60] Batch 1871, Loss 0.24914221465587616\n",
            "[Training Epoch 60] Batch 1872, Loss 0.2529693841934204\n",
            "[Training Epoch 60] Batch 1873, Loss 0.24577182531356812\n",
            "[Training Epoch 60] Batch 1874, Loss 0.2518492639064789\n",
            "[Training Epoch 60] Batch 1875, Loss 0.2543115019798279\n",
            "[Training Epoch 60] Batch 1876, Loss 0.28561660647392273\n",
            "[Training Epoch 60] Batch 1877, Loss 0.26705384254455566\n",
            "[Training Epoch 60] Batch 1878, Loss 0.25864607095718384\n",
            "[Training Epoch 60] Batch 1879, Loss 0.26345449686050415\n",
            "[Training Epoch 60] Batch 1880, Loss 0.2704196870326996\n",
            "[Training Epoch 60] Batch 1881, Loss 0.25286412239074707\n",
            "[Training Epoch 60] Batch 1882, Loss 0.27865439653396606\n",
            "[Training Epoch 60] Batch 1883, Loss 0.23464038968086243\n",
            "[Training Epoch 60] Batch 1884, Loss 0.2786511182785034\n",
            "[Training Epoch 60] Batch 1885, Loss 0.2742585241794586\n",
            "[Training Epoch 60] Batch 1886, Loss 0.27200832962989807\n",
            "[Training Epoch 60] Batch 1887, Loss 0.28745290637016296\n",
            "[Training Epoch 60] Batch 1888, Loss 0.2894151508808136\n",
            "[Training Epoch 60] Batch 1889, Loss 0.268518328666687\n",
            "[Training Epoch 60] Batch 1890, Loss 0.276577353477478\n",
            "[Training Epoch 60] Batch 1891, Loss 0.23289397358894348\n",
            "[Training Epoch 60] Batch 1892, Loss 0.27991464734077454\n",
            "[Training Epoch 60] Batch 1893, Loss 0.26626914739608765\n",
            "[Training Epoch 60] Batch 1894, Loss 0.25829458236694336\n",
            "[Training Epoch 60] Batch 1895, Loss 0.246862530708313\n",
            "[Training Epoch 60] Batch 1896, Loss 0.2658129036426544\n",
            "[Training Epoch 60] Batch 1897, Loss 0.27947837114334106\n",
            "[Training Epoch 60] Batch 1898, Loss 0.23918603360652924\n",
            "[Training Epoch 60] Batch 1899, Loss 0.25974953174591064\n",
            "[Training Epoch 60] Batch 1900, Loss 0.23994773626327515\n",
            "[Training Epoch 60] Batch 1901, Loss 0.25353291630744934\n",
            "[Training Epoch 60] Batch 1902, Loss 0.3028295934200287\n",
            "[Training Epoch 60] Batch 1903, Loss 0.2726966142654419\n",
            "[Training Epoch 60] Batch 1904, Loss 0.27079400420188904\n",
            "[Training Epoch 60] Batch 1905, Loss 0.2900301516056061\n",
            "[Training Epoch 60] Batch 1906, Loss 0.26251620054244995\n",
            "[Training Epoch 60] Batch 1907, Loss 0.25552797317504883\n",
            "[Training Epoch 60] Batch 1908, Loss 0.23255601525306702\n",
            "[Training Epoch 60] Batch 1909, Loss 0.2570936679840088\n",
            "[Training Epoch 60] Batch 1910, Loss 0.26780614256858826\n",
            "[Training Epoch 60] Batch 1911, Loss 0.23334313929080963\n",
            "[Training Epoch 60] Batch 1912, Loss 0.23660942912101746\n",
            "[Training Epoch 60] Batch 1913, Loss 0.26048797369003296\n",
            "[Training Epoch 60] Batch 1914, Loss 0.2550968527793884\n",
            "[Training Epoch 60] Batch 1915, Loss 0.25052976608276367\n",
            "[Training Epoch 60] Batch 1916, Loss 0.2623600661754608\n",
            "[Training Epoch 60] Batch 1917, Loss 0.24199488759040833\n",
            "[Training Epoch 60] Batch 1918, Loss 0.2466781884431839\n",
            "[Training Epoch 60] Batch 1919, Loss 0.27809765934944153\n",
            "[Training Epoch 60] Batch 1920, Loss 0.26971086859703064\n",
            "[Training Epoch 60] Batch 1921, Loss 0.29117831587791443\n",
            "[Training Epoch 60] Batch 1922, Loss 0.2600927948951721\n",
            "[Training Epoch 60] Batch 1923, Loss 0.24675002694129944\n",
            "[Training Epoch 60] Batch 1924, Loss 0.28033095598220825\n",
            "[Training Epoch 60] Batch 1925, Loss 0.26928824186325073\n",
            "[Training Epoch 60] Batch 1926, Loss 0.26168420910835266\n",
            "[Training Epoch 60] Batch 1927, Loss 0.27501600980758667\n",
            "[Training Epoch 60] Batch 1928, Loss 0.2632944881916046\n",
            "[Training Epoch 60] Batch 1929, Loss 0.26193737983703613\n",
            "[Training Epoch 60] Batch 1930, Loss 0.2635168731212616\n",
            "[Training Epoch 60] Batch 1931, Loss 0.2600663900375366\n",
            "[Training Epoch 60] Batch 1932, Loss 0.2609739601612091\n",
            "[Training Epoch 60] Batch 1933, Loss 0.29229187965393066\n",
            "[Training Epoch 60] Batch 1934, Loss 0.25050589442253113\n",
            "[Training Epoch 60] Batch 1935, Loss 0.2559697926044464\n",
            "[Training Epoch 60] Batch 1936, Loss 0.272860050201416\n",
            "[Training Epoch 60] Batch 1937, Loss 0.3057937026023865\n",
            "[Training Epoch 60] Batch 1938, Loss 0.2802444100379944\n",
            "[Training Epoch 60] Batch 1939, Loss 0.24849915504455566\n",
            "[Training Epoch 60] Batch 1940, Loss 0.25501561164855957\n",
            "[Training Epoch 60] Batch 1941, Loss 0.2689535915851593\n",
            "[Training Epoch 60] Batch 1942, Loss 0.2498195916414261\n",
            "[Training Epoch 60] Batch 1943, Loss 0.26779788732528687\n",
            "[Training Epoch 60] Batch 1944, Loss 0.25921136140823364\n",
            "[Training Epoch 60] Batch 1945, Loss 0.2504143714904785\n",
            "[Training Epoch 60] Batch 1946, Loss 0.2705039083957672\n",
            "[Training Epoch 60] Batch 1947, Loss 0.2762770652770996\n",
            "[Training Epoch 60] Batch 1948, Loss 0.2877252995967865\n",
            "[Training Epoch 60] Batch 1949, Loss 0.25041133165359497\n",
            "[Training Epoch 60] Batch 1950, Loss 0.2886916995048523\n",
            "[Training Epoch 60] Batch 1951, Loss 0.282579243183136\n",
            "[Training Epoch 60] Batch 1952, Loss 0.26615428924560547\n",
            "[Training Epoch 60] Batch 1953, Loss 0.3020322918891907\n",
            "[Training Epoch 60] Batch 1954, Loss 0.2615247070789337\n",
            "[Training Epoch 60] Batch 1955, Loss 0.2631620764732361\n",
            "[Training Epoch 60] Batch 1956, Loss 0.25312912464141846\n",
            "[Training Epoch 60] Batch 1957, Loss 0.2470853626728058\n",
            "[Training Epoch 60] Batch 1958, Loss 0.2668685019016266\n",
            "[Training Epoch 60] Batch 1959, Loss 0.2404899150133133\n",
            "[Training Epoch 60] Batch 1960, Loss 0.27690622210502625\n",
            "[Training Epoch 60] Batch 1961, Loss 0.27538010478019714\n",
            "[Training Epoch 60] Batch 1962, Loss 0.24391497671604156\n",
            "[Training Epoch 60] Batch 1963, Loss 0.26770418882369995\n",
            "[Training Epoch 60] Batch 1964, Loss 0.2808994650840759\n",
            "[Training Epoch 60] Batch 1965, Loss 0.25228819251060486\n",
            "[Training Epoch 60] Batch 1966, Loss 0.2685384750366211\n",
            "[Training Epoch 60] Batch 1967, Loss 0.22755196690559387\n",
            "[Training Epoch 60] Batch 1968, Loss 0.2529374063014984\n",
            "[Training Epoch 60] Batch 1969, Loss 0.26579582691192627\n",
            "[Training Epoch 60] Batch 1970, Loss 0.24638184905052185\n",
            "[Training Epoch 60] Batch 1971, Loss 0.2579817771911621\n",
            "[Training Epoch 60] Batch 1972, Loss 0.2925908863544464\n",
            "[Training Epoch 60] Batch 1973, Loss 0.25099775195121765\n",
            "[Training Epoch 60] Batch 1974, Loss 0.25566890835762024\n",
            "[Training Epoch 60] Batch 1975, Loss 0.28656426072120667\n",
            "[Training Epoch 60] Batch 1976, Loss 0.27185922861099243\n",
            "[Training Epoch 60] Batch 1977, Loss 0.2655037045478821\n",
            "[Training Epoch 60] Batch 1978, Loss 0.2639124393463135\n",
            "[Training Epoch 60] Batch 1979, Loss 0.27031174302101135\n",
            "[Training Epoch 60] Batch 1980, Loss 0.26393041014671326\n",
            "[Training Epoch 60] Batch 1981, Loss 0.29110226035118103\n",
            "[Training Epoch 60] Batch 1982, Loss 0.2473018765449524\n",
            "[Training Epoch 60] Batch 1983, Loss 0.2603223919868469\n",
            "[Training Epoch 60] Batch 1984, Loss 0.2936857044696808\n",
            "[Training Epoch 60] Batch 1985, Loss 0.229221910238266\n",
            "[Training Epoch 60] Batch 1986, Loss 0.2840690612792969\n",
            "[Training Epoch 60] Batch 1987, Loss 0.24627907574176788\n",
            "[Training Epoch 60] Batch 1988, Loss 0.24996885657310486\n",
            "[Training Epoch 60] Batch 1989, Loss 0.25225314497947693\n",
            "[Training Epoch 60] Batch 1990, Loss 0.27250340580940247\n",
            "[Training Epoch 60] Batch 1991, Loss 0.2871915102005005\n",
            "[Training Epoch 60] Batch 1992, Loss 0.3102695345878601\n",
            "[Training Epoch 60] Batch 1993, Loss 0.23268187046051025\n",
            "[Training Epoch 60] Batch 1994, Loss 0.2593686878681183\n",
            "[Training Epoch 60] Batch 1995, Loss 0.2584034204483032\n",
            "[Training Epoch 60] Batch 1996, Loss 0.2608877718448639\n",
            "[Training Epoch 60] Batch 1997, Loss 0.2612910270690918\n",
            "[Training Epoch 60] Batch 1998, Loss 0.27603617310523987\n",
            "[Training Epoch 60] Batch 1999, Loss 0.22672532498836517\n",
            "[Training Epoch 60] Batch 2000, Loss 0.2514179050922394\n",
            "[Training Epoch 60] Batch 2001, Loss 0.2671784460544586\n",
            "[Training Epoch 60] Batch 2002, Loss 0.277937114238739\n",
            "[Training Epoch 60] Batch 2003, Loss 0.2982823848724365\n",
            "[Training Epoch 60] Batch 2004, Loss 0.2851596772670746\n",
            "[Training Epoch 60] Batch 2005, Loss 0.30113309621810913\n",
            "[Training Epoch 60] Batch 2006, Loss 0.2779189646244049\n",
            "[Training Epoch 60] Batch 2007, Loss 0.30023396015167236\n",
            "[Training Epoch 60] Batch 2008, Loss 0.2683379352092743\n",
            "[Training Epoch 60] Batch 2009, Loss 0.24753212928771973\n",
            "[Training Epoch 60] Batch 2010, Loss 0.28412920236587524\n",
            "[Training Epoch 60] Batch 2011, Loss 0.2607418894767761\n",
            "[Training Epoch 60] Batch 2012, Loss 0.27255940437316895\n",
            "[Training Epoch 60] Batch 2013, Loss 0.2860650420188904\n",
            "[Training Epoch 60] Batch 2014, Loss 0.2543083727359772\n",
            "[Training Epoch 60] Batch 2015, Loss 0.2285730391740799\n",
            "[Training Epoch 60] Batch 2016, Loss 0.23215351998806\n",
            "[Training Epoch 60] Batch 2017, Loss 0.2864381968975067\n",
            "[Training Epoch 60] Batch 2018, Loss 0.263705313205719\n",
            "[Training Epoch 60] Batch 2019, Loss 0.27504926919937134\n",
            "[Training Epoch 60] Batch 2020, Loss 0.26811397075653076\n",
            "[Training Epoch 60] Batch 2021, Loss 0.2547682821750641\n",
            "[Training Epoch 60] Batch 2022, Loss 0.2584974765777588\n",
            "[Training Epoch 60] Batch 2023, Loss 0.26150017976760864\n",
            "[Training Epoch 60] Batch 2024, Loss 0.2660134732723236\n",
            "[Training Epoch 60] Batch 2025, Loss 0.28858375549316406\n",
            "[Training Epoch 60] Batch 2026, Loss 0.250163733959198\n",
            "[Training Epoch 60] Batch 2027, Loss 0.26013609766960144\n",
            "[Training Epoch 60] Batch 2028, Loss 0.2610265910625458\n",
            "[Training Epoch 60] Batch 2029, Loss 0.29016679525375366\n",
            "[Training Epoch 60] Batch 2030, Loss 0.22607271373271942\n",
            "[Training Epoch 60] Batch 2031, Loss 0.24315835535526276\n",
            "[Training Epoch 60] Batch 2032, Loss 0.2379387468099594\n",
            "[Training Epoch 60] Batch 2033, Loss 0.2711569666862488\n",
            "[Training Epoch 60] Batch 2034, Loss 0.2711472511291504\n",
            "[Training Epoch 60] Batch 2035, Loss 0.26305240392684937\n",
            "[Training Epoch 60] Batch 2036, Loss 0.27327704429626465\n",
            "[Training Epoch 60] Batch 2037, Loss 0.2766471207141876\n",
            "[Training Epoch 60] Batch 2038, Loss 0.292117714881897\n",
            "[Training Epoch 60] Batch 2039, Loss 0.26475194096565247\n",
            "[Training Epoch 60] Batch 2040, Loss 0.29512113332748413\n",
            "[Training Epoch 60] Batch 2041, Loss 0.23532772064208984\n",
            "[Training Epoch 60] Batch 2042, Loss 0.27749210596084595\n",
            "[Training Epoch 60] Batch 2043, Loss 0.24110147356987\n",
            "[Training Epoch 60] Batch 2044, Loss 0.2436583936214447\n",
            "[Training Epoch 60] Batch 2045, Loss 0.25943630933761597\n",
            "[Training Epoch 60] Batch 2046, Loss 0.28729763627052307\n",
            "[Training Epoch 60] Batch 2047, Loss 0.26936864852905273\n",
            "[Training Epoch 60] Batch 2048, Loss 0.28939110040664673\n",
            "[Training Epoch 60] Batch 2049, Loss 0.2628489136695862\n",
            "[Training Epoch 60] Batch 2050, Loss 0.26198410987854004\n",
            "[Training Epoch 60] Batch 2051, Loss 0.2521107494831085\n",
            "[Training Epoch 60] Batch 2052, Loss 0.27153825759887695\n",
            "[Training Epoch 60] Batch 2053, Loss 0.26064395904541016\n",
            "[Training Epoch 60] Batch 2054, Loss 0.29618120193481445\n",
            "[Training Epoch 60] Batch 2055, Loss 0.2382592111825943\n",
            "[Training Epoch 60] Batch 2056, Loss 0.2804369032382965\n",
            "[Training Epoch 60] Batch 2057, Loss 0.25693586468696594\n",
            "[Training Epoch 60] Batch 2058, Loss 0.2693234980106354\n",
            "[Training Epoch 60] Batch 2059, Loss 0.2821376621723175\n",
            "[Training Epoch 60] Batch 2060, Loss 0.2694718539714813\n",
            "[Training Epoch 60] Batch 2061, Loss 0.27198851108551025\n",
            "[Training Epoch 60] Batch 2062, Loss 0.26545408368110657\n",
            "[Training Epoch 60] Batch 2063, Loss 0.270857572555542\n",
            "[Training Epoch 60] Batch 2064, Loss 0.27808940410614014\n",
            "[Training Epoch 60] Batch 2065, Loss 0.2890409231185913\n",
            "[Training Epoch 60] Batch 2066, Loss 0.29767906665802\n",
            "[Training Epoch 60] Batch 2067, Loss 0.24964043498039246\n",
            "[Training Epoch 60] Batch 2068, Loss 0.29490306973457336\n",
            "[Training Epoch 60] Batch 2069, Loss 0.24561145901679993\n",
            "[Training Epoch 60] Batch 2070, Loss 0.2340577244758606\n",
            "[Training Epoch 60] Batch 2071, Loss 0.2669775187969208\n",
            "[Training Epoch 60] Batch 2072, Loss 0.26831454038619995\n",
            "[Training Epoch 60] Batch 2073, Loss 0.23731721937656403\n",
            "[Training Epoch 60] Batch 2074, Loss 0.26211634278297424\n",
            "[Training Epoch 60] Batch 2075, Loss 0.2867351472377777\n",
            "[Training Epoch 60] Batch 2076, Loss 0.24833957850933075\n",
            "[Training Epoch 60] Batch 2077, Loss 0.2303200364112854\n",
            "[Training Epoch 60] Batch 2078, Loss 0.2816983163356781\n",
            "[Training Epoch 60] Batch 2079, Loss 0.30682554841041565\n",
            "[Training Epoch 60] Batch 2080, Loss 0.26762914657592773\n",
            "[Training Epoch 60] Batch 2081, Loss 0.2573195993900299\n",
            "[Training Epoch 60] Batch 2082, Loss 0.26867741346359253\n",
            "[Training Epoch 60] Batch 2083, Loss 0.26823487877845764\n",
            "[Training Epoch 60] Batch 2084, Loss 0.22886352241039276\n",
            "[Training Epoch 60] Batch 2085, Loss 0.2728042006492615\n",
            "[Training Epoch 60] Batch 2086, Loss 0.24596408009529114\n",
            "[Training Epoch 60] Batch 2087, Loss 0.26760512590408325\n",
            "[Training Epoch 60] Batch 2088, Loss 0.2701808214187622\n",
            "[Training Epoch 60] Batch 2089, Loss 0.2746303081512451\n",
            "[Training Epoch 60] Batch 2090, Loss 0.25410395860671997\n",
            "[Training Epoch 60] Batch 2091, Loss 0.2407202273607254\n",
            "[Training Epoch 60] Batch 2092, Loss 0.2493976354598999\n",
            "[Training Epoch 60] Batch 2093, Loss 0.2799338400363922\n",
            "[Training Epoch 60] Batch 2094, Loss 0.2507869005203247\n",
            "[Training Epoch 60] Batch 2095, Loss 0.27647268772125244\n",
            "[Training Epoch 60] Batch 2096, Loss 0.28268271684646606\n",
            "[Training Epoch 60] Batch 2097, Loss 0.27896538376808167\n",
            "[Training Epoch 60] Batch 2098, Loss 0.24445748329162598\n",
            "[Training Epoch 60] Batch 2099, Loss 0.2538685202598572\n",
            "[Training Epoch 60] Batch 2100, Loss 0.2752263844013214\n",
            "[Training Epoch 60] Batch 2101, Loss 0.2729581296443939\n",
            "[Training Epoch 60] Batch 2102, Loss 0.26342451572418213\n",
            "[Training Epoch 60] Batch 2103, Loss 0.23838184773921967\n",
            "[Training Epoch 60] Batch 2104, Loss 0.2524673342704773\n",
            "[Training Epoch 60] Batch 2105, Loss 0.2883695960044861\n",
            "[Training Epoch 60] Batch 2106, Loss 0.2688896656036377\n",
            "[Training Epoch 60] Batch 2107, Loss 0.2764706313610077\n",
            "[Training Epoch 60] Batch 2108, Loss 0.2736298441886902\n",
            "[Training Epoch 60] Batch 2109, Loss 0.2607143521308899\n",
            "[Training Epoch 60] Batch 2110, Loss 0.26080456376075745\n",
            "[Training Epoch 60] Batch 2111, Loss 0.2149708867073059\n",
            "[Training Epoch 60] Batch 2112, Loss 0.2702685296535492\n",
            "[Training Epoch 60] Batch 2113, Loss 0.27562427520751953\n",
            "[Training Epoch 60] Batch 2114, Loss 0.2601909637451172\n",
            "[Training Epoch 60] Batch 2115, Loss 0.28704240918159485\n",
            "[Training Epoch 60] Batch 2116, Loss 0.2657429873943329\n",
            "[Training Epoch 60] Batch 2117, Loss 0.23984065651893616\n",
            "[Training Epoch 60] Batch 2118, Loss 0.2676964998245239\n",
            "[Training Epoch 60] Batch 2119, Loss 0.26484888792037964\n",
            "[Training Epoch 60] Batch 2120, Loss 0.27357882261276245\n",
            "[Training Epoch 60] Batch 2121, Loss 0.26305773854255676\n",
            "[Training Epoch 60] Batch 2122, Loss 0.2647625505924225\n",
            "[Training Epoch 60] Batch 2123, Loss 0.3003864288330078\n",
            "[Training Epoch 60] Batch 2124, Loss 0.2628324627876282\n",
            "[Training Epoch 60] Batch 2125, Loss 0.2434607744216919\n",
            "[Training Epoch 60] Batch 2126, Loss 0.2852866053581238\n",
            "[Training Epoch 60] Batch 2127, Loss 0.2586594820022583\n",
            "[Training Epoch 60] Batch 2128, Loss 0.2595691978931427\n",
            "[Training Epoch 60] Batch 2129, Loss 0.23074191808700562\n",
            "[Training Epoch 60] Batch 2130, Loss 0.2885196805000305\n",
            "[Training Epoch 60] Batch 2131, Loss 0.250770628452301\n",
            "[Training Epoch 60] Batch 2132, Loss 0.2570149898529053\n",
            "[Training Epoch 60] Batch 2133, Loss 0.23944377899169922\n",
            "[Training Epoch 60] Batch 2134, Loss 0.25820422172546387\n",
            "[Training Epoch 60] Batch 2135, Loss 0.2604284882545471\n",
            "[Training Epoch 60] Batch 2136, Loss 0.2035764455795288\n",
            "[Training Epoch 60] Batch 2137, Loss 0.24923238158226013\n",
            "[Training Epoch 60] Batch 2138, Loss 0.26504456996917725\n",
            "[Training Epoch 60] Batch 2139, Loss 0.2767728567123413\n",
            "[Training Epoch 60] Batch 2140, Loss 0.2748127579689026\n",
            "[Training Epoch 60] Batch 2141, Loss 0.25947481393814087\n",
            "[Training Epoch 60] Batch 2142, Loss 0.2795071601867676\n",
            "[Training Epoch 60] Batch 2143, Loss 0.27191728353500366\n",
            "[Training Epoch 60] Batch 2144, Loss 0.2590847909450531\n",
            "[Training Epoch 60] Batch 2145, Loss 0.2602825164794922\n",
            "[Training Epoch 60] Batch 2146, Loss 0.26242873072624207\n",
            "[Training Epoch 60] Batch 2147, Loss 0.2779124081134796\n",
            "[Training Epoch 60] Batch 2148, Loss 0.2522229552268982\n",
            "[Training Epoch 60] Batch 2149, Loss 0.266793429851532\n",
            "[Training Epoch 60] Batch 2150, Loss 0.2866411805152893\n",
            "[Training Epoch 60] Batch 2151, Loss 0.25308480858802795\n",
            "[Training Epoch 60] Batch 2152, Loss 0.25267109274864197\n",
            "[Training Epoch 60] Batch 2153, Loss 0.2501867115497589\n",
            "[Training Epoch 60] Batch 2154, Loss 0.2324676215648651\n",
            "[Training Epoch 60] Batch 2155, Loss 0.2479517161846161\n",
            "[Training Epoch 60] Batch 2156, Loss 0.22289620339870453\n",
            "[Training Epoch 60] Batch 2157, Loss 0.25763291120529175\n",
            "[Training Epoch 60] Batch 2158, Loss 0.26386702060699463\n",
            "[Training Epoch 60] Batch 2159, Loss 0.2705632746219635\n",
            "[Training Epoch 60] Batch 2160, Loss 0.2850738763809204\n",
            "[Training Epoch 60] Batch 2161, Loss 0.26489824056625366\n",
            "[Training Epoch 60] Batch 2162, Loss 0.27248966693878174\n",
            "[Training Epoch 60] Batch 2163, Loss 0.25884053111076355\n",
            "[Training Epoch 60] Batch 2164, Loss 0.27429550886154175\n",
            "[Training Epoch 60] Batch 2165, Loss 0.28176257014274597\n",
            "[Training Epoch 60] Batch 2166, Loss 0.29391926527023315\n",
            "[Training Epoch 60] Batch 2167, Loss 0.25720980763435364\n",
            "[Training Epoch 60] Batch 2168, Loss 0.2713242173194885\n",
            "[Training Epoch 60] Batch 2169, Loss 0.26728370785713196\n",
            "[Training Epoch 60] Batch 2170, Loss 0.23506009578704834\n",
            "[Training Epoch 60] Batch 2171, Loss 0.2623194754123688\n",
            "[Training Epoch 60] Batch 2172, Loss 0.27753588557243347\n",
            "[Training Epoch 60] Batch 2173, Loss 0.2681635022163391\n",
            "[Training Epoch 60] Batch 2174, Loss 0.27853208780288696\n",
            "[Training Epoch 60] Batch 2175, Loss 0.28588804602622986\n",
            "[Training Epoch 60] Batch 2176, Loss 0.2774045765399933\n",
            "[Training Epoch 60] Batch 2177, Loss 0.267104834318161\n",
            "[Training Epoch 60] Batch 2178, Loss 0.28884580731391907\n",
            "[Training Epoch 60] Batch 2179, Loss 0.24089482426643372\n",
            "[Training Epoch 60] Batch 2180, Loss 0.2488158643245697\n",
            "[Training Epoch 60] Batch 2181, Loss 0.2548326551914215\n",
            "[Training Epoch 60] Batch 2182, Loss 0.28213250637054443\n",
            "[Training Epoch 60] Batch 2183, Loss 0.25884363055229187\n",
            "[Training Epoch 60] Batch 2184, Loss 0.22803497314453125\n",
            "[Training Epoch 60] Batch 2185, Loss 0.2981471121311188\n",
            "[Training Epoch 60] Batch 2186, Loss 0.24651898443698883\n",
            "[Training Epoch 60] Batch 2187, Loss 0.2653486728668213\n",
            "[Training Epoch 60] Batch 2188, Loss 0.2671789824962616\n",
            "[Training Epoch 60] Batch 2189, Loss 0.243730366230011\n",
            "[Training Epoch 60] Batch 2190, Loss 0.27172476053237915\n",
            "[Training Epoch 60] Batch 2191, Loss 0.2694520950317383\n",
            "[Training Epoch 60] Batch 2192, Loss 0.24702951312065125\n",
            "[Training Epoch 60] Batch 2193, Loss 0.23638105392456055\n",
            "[Training Epoch 60] Batch 2194, Loss 0.2736365795135498\n",
            "[Training Epoch 60] Batch 2195, Loss 0.2600860893726349\n",
            "[Training Epoch 60] Batch 2196, Loss 0.24943368136882782\n",
            "[Training Epoch 60] Batch 2197, Loss 0.2982989251613617\n",
            "[Training Epoch 60] Batch 2198, Loss 0.2748905122280121\n",
            "[Training Epoch 60] Batch 2199, Loss 0.2668079137802124\n",
            "[Training Epoch 60] Batch 2200, Loss 0.2595108449459076\n",
            "[Training Epoch 60] Batch 2201, Loss 0.24840913712978363\n",
            "[Training Epoch 60] Batch 2202, Loss 0.27099156379699707\n",
            "[Training Epoch 60] Batch 2203, Loss 0.28386226296424866\n",
            "[Training Epoch 60] Batch 2204, Loss 0.274543434381485\n",
            "[Training Epoch 60] Batch 2205, Loss 0.2476453334093094\n",
            "[Training Epoch 60] Batch 2206, Loss 0.2873321771621704\n",
            "[Training Epoch 60] Batch 2207, Loss 0.2714366614818573\n",
            "[Training Epoch 60] Batch 2208, Loss 0.2625659108161926\n",
            "[Training Epoch 60] Batch 2209, Loss 0.27002179622650146\n",
            "[Training Epoch 60] Batch 2210, Loss 0.2839186191558838\n",
            "[Training Epoch 60] Batch 2211, Loss 0.2528771758079529\n",
            "[Training Epoch 60] Batch 2212, Loss 0.24564115703105927\n",
            "[Training Epoch 60] Batch 2213, Loss 0.2551589906215668\n",
            "[Training Epoch 60] Batch 2214, Loss 0.2588629722595215\n",
            "[Training Epoch 60] Batch 2215, Loss 0.2768666446208954\n",
            "[Training Epoch 60] Batch 2216, Loss 0.2764882445335388\n",
            "[Training Epoch 60] Batch 2217, Loss 0.2518194019794464\n",
            "[Training Epoch 60] Batch 2218, Loss 0.24998700618743896\n",
            "[Training Epoch 60] Batch 2219, Loss 0.26884621381759644\n",
            "[Training Epoch 60] Batch 2220, Loss 0.2538416385650635\n",
            "[Training Epoch 60] Batch 2221, Loss 0.25971537828445435\n",
            "[Training Epoch 60] Batch 2222, Loss 0.2708638906478882\n",
            "[Training Epoch 60] Batch 2223, Loss 0.2392989546060562\n",
            "[Training Epoch 60] Batch 2224, Loss 0.2535438537597656\n",
            "[Training Epoch 60] Batch 2225, Loss 0.2473580539226532\n",
            "[Training Epoch 60] Batch 2226, Loss 0.27733755111694336\n",
            "[Training Epoch 60] Batch 2227, Loss 0.27810588479042053\n",
            "[Training Epoch 60] Batch 2228, Loss 0.25447964668273926\n",
            "[Training Epoch 60] Batch 2229, Loss 0.27302250266075134\n",
            "[Training Epoch 60] Batch 2230, Loss 0.26727381348609924\n",
            "[Training Epoch 60] Batch 2231, Loss 0.2546337842941284\n",
            "[Training Epoch 60] Batch 2232, Loss 0.26484259963035583\n",
            "[Training Epoch 60] Batch 2233, Loss 0.2838331460952759\n",
            "[Training Epoch 60] Batch 2234, Loss 0.2687627971172333\n",
            "[Training Epoch 60] Batch 2235, Loss 0.2687830924987793\n",
            "[Training Epoch 60] Batch 2236, Loss 0.27395129203796387\n",
            "[Training Epoch 60] Batch 2237, Loss 0.23602983355522156\n",
            "[Training Epoch 60] Batch 2238, Loss 0.24856974184513092\n",
            "[Training Epoch 60] Batch 2239, Loss 0.2550097107887268\n",
            "[Training Epoch 60] Batch 2240, Loss 0.2564935088157654\n",
            "[Training Epoch 60] Batch 2241, Loss 0.2857212424278259\n",
            "[Training Epoch 60] Batch 2242, Loss 0.2613984942436218\n",
            "[Training Epoch 60] Batch 2243, Loss 0.2791103720664978\n",
            "[Training Epoch 60] Batch 2244, Loss 0.25744134187698364\n",
            "[Training Epoch 60] Batch 2245, Loss 0.24827918410301208\n",
            "[Training Epoch 60] Batch 2246, Loss 0.2695174217224121\n",
            "[Training Epoch 60] Batch 2247, Loss 0.2346775382757187\n",
            "[Training Epoch 60] Batch 2248, Loss 0.28424811363220215\n",
            "[Training Epoch 60] Batch 2249, Loss 0.2894556522369385\n",
            "[Training Epoch 60] Batch 2250, Loss 0.2346602827310562\n",
            "[Training Epoch 60] Batch 2251, Loss 0.2702890634536743\n",
            "[Training Epoch 60] Batch 2252, Loss 0.28058314323425293\n",
            "[Training Epoch 60] Batch 2253, Loss 0.2517356276512146\n",
            "[Training Epoch 60] Batch 2254, Loss 0.27886003255844116\n",
            "[Training Epoch 60] Batch 2255, Loss 0.2583189606666565\n",
            "[Training Epoch 60] Batch 2256, Loss 0.24099209904670715\n",
            "[Training Epoch 60] Batch 2257, Loss 0.2551926374435425\n",
            "[Training Epoch 60] Batch 2258, Loss 0.2659834027290344\n",
            "[Training Epoch 60] Batch 2259, Loss 0.2965419590473175\n",
            "[Training Epoch 60] Batch 2260, Loss 0.2738533616065979\n",
            "[Training Epoch 60] Batch 2261, Loss 0.2835307717323303\n",
            "[Training Epoch 60] Batch 2262, Loss 0.2785819470882416\n",
            "[Training Epoch 60] Batch 2263, Loss 0.24664831161499023\n",
            "[Training Epoch 60] Batch 2264, Loss 0.2612535357475281\n",
            "[Training Epoch 60] Batch 2265, Loss 0.25936347246170044\n",
            "[Training Epoch 60] Batch 2266, Loss 0.27433204650878906\n",
            "[Training Epoch 60] Batch 2267, Loss 0.2822876572608948\n",
            "[Training Epoch 60] Batch 2268, Loss 0.2750096321105957\n",
            "[Training Epoch 60] Batch 2269, Loss 0.25728800892829895\n",
            "[Training Epoch 60] Batch 2270, Loss 0.2823772132396698\n",
            "[Training Epoch 60] Batch 2271, Loss 0.2821468710899353\n",
            "[Training Epoch 60] Batch 2272, Loss 0.2518119812011719\n",
            "[Training Epoch 60] Batch 2273, Loss 0.2553899884223938\n",
            "[Training Epoch 60] Batch 2274, Loss 0.25468215346336365\n",
            "[Training Epoch 60] Batch 2275, Loss 0.2749621570110321\n",
            "[Training Epoch 60] Batch 2276, Loss 0.23814934492111206\n",
            "[Training Epoch 60] Batch 2277, Loss 0.24924814701080322\n",
            "[Training Epoch 60] Batch 2278, Loss 0.26504597067832947\n",
            "[Training Epoch 60] Batch 2279, Loss 0.2777217924594879\n",
            "[Training Epoch 60] Batch 2280, Loss 0.2548705041408539\n",
            "[Training Epoch 60] Batch 2281, Loss 0.253969669342041\n",
            "[Training Epoch 60] Batch 2282, Loss 0.24518701434135437\n",
            "[Training Epoch 60] Batch 2283, Loss 0.2410052865743637\n",
            "[Training Epoch 60] Batch 2284, Loss 0.2331267148256302\n",
            "[Training Epoch 60] Batch 2285, Loss 0.2622723877429962\n",
            "[Training Epoch 60] Batch 2286, Loss 0.26212841272354126\n",
            "[Training Epoch 60] Batch 2287, Loss 0.28481167554855347\n",
            "[Training Epoch 60] Batch 2288, Loss 0.2777751088142395\n",
            "[Training Epoch 60] Batch 2289, Loss 0.2441411018371582\n",
            "[Training Epoch 60] Batch 2290, Loss 0.2811444103717804\n",
            "[Training Epoch 60] Batch 2291, Loss 0.29696014523506165\n",
            "[Training Epoch 60] Batch 2292, Loss 0.25861403346061707\n",
            "[Training Epoch 60] Batch 2293, Loss 0.2613919675350189\n",
            "[Training Epoch 60] Batch 2294, Loss 0.24843360483646393\n",
            "[Training Epoch 60] Batch 2295, Loss 0.2673662602901459\n",
            "[Training Epoch 60] Batch 2296, Loss 0.2502490282058716\n",
            "[Training Epoch 60] Batch 2297, Loss 0.2720623016357422\n",
            "[Training Epoch 60] Batch 2298, Loss 0.3017283082008362\n",
            "[Training Epoch 60] Batch 2299, Loss 0.2896660566329956\n",
            "[Training Epoch 60] Batch 2300, Loss 0.25559550523757935\n",
            "[Training Epoch 60] Batch 2301, Loss 0.2526712119579315\n",
            "[Training Epoch 60] Batch 2302, Loss 0.24223867058753967\n",
            "[Training Epoch 60] Batch 2303, Loss 0.24900639057159424\n",
            "[Training Epoch 60] Batch 2304, Loss 0.29061537981033325\n",
            "[Training Epoch 60] Batch 2305, Loss 0.26820385456085205\n",
            "[Training Epoch 60] Batch 2306, Loss 0.3217092454433441\n",
            "[Training Epoch 60] Batch 2307, Loss 0.2417284995317459\n",
            "[Training Epoch 60] Batch 2308, Loss 0.24290385842323303\n",
            "[Training Epoch 60] Batch 2309, Loss 0.25643205642700195\n",
            "[Training Epoch 60] Batch 2310, Loss 0.2756885290145874\n",
            "[Training Epoch 60] Batch 2311, Loss 0.2620537579059601\n",
            "[Training Epoch 60] Batch 2312, Loss 0.25849032402038574\n",
            "[Training Epoch 60] Batch 2313, Loss 0.2780538499355316\n",
            "[Training Epoch 60] Batch 2314, Loss 0.25981220602989197\n",
            "[Training Epoch 60] Batch 2315, Loss 0.27947455644607544\n",
            "[Training Epoch 60] Batch 2316, Loss 0.2423757016658783\n",
            "[Training Epoch 60] Batch 2317, Loss 0.25620898604393005\n",
            "[Training Epoch 60] Batch 2318, Loss 0.27023178339004517\n",
            "[Training Epoch 60] Batch 2319, Loss 0.26040318608283997\n",
            "[Training Epoch 60] Batch 2320, Loss 0.2616337239742279\n",
            "[Training Epoch 60] Batch 2321, Loss 0.2626090347766876\n",
            "[Training Epoch 60] Batch 2322, Loss 0.2532373368740082\n",
            "[Training Epoch 60] Batch 2323, Loss 0.26505133509635925\n",
            "[Training Epoch 60] Batch 2324, Loss 0.26292574405670166\n",
            "[Training Epoch 60] Batch 2325, Loss 0.2873219847679138\n",
            "[Training Epoch 60] Batch 2326, Loss 0.24178679287433624\n",
            "[Training Epoch 60] Batch 2327, Loss 0.2591894567012787\n",
            "[Training Epoch 60] Batch 2328, Loss 0.2799220383167267\n",
            "[Training Epoch 60] Batch 2329, Loss 0.2564632296562195\n",
            "[Training Epoch 60] Batch 2330, Loss 0.2800823450088501\n",
            "[Training Epoch 60] Batch 2331, Loss 0.26708975434303284\n",
            "[Training Epoch 60] Batch 2332, Loss 0.25278156995773315\n",
            "[Training Epoch 60] Batch 2333, Loss 0.2672375440597534\n",
            "[Training Epoch 60] Batch 2334, Loss 0.26451003551483154\n",
            "[Training Epoch 60] Batch 2335, Loss 0.274702787399292\n",
            "[Training Epoch 60] Batch 2336, Loss 0.289267361164093\n",
            "[Training Epoch 60] Batch 2337, Loss 0.26793336868286133\n",
            "[Training Epoch 60] Batch 2338, Loss 0.2653716802597046\n",
            "[Training Epoch 60] Batch 2339, Loss 0.2862943410873413\n",
            "[Training Epoch 60] Batch 2340, Loss 0.2707116901874542\n",
            "[Training Epoch 60] Batch 2341, Loss 0.26776036620140076\n",
            "[Training Epoch 60] Batch 2342, Loss 0.2494744062423706\n",
            "[Training Epoch 60] Batch 2343, Loss 0.2685583233833313\n",
            "[Training Epoch 60] Batch 2344, Loss 0.22824499011039734\n",
            "[Training Epoch 60] Batch 2345, Loss 0.272562175989151\n",
            "[Training Epoch 60] Batch 2346, Loss 0.251542329788208\n",
            "[Training Epoch 60] Batch 2347, Loss 0.27501440048217773\n",
            "[Training Epoch 60] Batch 2348, Loss 0.2471400648355484\n",
            "[Training Epoch 60] Batch 2349, Loss 0.2557615041732788\n",
            "[Training Epoch 60] Batch 2350, Loss 0.26302018761634827\n",
            "[Training Epoch 60] Batch 2351, Loss 0.2570010721683502\n",
            "[Training Epoch 60] Batch 2352, Loss 0.27967730164527893\n",
            "[Training Epoch 60] Batch 2353, Loss 0.24170248210430145\n",
            "[Training Epoch 60] Batch 2354, Loss 0.2576664686203003\n",
            "[Training Epoch 60] Batch 2355, Loss 0.23767626285552979\n",
            "[Training Epoch 60] Batch 2356, Loss 0.23832890391349792\n",
            "[Training Epoch 60] Batch 2357, Loss 0.26809781789779663\n",
            "[Training Epoch 60] Batch 2358, Loss 0.2645713686943054\n",
            "[Training Epoch 60] Batch 2359, Loss 0.27242177724838257\n",
            "[Training Epoch 60] Batch 2360, Loss 0.2627217173576355\n",
            "[Training Epoch 60] Batch 2361, Loss 0.29149967432022095\n",
            "[Training Epoch 60] Batch 2362, Loss 0.23896589875221252\n",
            "[Training Epoch 60] Batch 2363, Loss 0.25137990713119507\n",
            "[Training Epoch 60] Batch 2364, Loss 0.2401093691587448\n",
            "[Training Epoch 60] Batch 2365, Loss 0.2557746469974518\n",
            "[Training Epoch 60] Batch 2366, Loss 0.2589772343635559\n",
            "[Training Epoch 60] Batch 2367, Loss 0.23961889743804932\n",
            "[Training Epoch 60] Batch 2368, Loss 0.278761625289917\n",
            "[Training Epoch 60] Batch 2369, Loss 0.2727097272872925\n",
            "[Training Epoch 60] Batch 2370, Loss 0.2528853118419647\n",
            "[Training Epoch 60] Batch 2371, Loss 0.23650921881198883\n",
            "[Training Epoch 60] Batch 2372, Loss 0.2360154390335083\n",
            "[Training Epoch 60] Batch 2373, Loss 0.25524264574050903\n",
            "[Training Epoch 60] Batch 2374, Loss 0.24620544910430908\n",
            "[Training Epoch 60] Batch 2375, Loss 0.2617707848548889\n",
            "[Training Epoch 60] Batch 2376, Loss 0.27472326159477234\n",
            "[Training Epoch 60] Batch 2377, Loss 0.2533092200756073\n",
            "[Training Epoch 60] Batch 2378, Loss 0.2474256008863449\n",
            "[Training Epoch 60] Batch 2379, Loss 0.27836641669273376\n",
            "[Training Epoch 60] Batch 2380, Loss 0.25804823637008667\n",
            "[Training Epoch 60] Batch 2381, Loss 0.2860076427459717\n",
            "[Training Epoch 60] Batch 2382, Loss 0.27599450945854187\n",
            "[Training Epoch 60] Batch 2383, Loss 0.25948357582092285\n",
            "[Training Epoch 60] Batch 2384, Loss 0.2933921217918396\n",
            "[Training Epoch 60] Batch 2385, Loss 0.25918564200401306\n",
            "[Training Epoch 60] Batch 2386, Loss 0.26671624183654785\n",
            "[Training Epoch 60] Batch 2387, Loss 0.2522723972797394\n",
            "[Training Epoch 60] Batch 2388, Loss 0.2622409760951996\n",
            "[Training Epoch 60] Batch 2389, Loss 0.24478550255298615\n",
            "[Training Epoch 60] Batch 2390, Loss 0.2664300203323364\n",
            "[Training Epoch 60] Batch 2391, Loss 0.259962797164917\n",
            "[Training Epoch 60] Batch 2392, Loss 0.265789270401001\n",
            "[Training Epoch 60] Batch 2393, Loss 0.23386655747890472\n",
            "[Training Epoch 60] Batch 2394, Loss 0.27957382798194885\n",
            "[Training Epoch 60] Batch 2395, Loss 0.28146639466285706\n",
            "[Training Epoch 60] Batch 2396, Loss 0.2804052531719208\n",
            "[Training Epoch 60] Batch 2397, Loss 0.27805349230766296\n",
            "[Training Epoch 60] Batch 2398, Loss 0.25514093041419983\n",
            "[Training Epoch 60] Batch 2399, Loss 0.2544165551662445\n",
            "[Training Epoch 60] Batch 2400, Loss 0.23505188524723053\n",
            "[Training Epoch 60] Batch 2401, Loss 0.24468787014484406\n",
            "[Training Epoch 60] Batch 2402, Loss 0.2728879451751709\n",
            "[Training Epoch 60] Batch 2403, Loss 0.2699558436870575\n",
            "[Training Epoch 60] Batch 2404, Loss 0.26535987854003906\n",
            "[Training Epoch 60] Batch 2405, Loss 0.2770530581474304\n",
            "[Training Epoch 60] Batch 2406, Loss 0.31018176674842834\n",
            "[Training Epoch 60] Batch 2407, Loss 0.2848835587501526\n",
            "[Training Epoch 60] Batch 2408, Loss 0.28360167145729065\n",
            "[Training Epoch 60] Batch 2409, Loss 0.2847885489463806\n",
            "[Training Epoch 60] Batch 2410, Loss 0.28453606367111206\n",
            "[Training Epoch 60] Batch 2411, Loss 0.2719065248966217\n",
            "[Training Epoch 60] Batch 2412, Loss 0.2616305947303772\n",
            "[Training Epoch 60] Batch 2413, Loss 0.2540423274040222\n",
            "[Training Epoch 60] Batch 2414, Loss 0.24103859066963196\n",
            "[Training Epoch 60] Batch 2415, Loss 0.2529403269290924\n",
            "[Training Epoch 60] Batch 2416, Loss 0.26376253366470337\n",
            "[Training Epoch 60] Batch 2417, Loss 0.2900886535644531\n",
            "[Training Epoch 60] Batch 2418, Loss 0.27422836422920227\n",
            "[Training Epoch 60] Batch 2419, Loss 0.2558981478214264\n",
            "[Training Epoch 60] Batch 2420, Loss 0.25771620869636536\n",
            "[Training Epoch 60] Batch 2421, Loss 0.2804282605648041\n",
            "[Training Epoch 60] Batch 2422, Loss 0.2712239921092987\n",
            "[Training Epoch 60] Batch 2423, Loss 0.2805526554584503\n",
            "[Training Epoch 60] Batch 2424, Loss 0.22668719291687012\n",
            "[Training Epoch 60] Batch 2425, Loss 0.28997355699539185\n",
            "[Training Epoch 60] Batch 2426, Loss 0.273362934589386\n",
            "[Training Epoch 60] Batch 2427, Loss 0.25203871726989746\n",
            "[Training Epoch 60] Batch 2428, Loss 0.25564128160476685\n",
            "[Training Epoch 60] Batch 2429, Loss 0.26643961668014526\n",
            "[Training Epoch 60] Batch 2430, Loss 0.24935826659202576\n",
            "[Training Epoch 60] Batch 2431, Loss 0.23627041280269623\n",
            "[Training Epoch 60] Batch 2432, Loss 0.25628402829170227\n",
            "[Training Epoch 60] Batch 2433, Loss 0.25780555605888367\n",
            "[Training Epoch 60] Batch 2434, Loss 0.27490466833114624\n",
            "[Training Epoch 60] Batch 2435, Loss 0.2820453941822052\n",
            "[Training Epoch 60] Batch 2436, Loss 0.23544929921627045\n",
            "[Training Epoch 60] Batch 2437, Loss 0.2641661763191223\n",
            "[Training Epoch 60] Batch 2438, Loss 0.2432662546634674\n",
            "[Training Epoch 60] Batch 2439, Loss 0.26421624422073364\n",
            "[Training Epoch 60] Batch 2440, Loss 0.2470531165599823\n",
            "[Training Epoch 60] Batch 2441, Loss 0.2736603021621704\n",
            "[Training Epoch 60] Batch 2442, Loss 0.22542110085487366\n",
            "[Training Epoch 60] Batch 2443, Loss 0.2441992610692978\n",
            "[Training Epoch 60] Batch 2444, Loss 0.264228492975235\n",
            "[Training Epoch 60] Batch 2445, Loss 0.26303064823150635\n",
            "[Training Epoch 60] Batch 2446, Loss 0.290541410446167\n",
            "[Training Epoch 60] Batch 2447, Loss 0.2696540057659149\n",
            "[Training Epoch 60] Batch 2448, Loss 0.269599050283432\n",
            "[Training Epoch 60] Batch 2449, Loss 0.25989168882369995\n",
            "[Training Epoch 60] Batch 2450, Loss 0.25849035382270813\n",
            "[Training Epoch 60] Batch 2451, Loss 0.27431055903434753\n",
            "[Training Epoch 60] Batch 2452, Loss 0.28340944647789\n",
            "[Training Epoch 60] Batch 2453, Loss 0.25907665491104126\n",
            "[Training Epoch 60] Batch 2454, Loss 0.26458656787872314\n",
            "[Training Epoch 60] Batch 2455, Loss 0.2762547731399536\n",
            "[Training Epoch 60] Batch 2456, Loss 0.24524778127670288\n",
            "[Training Epoch 60] Batch 2457, Loss 0.27838388085365295\n",
            "[Training Epoch 60] Batch 2458, Loss 0.280616819858551\n",
            "[Training Epoch 60] Batch 2459, Loss 0.2759028375148773\n",
            "[Training Epoch 60] Batch 2460, Loss 0.2733702063560486\n",
            "[Training Epoch 60] Batch 2461, Loss 0.27255845069885254\n",
            "[Training Epoch 60] Batch 2462, Loss 0.30539876222610474\n",
            "[Training Epoch 60] Batch 2463, Loss 0.268279105424881\n",
            "[Training Epoch 60] Batch 2464, Loss 0.27109211683273315\n",
            "[Training Epoch 60] Batch 2465, Loss 0.25263550877571106\n",
            "[Training Epoch 60] Batch 2466, Loss 0.25279372930526733\n",
            "[Training Epoch 60] Batch 2467, Loss 0.2848845422267914\n",
            "[Training Epoch 60] Batch 2468, Loss 0.24908268451690674\n",
            "[Training Epoch 60] Batch 2469, Loss 0.2584826350212097\n",
            "[Training Epoch 60] Batch 2470, Loss 0.2659642696380615\n",
            "[Training Epoch 60] Batch 2471, Loss 0.26948869228363037\n",
            "[Training Epoch 60] Batch 2472, Loss 0.26921382546424866\n",
            "[Training Epoch 60] Batch 2473, Loss 0.26384881138801575\n",
            "[Training Epoch 60] Batch 2474, Loss 0.2789180874824524\n",
            "[Training Epoch 60] Batch 2475, Loss 0.25334084033966064\n",
            "[Training Epoch 60] Batch 2476, Loss 0.2649592161178589\n",
            "[Training Epoch 60] Batch 2477, Loss 0.2726674973964691\n",
            "[Training Epoch 60] Batch 2478, Loss 0.23672068119049072\n",
            "[Training Epoch 60] Batch 2479, Loss 0.2483903169631958\n",
            "[Training Epoch 60] Batch 2480, Loss 0.2422734797000885\n",
            "[Training Epoch 60] Batch 2481, Loss 0.2772035300731659\n",
            "[Training Epoch 60] Batch 2482, Loss 0.23053467273712158\n",
            "[Training Epoch 60] Batch 2483, Loss 0.25926584005355835\n",
            "[Training Epoch 60] Batch 2484, Loss 0.24711576104164124\n",
            "[Training Epoch 60] Batch 2485, Loss 0.25709986686706543\n",
            "[Training Epoch 60] Batch 2486, Loss 0.28002339601516724\n",
            "[Training Epoch 60] Batch 2487, Loss 0.3028322458267212\n",
            "[Training Epoch 60] Batch 2488, Loss 0.2475358098745346\n",
            "[Training Epoch 60] Batch 2489, Loss 0.2692003846168518\n",
            "[Training Epoch 60] Batch 2490, Loss 0.24965761601924896\n",
            "[Training Epoch 60] Batch 2491, Loss 0.2574150562286377\n",
            "[Training Epoch 60] Batch 2492, Loss 0.2637445330619812\n",
            "[Training Epoch 60] Batch 2493, Loss 0.28506332635879517\n",
            "[Training Epoch 60] Batch 2494, Loss 0.24971605837345123\n",
            "[Training Epoch 60] Batch 2495, Loss 0.2608441114425659\n",
            "[Training Epoch 60] Batch 2496, Loss 0.24700839817523956\n",
            "[Training Epoch 60] Batch 2497, Loss 0.24680498242378235\n",
            "[Training Epoch 60] Batch 2498, Loss 0.2831250727176666\n",
            "[Training Epoch 60] Batch 2499, Loss 0.2740020155906677\n",
            "[Training Epoch 60] Batch 2500, Loss 0.25785863399505615\n",
            "[Training Epoch 60] Batch 2501, Loss 0.28689900040626526\n",
            "[Training Epoch 60] Batch 2502, Loss 0.2627163231372833\n",
            "[Training Epoch 60] Batch 2503, Loss 0.27177557349205017\n",
            "[Training Epoch 60] Batch 2504, Loss 0.2543073892593384\n",
            "[Training Epoch 60] Batch 2505, Loss 0.2734755575656891\n",
            "[Training Epoch 60] Batch 2506, Loss 0.2620241045951843\n",
            "[Training Epoch 60] Batch 2507, Loss 0.2534412741661072\n",
            "[Training Epoch 60] Batch 2508, Loss 0.2623421251773834\n",
            "[Training Epoch 60] Batch 2509, Loss 0.257717490196228\n",
            "[Training Epoch 60] Batch 2510, Loss 0.22621266543865204\n",
            "[Training Epoch 60] Batch 2511, Loss 0.2657836079597473\n",
            "[Training Epoch 60] Batch 2512, Loss 0.25728729367256165\n",
            "[Training Epoch 60] Batch 2513, Loss 0.2885386645793915\n",
            "[Training Epoch 60] Batch 2514, Loss 0.2548537254333496\n",
            "[Training Epoch 60] Batch 2515, Loss 0.26037487387657166\n",
            "[Training Epoch 60] Batch 2516, Loss 0.2748088836669922\n",
            "[Training Epoch 60] Batch 2517, Loss 0.27384644746780396\n",
            "[Training Epoch 60] Batch 2518, Loss 0.27483904361724854\n",
            "[Training Epoch 60] Batch 2519, Loss 0.22482679784297943\n",
            "[Training Epoch 60] Batch 2520, Loss 0.2531408965587616\n",
            "[Training Epoch 60] Batch 2521, Loss 0.2696515619754791\n",
            "[Training Epoch 60] Batch 2522, Loss 0.2975907325744629\n",
            "[Training Epoch 60] Batch 2523, Loss 0.30072805285453796\n",
            "[Training Epoch 60] Batch 2524, Loss 0.2673285901546478\n",
            "[Training Epoch 60] Batch 2525, Loss 0.26776349544525146\n",
            "[Training Epoch 60] Batch 2526, Loss 0.2641272246837616\n",
            "[Training Epoch 60] Batch 2527, Loss 0.27626994252204895\n",
            "[Training Epoch 60] Batch 2528, Loss 0.2474818378686905\n",
            "[Training Epoch 60] Batch 2529, Loss 0.27323204278945923\n",
            "[Training Epoch 60] Batch 2530, Loss 0.2792550325393677\n",
            "[Training Epoch 60] Batch 2531, Loss 0.2242385894060135\n",
            "[Training Epoch 60] Batch 2532, Loss 0.26199787855148315\n",
            "[Training Epoch 60] Batch 2533, Loss 0.28340500593185425\n",
            "[Training Epoch 60] Batch 2534, Loss 0.2442169338464737\n",
            "[Training Epoch 60] Batch 2535, Loss 0.2465134561061859\n",
            "[Training Epoch 60] Batch 2536, Loss 0.2709376811981201\n",
            "[Training Epoch 60] Batch 2537, Loss 0.26424121856689453\n",
            "[Training Epoch 60] Batch 2538, Loss 0.27954965829849243\n",
            "[Training Epoch 60] Batch 2539, Loss 0.23925895988941193\n",
            "[Training Epoch 60] Batch 2540, Loss 0.2596503794193268\n",
            "[Training Epoch 60] Batch 2541, Loss 0.26798558235168457\n",
            "[Training Epoch 60] Batch 2542, Loss 0.30776768922805786\n",
            "[Training Epoch 60] Batch 2543, Loss 0.2503686249256134\n",
            "[Training Epoch 60] Batch 2544, Loss 0.29763561487197876\n",
            "[Training Epoch 60] Batch 2545, Loss 0.2839663326740265\n",
            "[Training Epoch 60] Batch 2546, Loss 0.262001097202301\n",
            "[Training Epoch 60] Batch 2547, Loss 0.2752632796764374\n",
            "[Training Epoch 60] Batch 2548, Loss 0.25780168175697327\n",
            "[Training Epoch 60] Batch 2549, Loss 0.27860134840011597\n",
            "[Training Epoch 60] Batch 2550, Loss 0.25632303953170776\n",
            "[Training Epoch 60] Batch 2551, Loss 0.27797117829322815\n",
            "[Training Epoch 60] Batch 2552, Loss 0.24079880118370056\n",
            "[Training Epoch 60] Batch 2553, Loss 0.2609317898750305\n",
            "[Training Epoch 60] Batch 2554, Loss 0.2757251560688019\n",
            "[Training Epoch 60] Batch 2555, Loss 0.25774288177490234\n",
            "[Training Epoch 60] Batch 2556, Loss 0.27155202627182007\n",
            "[Training Epoch 60] Batch 2557, Loss 0.26229721307754517\n",
            "[Training Epoch 60] Batch 2558, Loss 0.26563286781311035\n",
            "[Training Epoch 60] Batch 2559, Loss 0.3042566180229187\n",
            "[Training Epoch 60] Batch 2560, Loss 0.2967783510684967\n",
            "[Training Epoch 60] Batch 2561, Loss 0.27991679310798645\n",
            "[Training Epoch 60] Batch 2562, Loss 0.27257034182548523\n",
            "[Training Epoch 60] Batch 2563, Loss 0.2505435347557068\n",
            "[Training Epoch 60] Batch 2564, Loss 0.25448983907699585\n",
            "[Training Epoch 60] Batch 2565, Loss 0.27589720487594604\n",
            "[Training Epoch 60] Batch 2566, Loss 0.2734958529472351\n",
            "[Training Epoch 60] Batch 2567, Loss 0.24380527436733246\n",
            "[Training Epoch 60] Batch 2568, Loss 0.27372094988822937\n",
            "[Training Epoch 60] Batch 2569, Loss 0.2730334997177124\n",
            "[Training Epoch 60] Batch 2570, Loss 0.27761542797088623\n",
            "[Training Epoch 60] Batch 2571, Loss 0.26667341589927673\n",
            "[Training Epoch 60] Batch 2572, Loss 0.267391562461853\n",
            "[Training Epoch 60] Batch 2573, Loss 0.256242573261261\n",
            "[Training Epoch 60] Batch 2574, Loss 0.3001209795475006\n",
            "[Training Epoch 60] Batch 2575, Loss 0.22725029289722443\n",
            "[Training Epoch 60] Batch 2576, Loss 0.2546278238296509\n",
            "[Training Epoch 60] Batch 2577, Loss 0.2566685676574707\n",
            "[Training Epoch 60] Batch 2578, Loss 0.2825337052345276\n",
            "[Training Epoch 60] Batch 2579, Loss 0.2573874592781067\n",
            "[Training Epoch 60] Batch 2580, Loss 0.24588006734848022\n",
            "[Training Epoch 60] Batch 2581, Loss 0.27539461851119995\n",
            "[Training Epoch 60] Batch 2582, Loss 0.2525537312030792\n",
            "[Training Epoch 60] Batch 2583, Loss 0.2527065575122833\n",
            "[Training Epoch 60] Batch 2584, Loss 0.2635686695575714\n",
            "[Training Epoch 60] Batch 2585, Loss 0.2731132209300995\n",
            "[Training Epoch 60] Batch 2586, Loss 0.26638373732566833\n",
            "[Training Epoch 60] Batch 2587, Loss 0.26411864161491394\n",
            "[Training Epoch 60] Batch 2588, Loss 0.2641945481300354\n",
            "[Training Epoch 60] Batch 2589, Loss 0.2731725871562958\n",
            "[Training Epoch 60] Batch 2590, Loss 0.27315574884414673\n",
            "[Training Epoch 60] Batch 2591, Loss 0.243165522813797\n",
            "[Training Epoch 60] Batch 2592, Loss 0.28118520975112915\n",
            "[Training Epoch 60] Batch 2593, Loss 0.2726369798183441\n",
            "[Training Epoch 60] Batch 2594, Loss 0.26098737120628357\n",
            "[Training Epoch 60] Batch 2595, Loss 0.263070672750473\n",
            "[Training Epoch 60] Batch 2596, Loss 0.261911004781723\n",
            "[Training Epoch 60] Batch 2597, Loss 0.2684074342250824\n",
            "[Training Epoch 60] Batch 2598, Loss 0.24866029620170593\n",
            "[Training Epoch 60] Batch 2599, Loss 0.25382035970687866\n",
            "[Training Epoch 60] Batch 2600, Loss 0.25770115852355957\n",
            "[Training Epoch 60] Batch 2601, Loss 0.24595487117767334\n",
            "[Training Epoch 60] Batch 2602, Loss 0.27348557114601135\n",
            "[Training Epoch 60] Batch 2603, Loss 0.24913537502288818\n",
            "[Training Epoch 60] Batch 2604, Loss 0.2636679410934448\n",
            "[Training Epoch 60] Batch 2605, Loss 0.21315789222717285\n",
            "[Training Epoch 60] Batch 2606, Loss 0.2640727460384369\n",
            "[Training Epoch 60] Batch 2607, Loss 0.26463383436203003\n",
            "[Training Epoch 60] Batch 2608, Loss 0.2509477436542511\n",
            "[Training Epoch 60] Batch 2609, Loss 0.26409560441970825\n",
            "[Training Epoch 60] Batch 2610, Loss 0.25650936365127563\n",
            "[Training Epoch 60] Batch 2611, Loss 0.2564878463745117\n",
            "[Training Epoch 60] Batch 2612, Loss 0.28126490116119385\n",
            "[Training Epoch 60] Batch 2613, Loss 0.2766837179660797\n",
            "[Training Epoch 60] Batch 2614, Loss 0.26951637864112854\n",
            "[Training Epoch 60] Batch 2615, Loss 0.275823175907135\n",
            "[Training Epoch 60] Batch 2616, Loss 0.2508072555065155\n",
            "[Training Epoch 60] Batch 2617, Loss 0.25099247694015503\n",
            "[Training Epoch 60] Batch 2618, Loss 0.24000047147274017\n",
            "[Training Epoch 60] Batch 2619, Loss 0.2865908145904541\n",
            "[Training Epoch 60] Batch 2620, Loss 0.2832067906856537\n",
            "[Training Epoch 60] Batch 2621, Loss 0.28042536973953247\n",
            "[Training Epoch 60] Batch 2622, Loss 0.2797437608242035\n",
            "[Training Epoch 60] Batch 2623, Loss 0.2673795521259308\n",
            "[Training Epoch 60] Batch 2624, Loss 0.25630638003349304\n",
            "[Training Epoch 60] Batch 2625, Loss 0.23619617521762848\n",
            "[Training Epoch 60] Batch 2626, Loss 0.29393139481544495\n",
            "[Training Epoch 60] Batch 2627, Loss 0.2574113607406616\n",
            "[Training Epoch 60] Batch 2628, Loss 0.25439170002937317\n",
            "[Training Epoch 60] Batch 2629, Loss 0.27832937240600586\n",
            "[Training Epoch 60] Batch 2630, Loss 0.2484665811061859\n",
            "[Training Epoch 60] Batch 2631, Loss 0.2712303400039673\n",
            "[Training Epoch 60] Batch 2632, Loss 0.2658369541168213\n",
            "[Training Epoch 60] Batch 2633, Loss 0.27785947918891907\n",
            "[Training Epoch 60] Batch 2634, Loss 0.2743677496910095\n",
            "[Training Epoch 60] Batch 2635, Loss 0.26977449655532837\n",
            "[Training Epoch 60] Batch 2636, Loss 0.2602917551994324\n",
            "[Training Epoch 60] Batch 2637, Loss 0.2859366238117218\n",
            "[Training Epoch 60] Batch 2638, Loss 0.2615734934806824\n",
            "[Training Epoch 60] Batch 2639, Loss 0.2858683466911316\n",
            "[Training Epoch 60] Batch 2640, Loss 0.2592012882232666\n",
            "[Training Epoch 60] Batch 2641, Loss 0.2829161286354065\n",
            "[Training Epoch 60] Batch 2642, Loss 0.26779890060424805\n",
            "[Training Epoch 60] Batch 2643, Loss 0.27442359924316406\n",
            "[Training Epoch 60] Batch 2644, Loss 0.2563476860523224\n",
            "[Training Epoch 60] Batch 2645, Loss 0.257254958152771\n",
            "[Training Epoch 60] Batch 2646, Loss 0.24554558098316193\n",
            "[Training Epoch 60] Batch 2647, Loss 0.29107171297073364\n",
            "[Training Epoch 60] Batch 2648, Loss 0.28843772411346436\n",
            "[Training Epoch 60] Batch 2649, Loss 0.27232080698013306\n",
            "[Training Epoch 60] Batch 2650, Loss 0.2627778649330139\n",
            "[Training Epoch 60] Batch 2651, Loss 0.26737675070762634\n",
            "[Training Epoch 60] Batch 2652, Loss 0.2836417853832245\n",
            "[Training Epoch 60] Batch 2653, Loss 0.2620016932487488\n",
            "[Training Epoch 60] Batch 2654, Loss 0.27425745129585266\n",
            "[Training Epoch 60] Batch 2655, Loss 0.27061405777931213\n",
            "[Training Epoch 60] Batch 2656, Loss 0.2602079212665558\n",
            "[Training Epoch 60] Batch 2657, Loss 0.2644250988960266\n",
            "[Training Epoch 60] Batch 2658, Loss 0.2722344994544983\n",
            "[Training Epoch 60] Batch 2659, Loss 0.278095006942749\n",
            "[Training Epoch 60] Batch 2660, Loss 0.25919538736343384\n",
            "[Training Epoch 60] Batch 2661, Loss 0.23727749288082123\n",
            "[Training Epoch 60] Batch 2662, Loss 0.29927027225494385\n",
            "[Training Epoch 60] Batch 2663, Loss 0.24214065074920654\n",
            "[Training Epoch 60] Batch 2664, Loss 0.24424797296524048\n",
            "[Training Epoch 60] Batch 2665, Loss 0.264635294675827\n",
            "[Training Epoch 60] Batch 2666, Loss 0.23926149308681488\n",
            "[Training Epoch 60] Batch 2667, Loss 0.2706286609172821\n",
            "[Training Epoch 60] Batch 2668, Loss 0.2712492346763611\n",
            "[Training Epoch 60] Batch 2669, Loss 0.2745327353477478\n",
            "[Training Epoch 60] Batch 2670, Loss 0.2537563443183899\n",
            "[Training Epoch 60] Batch 2671, Loss 0.27043652534484863\n",
            "[Training Epoch 60] Batch 2672, Loss 0.24558493494987488\n",
            "[Training Epoch 60] Batch 2673, Loss 0.2611387073993683\n",
            "[Training Epoch 60] Batch 2674, Loss 0.2886446416378021\n",
            "[Training Epoch 60] Batch 2675, Loss 0.2767646312713623\n",
            "[Training Epoch 60] Batch 2676, Loss 0.2572018802165985\n",
            "[Training Epoch 60] Batch 2677, Loss 0.2649003565311432\n",
            "[Training Epoch 60] Batch 2678, Loss 0.24370726943016052\n",
            "[Training Epoch 60] Batch 2679, Loss 0.23469501733779907\n",
            "[Training Epoch 60] Batch 2680, Loss 0.26499444246292114\n",
            "[Training Epoch 60] Batch 2681, Loss 0.27512288093566895\n",
            "[Training Epoch 60] Batch 2682, Loss 0.265977144241333\n",
            "[Training Epoch 60] Batch 2683, Loss 0.25375160574913025\n",
            "[Training Epoch 60] Batch 2684, Loss 0.2583726644515991\n",
            "[Training Epoch 60] Batch 2685, Loss 0.2781504690647125\n",
            "[Training Epoch 60] Batch 2686, Loss 0.2663346529006958\n",
            "[Training Epoch 60] Batch 2687, Loss 0.28464218974113464\n",
            "[Training Epoch 60] Batch 2688, Loss 0.2523801624774933\n",
            "[Training Epoch 60] Batch 2689, Loss 0.26029205322265625\n",
            "[Training Epoch 60] Batch 2690, Loss 0.2692190110683441\n",
            "[Training Epoch 60] Batch 2691, Loss 0.23687154054641724\n",
            "[Training Epoch 60] Batch 2692, Loss 0.2516331672668457\n",
            "[Training Epoch 60] Batch 2693, Loss 0.2655150592327118\n",
            "[Training Epoch 60] Batch 2694, Loss 0.2734888792037964\n",
            "[Training Epoch 60] Batch 2695, Loss 0.2542082667350769\n",
            "[Training Epoch 60] Batch 2696, Loss 0.2787766456604004\n",
            "[Training Epoch 60] Batch 2697, Loss 0.2579655349254608\n",
            "[Training Epoch 60] Batch 2698, Loss 0.23756690323352814\n",
            "[Training Epoch 60] Batch 2699, Loss 0.23194539546966553\n",
            "[Training Epoch 60] Batch 2700, Loss 0.24446183443069458\n",
            "[Training Epoch 60] Batch 2701, Loss 0.29063737392425537\n",
            "[Training Epoch 60] Batch 2702, Loss 0.2826436460018158\n",
            "[Training Epoch 60] Batch 2703, Loss 0.25179705023765564\n",
            "[Training Epoch 60] Batch 2704, Loss 0.2719234824180603\n",
            "[Training Epoch 60] Batch 2705, Loss 0.24116146564483643\n",
            "[Training Epoch 60] Batch 2706, Loss 0.27136126160621643\n",
            "[Training Epoch 60] Batch 2707, Loss 0.28746119141578674\n",
            "[Training Epoch 60] Batch 2708, Loss 0.2490740567445755\n",
            "[Training Epoch 60] Batch 2709, Loss 0.2643366754055023\n",
            "[Training Epoch 60] Batch 2710, Loss 0.2791432738304138\n",
            "[Training Epoch 60] Batch 2711, Loss 0.26458531618118286\n",
            "[Training Epoch 60] Batch 2712, Loss 0.26074978709220886\n",
            "[Training Epoch 60] Batch 2713, Loss 0.2793407738208771\n",
            "[Training Epoch 60] Batch 2714, Loss 0.26711970567703247\n",
            "[Training Epoch 60] Batch 2715, Loss 0.25091391801834106\n",
            "[Training Epoch 60] Batch 2716, Loss 0.25764796137809753\n",
            "[Training Epoch 60] Batch 2717, Loss 0.2744026780128479\n",
            "[Training Epoch 60] Batch 2718, Loss 0.2588438391685486\n",
            "[Training Epoch 60] Batch 2719, Loss 0.25732800364494324\n",
            "[Training Epoch 60] Batch 2720, Loss 0.28091830015182495\n",
            "[Training Epoch 60] Batch 2721, Loss 0.2465355545282364\n",
            "[Training Epoch 60] Batch 2722, Loss 0.26243433356285095\n",
            "[Training Epoch 60] Batch 2723, Loss 0.25309550762176514\n",
            "[Training Epoch 60] Batch 2724, Loss 0.2892272472381592\n",
            "[Training Epoch 60] Batch 2725, Loss 0.2384776473045349\n",
            "[Training Epoch 60] Batch 2726, Loss 0.2854405343532562\n",
            "[Training Epoch 60] Batch 2727, Loss 0.2800711989402771\n",
            "[Training Epoch 60] Batch 2728, Loss 0.25209948420524597\n",
            "[Training Epoch 60] Batch 2729, Loss 0.2426900863647461\n",
            "[Training Epoch 60] Batch 2730, Loss 0.2804008722305298\n",
            "[Training Epoch 60] Batch 2731, Loss 0.25803637504577637\n",
            "[Training Epoch 60] Batch 2732, Loss 0.27750739455223083\n",
            "[Training Epoch 60] Batch 2733, Loss 0.3007810115814209\n",
            "[Training Epoch 60] Batch 2734, Loss 0.26855507493019104\n",
            "[Training Epoch 60] Batch 2735, Loss 0.25587084889411926\n",
            "[Training Epoch 60] Batch 2736, Loss 0.2790110111236572\n",
            "[Training Epoch 60] Batch 2737, Loss 0.24702909588813782\n",
            "[Training Epoch 60] Batch 2738, Loss 0.27920329570770264\n",
            "[Training Epoch 60] Batch 2739, Loss 0.2574877142906189\n",
            "[Training Epoch 60] Batch 2740, Loss 0.2508949637413025\n",
            "[Training Epoch 60] Batch 2741, Loss 0.28600016236305237\n",
            "[Training Epoch 60] Batch 2742, Loss 0.248233363032341\n",
            "[Training Epoch 60] Batch 2743, Loss 0.2527347207069397\n",
            "[Training Epoch 60] Batch 2744, Loss 0.2785939574241638\n",
            "[Training Epoch 60] Batch 2745, Loss 0.26782312989234924\n",
            "[Training Epoch 60] Batch 2746, Loss 0.23405314981937408\n",
            "[Training Epoch 60] Batch 2747, Loss 0.24686133861541748\n",
            "[Training Epoch 60] Batch 2748, Loss 0.26648011803627014\n",
            "[Training Epoch 60] Batch 2749, Loss 0.2563679814338684\n",
            "[Training Epoch 60] Batch 2750, Loss 0.26208624243736267\n",
            "[Training Epoch 60] Batch 2751, Loss 0.2809317111968994\n",
            "[Training Epoch 60] Batch 2752, Loss 0.2705965042114258\n",
            "[Training Epoch 60] Batch 2753, Loss 0.26896852254867554\n",
            "[Training Epoch 60] Batch 2754, Loss 0.2760082185268402\n",
            "[Training Epoch 60] Batch 2755, Loss 0.30132848024368286\n",
            "[Training Epoch 60] Batch 2756, Loss 0.2820952832698822\n",
            "[Training Epoch 60] Batch 2757, Loss 0.2625742554664612\n",
            "[Training Epoch 60] Batch 2758, Loss 0.2513159215450287\n",
            "[Training Epoch 60] Batch 2759, Loss 0.2595342993736267\n",
            "[Training Epoch 60] Batch 2760, Loss 0.25905707478523254\n",
            "[Training Epoch 60] Batch 2761, Loss 0.24607983231544495\n",
            "[Training Epoch 60] Batch 2762, Loss 0.25817811489105225\n",
            "[Training Epoch 60] Batch 2763, Loss 0.2840796113014221\n",
            "[Training Epoch 60] Batch 2764, Loss 0.27143993973731995\n",
            "[Training Epoch 60] Batch 2765, Loss 0.28928571939468384\n",
            "[Training Epoch 60] Batch 2766, Loss 0.27412664890289307\n",
            "[Training Epoch 60] Batch 2767, Loss 0.25619980692863464\n",
            "[Training Epoch 60] Batch 2768, Loss 0.23852914571762085\n",
            "[Training Epoch 60] Batch 2769, Loss 0.2761954069137573\n",
            "[Training Epoch 60] Batch 2770, Loss 0.25710225105285645\n",
            "[Training Epoch 60] Batch 2771, Loss 0.2521734833717346\n",
            "[Training Epoch 60] Batch 2772, Loss 0.2900976240634918\n",
            "[Training Epoch 60] Batch 2773, Loss 0.24082337319850922\n",
            "[Training Epoch 60] Batch 2774, Loss 0.26381734013557434\n",
            "[Training Epoch 60] Batch 2775, Loss 0.25484099984169006\n",
            "[Training Epoch 60] Batch 2776, Loss 0.2724638283252716\n",
            "[Training Epoch 60] Batch 2777, Loss 0.23544950783252716\n",
            "[Training Epoch 60] Batch 2778, Loss 0.2797994613647461\n",
            "[Training Epoch 60] Batch 2779, Loss 0.2657819390296936\n",
            "[Training Epoch 60] Batch 2780, Loss 0.24395856261253357\n",
            "[Training Epoch 60] Batch 2781, Loss 0.2689610421657562\n",
            "[Training Epoch 60] Batch 2782, Loss 0.2808588147163391\n",
            "[Training Epoch 60] Batch 2783, Loss 0.28593727946281433\n",
            "[Training Epoch 60] Batch 2784, Loss 0.26391294598579407\n",
            "[Training Epoch 60] Batch 2785, Loss 0.26284003257751465\n",
            "[Training Epoch 60] Batch 2786, Loss 0.2670917809009552\n",
            "[Training Epoch 60] Batch 2787, Loss 0.25365546345710754\n",
            "[Training Epoch 60] Batch 2788, Loss 0.26147568225860596\n",
            "[Training Epoch 60] Batch 2789, Loss 0.27008795738220215\n",
            "[Training Epoch 60] Batch 2790, Loss 0.22836866974830627\n",
            "[Training Epoch 60] Batch 2791, Loss 0.288583904504776\n",
            "[Training Epoch 60] Batch 2792, Loss 0.26110661029815674\n",
            "[Training Epoch 60] Batch 2793, Loss 0.26921525597572327\n",
            "[Training Epoch 60] Batch 2794, Loss 0.26272034645080566\n",
            "[Training Epoch 60] Batch 2795, Loss 0.27628690004348755\n",
            "[Training Epoch 60] Batch 2796, Loss 0.23121662437915802\n",
            "[Training Epoch 60] Batch 2797, Loss 0.22577263414859772\n",
            "[Training Epoch 60] Batch 2798, Loss 0.2840920090675354\n",
            "[Training Epoch 60] Batch 2799, Loss 0.2634690999984741\n",
            "[Training Epoch 60] Batch 2800, Loss 0.23731298744678497\n",
            "[Training Epoch 60] Batch 2801, Loss 0.3073210120201111\n",
            "[Training Epoch 60] Batch 2802, Loss 0.2653438448905945\n",
            "[Training Epoch 60] Batch 2803, Loss 0.23094594478607178\n",
            "[Training Epoch 60] Batch 2804, Loss 0.2636856436729431\n",
            "[Training Epoch 60] Batch 2805, Loss 0.2754559814929962\n",
            "[Training Epoch 60] Batch 2806, Loss 0.2743535339832306\n",
            "[Training Epoch 60] Batch 2807, Loss 0.23361369967460632\n",
            "[Training Epoch 60] Batch 2808, Loss 0.2473500818014145\n",
            "[Training Epoch 60] Batch 2809, Loss 0.24570389091968536\n",
            "[Training Epoch 60] Batch 2810, Loss 0.24715164303779602\n",
            "[Training Epoch 60] Batch 2811, Loss 0.2916361391544342\n",
            "[Training Epoch 60] Batch 2812, Loss 0.2755080461502075\n",
            "[Training Epoch 60] Batch 2813, Loss 0.2583974003791809\n",
            "[Training Epoch 60] Batch 2814, Loss 0.27137744426727295\n",
            "[Training Epoch 60] Batch 2815, Loss 0.2618107497692108\n",
            "[Training Epoch 60] Batch 2816, Loss 0.24134398996829987\n",
            "[Training Epoch 60] Batch 2817, Loss 0.24253776669502258\n",
            "[Training Epoch 60] Batch 2818, Loss 0.2860628366470337\n",
            "[Training Epoch 60] Batch 2819, Loss 0.27782413363456726\n",
            "[Training Epoch 60] Batch 2820, Loss 0.24223215878009796\n",
            "[Training Epoch 60] Batch 2821, Loss 0.2641969323158264\n",
            "[Training Epoch 60] Batch 2822, Loss 0.26451584696769714\n",
            "[Training Epoch 60] Batch 2823, Loss 0.24334284663200378\n",
            "[Training Epoch 60] Batch 2824, Loss 0.23388144373893738\n",
            "[Training Epoch 60] Batch 2825, Loss 0.22393402457237244\n",
            "[Training Epoch 60] Batch 2826, Loss 0.25133219361305237\n",
            "[Training Epoch 60] Batch 2827, Loss 0.25771236419677734\n",
            "[Training Epoch 60] Batch 2828, Loss 0.2806593179702759\n",
            "[Training Epoch 60] Batch 2829, Loss 0.26315590739250183\n",
            "[Training Epoch 60] Batch 2830, Loss 0.24431899189949036\n",
            "[Training Epoch 60] Batch 2831, Loss 0.284903883934021\n",
            "[Training Epoch 60] Batch 2832, Loss 0.32532110810279846\n",
            "[Training Epoch 60] Batch 2833, Loss 0.2564547061920166\n",
            "[Training Epoch 60] Batch 2834, Loss 0.25034099817276\n",
            "[Training Epoch 60] Batch 2835, Loss 0.2510088086128235\n",
            "[Training Epoch 60] Batch 2836, Loss 0.258272647857666\n",
            "[Training Epoch 60] Batch 2837, Loss 0.2742465138435364\n",
            "[Training Epoch 60] Batch 2838, Loss 0.25344380736351013\n",
            "[Training Epoch 60] Batch 2839, Loss 0.25365573167800903\n",
            "[Training Epoch 60] Batch 2840, Loss 0.27382075786590576\n",
            "[Training Epoch 60] Batch 2841, Loss 0.29115572571754456\n",
            "[Training Epoch 60] Batch 2842, Loss 0.28208088874816895\n",
            "[Training Epoch 60] Batch 2843, Loss 0.2536993622779846\n",
            "[Training Epoch 60] Batch 2844, Loss 0.24975624680519104\n",
            "[Training Epoch 60] Batch 2845, Loss 0.26893287897109985\n",
            "[Training Epoch 60] Batch 2846, Loss 0.26889142394065857\n",
            "[Training Epoch 60] Batch 2847, Loss 0.281515508890152\n",
            "[Training Epoch 60] Batch 2848, Loss 0.2685309946537018\n",
            "[Training Epoch 60] Batch 2849, Loss 0.2511257529258728\n",
            "[Training Epoch 60] Batch 2850, Loss 0.2854733467102051\n",
            "[Training Epoch 60] Batch 2851, Loss 0.2585573196411133\n",
            "[Training Epoch 60] Batch 2852, Loss 0.28133925795555115\n",
            "[Training Epoch 60] Batch 2853, Loss 0.26857244968414307\n",
            "[Training Epoch 60] Batch 2854, Loss 0.2591168284416199\n",
            "[Training Epoch 60] Batch 2855, Loss 0.26333048939704895\n",
            "[Training Epoch 60] Batch 2856, Loss 0.27590233087539673\n",
            "[Training Epoch 60] Batch 2857, Loss 0.2541666626930237\n",
            "[Training Epoch 60] Batch 2858, Loss 0.2665947675704956\n",
            "[Training Epoch 60] Batch 2859, Loss 0.2583952844142914\n",
            "[Training Epoch 60] Batch 2860, Loss 0.2575357258319855\n",
            "[Training Epoch 60] Batch 2861, Loss 0.2886764705181122\n",
            "[Training Epoch 60] Batch 2862, Loss 0.2599308490753174\n",
            "[Training Epoch 60] Batch 2863, Loss 0.2733854353427887\n",
            "[Training Epoch 60] Batch 2864, Loss 0.2546156644821167\n",
            "[Training Epoch 60] Batch 2865, Loss 0.2460786998271942\n",
            "[Training Epoch 60] Batch 2866, Loss 0.27390241622924805\n",
            "[Training Epoch 60] Batch 2867, Loss 0.25846704840660095\n",
            "[Training Epoch 60] Batch 2868, Loss 0.28431227803230286\n",
            "[Training Epoch 60] Batch 2869, Loss 0.2584528625011444\n",
            "[Training Epoch 60] Batch 2870, Loss 0.2535052001476288\n",
            "[Training Epoch 60] Batch 2871, Loss 0.27109473943710327\n",
            "[Training Epoch 60] Batch 2872, Loss 0.2511283755302429\n",
            "[Training Epoch 60] Batch 2873, Loss 0.2439407855272293\n",
            "[Training Epoch 60] Batch 2874, Loss 0.25971657037734985\n",
            "[Training Epoch 60] Batch 2875, Loss 0.2464119791984558\n",
            "[Training Epoch 60] Batch 2876, Loss 0.2689805030822754\n",
            "[Training Epoch 60] Batch 2877, Loss 0.27524423599243164\n",
            "[Training Epoch 60] Batch 2878, Loss 0.24937494099140167\n",
            "[Training Epoch 60] Batch 2879, Loss 0.25530824065208435\n",
            "[Training Epoch 60] Batch 2880, Loss 0.24307943880558014\n",
            "[Training Epoch 60] Batch 2881, Loss 0.25364503264427185\n",
            "[Training Epoch 60] Batch 2882, Loss 0.2662068009376526\n",
            "[Training Epoch 60] Batch 2883, Loss 0.26332905888557434\n",
            "[Training Epoch 60] Batch 2884, Loss 0.2494097799062729\n",
            "[Training Epoch 60] Batch 2885, Loss 0.2632453739643097\n",
            "[Training Epoch 60] Batch 2886, Loss 0.29027315974235535\n",
            "[Training Epoch 60] Batch 2887, Loss 0.25574222207069397\n",
            "[Training Epoch 60] Batch 2888, Loss 0.277732253074646\n",
            "[Training Epoch 60] Batch 2889, Loss 0.26375576853752136\n",
            "[Training Epoch 60] Batch 2890, Loss 0.26858043670654297\n",
            "[Training Epoch 60] Batch 2891, Loss 0.25196176767349243\n",
            "[Training Epoch 60] Batch 2892, Loss 0.23554107546806335\n",
            "[Training Epoch 60] Batch 2893, Loss 0.2900826036930084\n",
            "[Training Epoch 60] Batch 2894, Loss 0.2703247666358948\n",
            "[Training Epoch 60] Batch 2895, Loss 0.24532876908779144\n",
            "[Training Epoch 60] Batch 2896, Loss 0.2787990868091583\n",
            "[Training Epoch 60] Batch 2897, Loss 0.2514171302318573\n",
            "[Training Epoch 60] Batch 2898, Loss 0.25092509388923645\n",
            "[Training Epoch 60] Batch 2899, Loss 0.2459626942873001\n",
            "[Training Epoch 60] Batch 2900, Loss 0.2529965341091156\n",
            "[Training Epoch 60] Batch 2901, Loss 0.24066945910453796\n",
            "[Training Epoch 60] Batch 2902, Loss 0.3017845153808594\n",
            "[Training Epoch 60] Batch 2903, Loss 0.25640541315078735\n",
            "[Training Epoch 60] Batch 2904, Loss 0.2476050853729248\n",
            "[Training Epoch 60] Batch 2905, Loss 0.25055694580078125\n",
            "[Training Epoch 60] Batch 2906, Loss 0.24974514544010162\n",
            "[Training Epoch 60] Batch 2907, Loss 0.24107986688613892\n",
            "[Training Epoch 60] Batch 2908, Loss 0.27818572521209717\n",
            "[Training Epoch 60] Batch 2909, Loss 0.24122503399848938\n",
            "[Training Epoch 60] Batch 2910, Loss 0.26351770758628845\n",
            "[Training Epoch 60] Batch 2911, Loss 0.2720560133457184\n",
            "[Training Epoch 60] Batch 2912, Loss 0.2600102424621582\n",
            "[Training Epoch 60] Batch 2913, Loss 0.249518021941185\n",
            "[Training Epoch 60] Batch 2914, Loss 0.27645909786224365\n",
            "[Training Epoch 60] Batch 2915, Loss 0.2839546799659729\n",
            "[Training Epoch 60] Batch 2916, Loss 0.24474041163921356\n",
            "[Training Epoch 60] Batch 2917, Loss 0.2569080591201782\n",
            "[Training Epoch 60] Batch 2918, Loss 0.266384482383728\n",
            "[Training Epoch 60] Batch 2919, Loss 0.28408923745155334\n",
            "[Training Epoch 60] Batch 2920, Loss 0.2682344615459442\n",
            "[Training Epoch 60] Batch 2921, Loss 0.2564125657081604\n",
            "[Training Epoch 60] Batch 2922, Loss 0.2559942901134491\n",
            "[Training Epoch 60] Batch 2923, Loss 0.27075058221817017\n",
            "[Training Epoch 60] Batch 2924, Loss 0.2453000545501709\n",
            "[Training Epoch 60] Batch 2925, Loss 0.2532774806022644\n",
            "[Training Epoch 60] Batch 2926, Loss 0.26784563064575195\n",
            "[Training Epoch 60] Batch 2927, Loss 0.27617335319519043\n",
            "[Training Epoch 60] Batch 2928, Loss 0.27217549085617065\n",
            "[Training Epoch 60] Batch 2929, Loss 0.24976789951324463\n",
            "[Training Epoch 60] Batch 2930, Loss 0.2523593008518219\n",
            "[Training Epoch 60] Batch 2931, Loss 0.2685290277004242\n",
            "[Training Epoch 60] Batch 2932, Loss 0.2783420979976654\n",
            "[Training Epoch 60] Batch 2933, Loss 0.2473621666431427\n",
            "[Training Epoch 60] Batch 2934, Loss 0.26208874583244324\n",
            "[Training Epoch 60] Batch 2935, Loss 0.24401625990867615\n",
            "[Training Epoch 60] Batch 2936, Loss 0.25015702843666077\n",
            "[Training Epoch 60] Batch 2937, Loss 0.269483745098114\n",
            "[Training Epoch 60] Batch 2938, Loss 0.24851995706558228\n",
            "[Training Epoch 60] Batch 2939, Loss 0.27924299240112305\n",
            "[Training Epoch 60] Batch 2940, Loss 0.27404671907424927\n",
            "[Training Epoch 60] Batch 2941, Loss 0.2409769594669342\n",
            "[Training Epoch 60] Batch 2942, Loss 0.24255961179733276\n",
            "[Training Epoch 60] Batch 2943, Loss 0.2702018916606903\n",
            "[Training Epoch 60] Batch 2944, Loss 0.2328256070613861\n",
            "[Training Epoch 60] Batch 2945, Loss 0.2642718255519867\n",
            "[Training Epoch 60] Batch 2946, Loss 0.25076431035995483\n",
            "[Training Epoch 60] Batch 2947, Loss 0.2538691461086273\n",
            "[Training Epoch 60] Batch 2948, Loss 0.25018253922462463\n",
            "[Training Epoch 60] Batch 2949, Loss 0.2753089964389801\n",
            "[Training Epoch 60] Batch 2950, Loss 0.25728386640548706\n",
            "[Training Epoch 60] Batch 2951, Loss 0.297878235578537\n",
            "[Training Epoch 60] Batch 2952, Loss 0.24564525485038757\n",
            "[Training Epoch 60] Batch 2953, Loss 0.23459039628505707\n",
            "[Training Epoch 60] Batch 2954, Loss 0.26450857520103455\n",
            "[Training Epoch 60] Batch 2955, Loss 0.29616162180900574\n",
            "[Training Epoch 60] Batch 2956, Loss 0.2720174491405487\n",
            "[Training Epoch 60] Batch 2957, Loss 0.24011565744876862\n",
            "[Training Epoch 60] Batch 2958, Loss 0.27911660075187683\n",
            "[Training Epoch 60] Batch 2959, Loss 0.2488079071044922\n",
            "[Training Epoch 60] Batch 2960, Loss 0.2511708438396454\n",
            "[Training Epoch 60] Batch 2961, Loss 0.27179235219955444\n",
            "[Training Epoch 60] Batch 2962, Loss 0.26002147793769836\n",
            "[Training Epoch 60] Batch 2963, Loss 0.26084935665130615\n",
            "[Training Epoch 60] Batch 2964, Loss 0.2736048996448517\n",
            "[Training Epoch 60] Batch 2965, Loss 0.27885130047798157\n",
            "[Training Epoch 60] Batch 2966, Loss 0.2929091155529022\n",
            "[Training Epoch 60] Batch 2967, Loss 0.25794410705566406\n",
            "[Training Epoch 60] Batch 2968, Loss 0.24593672156333923\n",
            "[Training Epoch 60] Batch 2969, Loss 0.23943406343460083\n",
            "[Training Epoch 60] Batch 2970, Loss 0.2685578465461731\n",
            "[Training Epoch 60] Batch 2971, Loss 0.27179938554763794\n",
            "[Training Epoch 60] Batch 2972, Loss 0.28893959522247314\n",
            "[Training Epoch 60] Batch 2973, Loss 0.27924713492393494\n",
            "[Training Epoch 60] Batch 2974, Loss 0.26746463775634766\n",
            "[Training Epoch 60] Batch 2975, Loss 0.2756686806678772\n",
            "[Training Epoch 60] Batch 2976, Loss 0.24069228768348694\n",
            "[Training Epoch 60] Batch 2977, Loss 0.24810871481895447\n",
            "[Training Epoch 60] Batch 2978, Loss 0.24346576631069183\n",
            "[Training Epoch 60] Batch 2979, Loss 0.2844088673591614\n",
            "[Training Epoch 60] Batch 2980, Loss 0.25052595138549805\n",
            "[Training Epoch 60] Batch 2981, Loss 0.2538917660713196\n",
            "[Training Epoch 60] Batch 2982, Loss 0.2917575538158417\n",
            "[Training Epoch 60] Batch 2983, Loss 0.2582380473613739\n",
            "[Training Epoch 60] Batch 2984, Loss 0.28747183084487915\n",
            "[Training Epoch 60] Batch 2985, Loss 0.23846472799777985\n",
            "[Training Epoch 60] Batch 2986, Loss 0.27403920888900757\n",
            "[Training Epoch 60] Batch 2987, Loss 0.25086602568626404\n",
            "[Training Epoch 60] Batch 2988, Loss 0.28572094440460205\n",
            "[Training Epoch 60] Batch 2989, Loss 0.2607367932796478\n",
            "[Training Epoch 60] Batch 2990, Loss 0.2684831917285919\n",
            "[Training Epoch 60] Batch 2991, Loss 0.23472760617733002\n",
            "[Training Epoch 60] Batch 2992, Loss 0.2643958628177643\n",
            "[Training Epoch 60] Batch 2993, Loss 0.23148947954177856\n",
            "[Training Epoch 60] Batch 2994, Loss 0.245639368891716\n",
            "[Training Epoch 60] Batch 2995, Loss 0.23796215653419495\n",
            "[Training Epoch 60] Batch 2996, Loss 0.24129173159599304\n",
            "[Training Epoch 60] Batch 2997, Loss 0.29209771752357483\n",
            "[Training Epoch 60] Batch 2998, Loss 0.26494187116622925\n",
            "[Training Epoch 60] Batch 2999, Loss 0.2642744183540344\n",
            "[Training Epoch 60] Batch 3000, Loss 0.25496697425842285\n",
            "[Training Epoch 60] Batch 3001, Loss 0.25009259581565857\n",
            "[Training Epoch 60] Batch 3002, Loss 0.26832371950149536\n",
            "[Training Epoch 60] Batch 3003, Loss 0.25689175724983215\n",
            "[Training Epoch 60] Batch 3004, Loss 0.25864702463150024\n",
            "[Training Epoch 60] Batch 3005, Loss 0.2604069411754608\n",
            "[Training Epoch 60] Batch 3006, Loss 0.2635900676250458\n",
            "[Training Epoch 60] Batch 3007, Loss 0.24898876249790192\n",
            "[Training Epoch 60] Batch 3008, Loss 0.266256183385849\n",
            "[Training Epoch 60] Batch 3009, Loss 0.26873257756233215\n",
            "[Training Epoch 60] Batch 3010, Loss 0.273266464471817\n",
            "[Training Epoch 60] Batch 3011, Loss 0.22513344883918762\n",
            "[Training Epoch 60] Batch 3012, Loss 0.28589290380477905\n",
            "[Training Epoch 60] Batch 3013, Loss 0.25419777631759644\n",
            "[Training Epoch 60] Batch 3014, Loss 0.2521464228630066\n",
            "[Training Epoch 60] Batch 3015, Loss 0.24990130960941315\n",
            "[Training Epoch 60] Batch 3016, Loss 0.27624523639678955\n",
            "[Training Epoch 60] Batch 3017, Loss 0.2687467932701111\n",
            "[Training Epoch 60] Batch 3018, Loss 0.2582259476184845\n",
            "[Training Epoch 60] Batch 3019, Loss 0.2589365541934967\n",
            "[Training Epoch 60] Batch 3020, Loss 0.25657519698143005\n",
            "[Training Epoch 60] Batch 3021, Loss 0.2508428394794464\n",
            "[Training Epoch 60] Batch 3022, Loss 0.25218844413757324\n",
            "[Training Epoch 60] Batch 3023, Loss 0.25845593214035034\n",
            "[Training Epoch 60] Batch 3024, Loss 0.26151081919670105\n",
            "[Training Epoch 60] Batch 3025, Loss 0.26133599877357483\n",
            "[Training Epoch 60] Batch 3026, Loss 0.22774548828601837\n",
            "[Training Epoch 60] Batch 3027, Loss 0.24866320192813873\n",
            "[Training Epoch 60] Batch 3028, Loss 0.2563960552215576\n",
            "[Training Epoch 60] Batch 3029, Loss 0.27071765065193176\n",
            "[Training Epoch 60] Batch 3030, Loss 0.24994753301143646\n",
            "[Training Epoch 60] Batch 3031, Loss 0.27315279841423035\n",
            "[Training Epoch 60] Batch 3032, Loss 0.24170821905136108\n",
            "[Training Epoch 60] Batch 3033, Loss 0.22850990295410156\n",
            "[Training Epoch 60] Batch 3034, Loss 0.26136770844459534\n",
            "[Training Epoch 60] Batch 3035, Loss 0.2816084325313568\n",
            "[Training Epoch 60] Batch 3036, Loss 0.2613392472267151\n",
            "[Training Epoch 60] Batch 3037, Loss 0.26120394468307495\n",
            "[Training Epoch 60] Batch 3038, Loss 0.2483467161655426\n",
            "[Training Epoch 60] Batch 3039, Loss 0.273174911737442\n",
            "[Training Epoch 60] Batch 3040, Loss 0.27951401472091675\n",
            "[Training Epoch 60] Batch 3041, Loss 0.24304500222206116\n",
            "[Training Epoch 60] Batch 3042, Loss 0.2912975549697876\n",
            "[Training Epoch 60] Batch 3043, Loss 0.2388489693403244\n",
            "[Training Epoch 60] Batch 3044, Loss 0.26383358240127563\n",
            "[Training Epoch 60] Batch 3045, Loss 0.2452782839536667\n",
            "[Training Epoch 60] Batch 3046, Loss 0.24879130721092224\n",
            "[Training Epoch 60] Batch 3047, Loss 0.2998543083667755\n",
            "[Training Epoch 60] Batch 3048, Loss 0.26871126890182495\n",
            "[Training Epoch 60] Batch 3049, Loss 0.29301536083221436\n",
            "[Training Epoch 60] Batch 3050, Loss 0.28464260697364807\n",
            "[Training Epoch 60] Batch 3051, Loss 0.22160105407238007\n",
            "[Training Epoch 60] Batch 3052, Loss 0.29814308881759644\n",
            "[Training Epoch 60] Batch 3053, Loss 0.26136988401412964\n",
            "[Training Epoch 60] Batch 3054, Loss 0.2694544196128845\n",
            "[Training Epoch 60] Batch 3055, Loss 0.2537133991718292\n",
            "[Training Epoch 60] Batch 3056, Loss 0.2561003863811493\n",
            "[Training Epoch 60] Batch 3057, Loss 0.27103695273399353\n",
            "[Training Epoch 60] Batch 3058, Loss 0.25321078300476074\n",
            "[Training Epoch 60] Batch 3059, Loss 0.23985713720321655\n",
            "[Training Epoch 60] Batch 3060, Loss 0.2401617169380188\n",
            "[Training Epoch 60] Batch 3061, Loss 0.2650132179260254\n",
            "[Training Epoch 60] Batch 3062, Loss 0.2569793164730072\n",
            "[Training Epoch 60] Batch 3063, Loss 0.23703786730766296\n",
            "[Training Epoch 60] Batch 3064, Loss 0.2733822464942932\n",
            "[Training Epoch 60] Batch 3065, Loss 0.30271437764167786\n",
            "[Training Epoch 60] Batch 3066, Loss 0.2642034888267517\n",
            "[Training Epoch 60] Batch 3067, Loss 0.28463754057884216\n",
            "[Training Epoch 60] Batch 3068, Loss 0.2514926493167877\n",
            "[Training Epoch 60] Batch 3069, Loss 0.2676680088043213\n",
            "[Training Epoch 60] Batch 3070, Loss 0.2671821117401123\n",
            "[Training Epoch 60] Batch 3071, Loss 0.24049997329711914\n",
            "[Training Epoch 60] Batch 3072, Loss 0.2907264530658722\n",
            "[Training Epoch 60] Batch 3073, Loss 0.271274209022522\n",
            "[Training Epoch 60] Batch 3074, Loss 0.22964900732040405\n",
            "[Training Epoch 60] Batch 3075, Loss 0.30964696407318115\n",
            "[Training Epoch 60] Batch 3076, Loss 0.23086777329444885\n",
            "[Training Epoch 60] Batch 3077, Loss 0.27799785137176514\n",
            "[Training Epoch 60] Batch 3078, Loss 0.2643384337425232\n",
            "[Training Epoch 60] Batch 3079, Loss 0.2553134858608246\n",
            "[Training Epoch 60] Batch 3080, Loss 0.2688460946083069\n",
            "[Training Epoch 60] Batch 3081, Loss 0.2607022225856781\n",
            "[Training Epoch 60] Batch 3082, Loss 0.22247810661792755\n",
            "[Training Epoch 60] Batch 3083, Loss 0.26068243384361267\n",
            "[Training Epoch 60] Batch 3084, Loss 0.27622973918914795\n",
            "[Training Epoch 60] Batch 3085, Loss 0.28831565380096436\n",
            "[Training Epoch 60] Batch 3086, Loss 0.2677153944969177\n",
            "[Training Epoch 60] Batch 3087, Loss 0.25905588269233704\n",
            "[Training Epoch 60] Batch 3088, Loss 0.2801207900047302\n",
            "[Training Epoch 60] Batch 3089, Loss 0.2810611128807068\n",
            "[Training Epoch 60] Batch 3090, Loss 0.2736833691596985\n",
            "[Training Epoch 60] Batch 3091, Loss 0.29576635360717773\n",
            "[Training Epoch 60] Batch 3092, Loss 0.25959646701812744\n",
            "[Training Epoch 60] Batch 3093, Loss 0.28434816002845764\n",
            "[Training Epoch 60] Batch 3094, Loss 0.2315259873867035\n",
            "[Training Epoch 60] Batch 3095, Loss 0.2944876253604889\n",
            "[Training Epoch 60] Batch 3096, Loss 0.25868386030197144\n",
            "[Training Epoch 60] Batch 3097, Loss 0.28146180510520935\n",
            "[Training Epoch 60] Batch 3098, Loss 0.24517735838890076\n",
            "[Training Epoch 60] Batch 3099, Loss 0.277087539434433\n",
            "[Training Epoch 60] Batch 3100, Loss 0.23838874697685242\n",
            "[Training Epoch 60] Batch 3101, Loss 0.2674543261528015\n",
            "[Training Epoch 60] Batch 3102, Loss 0.259105384349823\n",
            "[Training Epoch 60] Batch 3103, Loss 0.2677364945411682\n",
            "[Training Epoch 60] Batch 3104, Loss 0.2862960696220398\n",
            "[Training Epoch 60] Batch 3105, Loss 0.24998828768730164\n",
            "[Training Epoch 60] Batch 3106, Loss 0.2833346128463745\n",
            "[Training Epoch 60] Batch 3107, Loss 0.2706689238548279\n",
            "[Training Epoch 60] Batch 3108, Loss 0.24480345845222473\n",
            "[Training Epoch 60] Batch 3109, Loss 0.2604146897792816\n",
            "[Training Epoch 60] Batch 3110, Loss 0.2776094377040863\n",
            "[Training Epoch 60] Batch 3111, Loss 0.2740101218223572\n",
            "[Training Epoch 60] Batch 3112, Loss 0.26902055740356445\n",
            "[Training Epoch 60] Batch 3113, Loss 0.244292750954628\n",
            "[Training Epoch 60] Batch 3114, Loss 0.2518995404243469\n",
            "[Training Epoch 60] Batch 3115, Loss 0.23128663003444672\n",
            "[Training Epoch 60] Batch 3116, Loss 0.2502177357673645\n",
            "[Training Epoch 60] Batch 3117, Loss 0.2415040284395218\n",
            "[Training Epoch 60] Batch 3118, Loss 0.257612407207489\n",
            "[Training Epoch 60] Batch 3119, Loss 0.27160927653312683\n",
            "[Training Epoch 60] Batch 3120, Loss 0.27600738406181335\n",
            "[Training Epoch 60] Batch 3121, Loss 0.25007396936416626\n",
            "[Training Epoch 60] Batch 3122, Loss 0.2821235656738281\n",
            "[Training Epoch 60] Batch 3123, Loss 0.2665204405784607\n",
            "[Training Epoch 60] Batch 3124, Loss 0.26117926836013794\n",
            "[Training Epoch 60] Batch 3125, Loss 0.27680322527885437\n",
            "[Training Epoch 60] Batch 3126, Loss 0.26155778765678406\n",
            "[Training Epoch 60] Batch 3127, Loss 0.2858474850654602\n",
            "[Training Epoch 60] Batch 3128, Loss 0.2444799542427063\n",
            "[Training Epoch 60] Batch 3129, Loss 0.2239072024822235\n",
            "[Training Epoch 60] Batch 3130, Loss 0.26198822259902954\n",
            "[Training Epoch 60] Batch 3131, Loss 0.2633463144302368\n",
            "[Training Epoch 60] Batch 3132, Loss 0.23213225603103638\n",
            "[Training Epoch 60] Batch 3133, Loss 0.28719648718833923\n",
            "[Training Epoch 60] Batch 3134, Loss 0.282730370759964\n",
            "[Training Epoch 60] Batch 3135, Loss 0.23988351225852966\n",
            "[Training Epoch 60] Batch 3136, Loss 0.25318312644958496\n",
            "[Training Epoch 60] Batch 3137, Loss 0.23862065374851227\n",
            "[Training Epoch 60] Batch 3138, Loss 0.271861732006073\n",
            "[Training Epoch 60] Batch 3139, Loss 0.2799794673919678\n",
            "[Training Epoch 60] Batch 3140, Loss 0.24335867166519165\n",
            "[Training Epoch 60] Batch 3141, Loss 0.2516769766807556\n",
            "[Training Epoch 60] Batch 3142, Loss 0.25990572571754456\n",
            "[Training Epoch 60] Batch 3143, Loss 0.2550380825996399\n",
            "[Training Epoch 60] Batch 3144, Loss 0.2637539505958557\n",
            "[Training Epoch 60] Batch 3145, Loss 0.2814091145992279\n",
            "[Training Epoch 60] Batch 3146, Loss 0.2620519995689392\n",
            "[Training Epoch 60] Batch 3147, Loss 0.253302663564682\n",
            "[Training Epoch 60] Batch 3148, Loss 0.2572530210018158\n",
            "[Training Epoch 60] Batch 3149, Loss 0.2608011066913605\n",
            "[Training Epoch 60] Batch 3150, Loss 0.2587560713291168\n",
            "[Training Epoch 60] Batch 3151, Loss 0.26827794313430786\n",
            "[Training Epoch 60] Batch 3152, Loss 0.2496262490749359\n",
            "[Training Epoch 60] Batch 3153, Loss 0.26124459505081177\n",
            "[Training Epoch 60] Batch 3154, Loss 0.28880566358566284\n",
            "[Training Epoch 60] Batch 3155, Loss 0.2658735513687134\n",
            "[Training Epoch 60] Batch 3156, Loss 0.275870144367218\n",
            "[Training Epoch 60] Batch 3157, Loss 0.2720639109611511\n",
            "[Training Epoch 60] Batch 3158, Loss 0.2736252546310425\n",
            "[Training Epoch 60] Batch 3159, Loss 0.23525948822498322\n",
            "[Training Epoch 60] Batch 3160, Loss 0.2405816614627838\n",
            "[Training Epoch 60] Batch 3161, Loss 0.2822301685810089\n",
            "[Training Epoch 60] Batch 3162, Loss 0.2708702087402344\n",
            "[Training Epoch 60] Batch 3163, Loss 0.257312536239624\n",
            "[Training Epoch 60] Batch 3164, Loss 0.2963315546512604\n",
            "[Training Epoch 60] Batch 3165, Loss 0.25138401985168457\n",
            "[Training Epoch 60] Batch 3166, Loss 0.2590780556201935\n",
            "[Training Epoch 60] Batch 3167, Loss 0.2753353714942932\n",
            "[Training Epoch 60] Batch 3168, Loss 0.2751139998435974\n",
            "[Training Epoch 60] Batch 3169, Loss 0.273637592792511\n",
            "[Training Epoch 60] Batch 3170, Loss 0.26482734084129333\n",
            "[Training Epoch 60] Batch 3171, Loss 0.2955157458782196\n",
            "[Training Epoch 60] Batch 3172, Loss 0.2686324417591095\n",
            "[Training Epoch 60] Batch 3173, Loss 0.27117103338241577\n",
            "[Training Epoch 60] Batch 3174, Loss 0.26801398396492004\n",
            "[Training Epoch 60] Batch 3175, Loss 0.2525751292705536\n",
            "[Training Epoch 60] Batch 3176, Loss 0.26059865951538086\n",
            "[Training Epoch 60] Batch 3177, Loss 0.2761962413787842\n",
            "[Training Epoch 60] Batch 3178, Loss 0.2890159487724304\n",
            "[Training Epoch 60] Batch 3179, Loss 0.27582815289497375\n",
            "[Training Epoch 60] Batch 3180, Loss 0.28997939825057983\n",
            "[Training Epoch 60] Batch 3181, Loss 0.2896679937839508\n",
            "[Training Epoch 60] Batch 3182, Loss 0.2307925522327423\n",
            "[Training Epoch 60] Batch 3183, Loss 0.2901846766471863\n",
            "[Training Epoch 60] Batch 3184, Loss 0.2653181552886963\n",
            "[Training Epoch 60] Batch 3185, Loss 0.272505521774292\n",
            "[Training Epoch 60] Batch 3186, Loss 0.2722717225551605\n",
            "[Training Epoch 60] Batch 3187, Loss 0.2274109423160553\n",
            "[Training Epoch 60] Batch 3188, Loss 0.2628631889820099\n",
            "[Training Epoch 60] Batch 3189, Loss 0.26407599449157715\n",
            "[Training Epoch 60] Batch 3190, Loss 0.2615334689617157\n",
            "[Training Epoch 60] Batch 3191, Loss 0.24803291261196136\n",
            "[Training Epoch 60] Batch 3192, Loss 0.2803727984428406\n",
            "[Training Epoch 60] Batch 3193, Loss 0.25199663639068604\n",
            "[Training Epoch 60] Batch 3194, Loss 0.28148767352104187\n",
            "[Training Epoch 60] Batch 3195, Loss 0.2700088620185852\n",
            "[Training Epoch 60] Batch 3196, Loss 0.24984347820281982\n",
            "[Training Epoch 60] Batch 3197, Loss 0.25522181391716003\n",
            "[Training Epoch 60] Batch 3198, Loss 0.24740567803382874\n",
            "[Training Epoch 60] Batch 3199, Loss 0.24561752378940582\n",
            "[Training Epoch 60] Batch 3200, Loss 0.2829930782318115\n",
            "[Training Epoch 60] Batch 3201, Loss 0.2464594542980194\n",
            "[Training Epoch 60] Batch 3202, Loss 0.26362791657447815\n",
            "[Training Epoch 60] Batch 3203, Loss 0.255126416683197\n",
            "[Training Epoch 60] Batch 3204, Loss 0.27627354860305786\n",
            "[Training Epoch 60] Batch 3205, Loss 0.26476508378982544\n",
            "[Training Epoch 60] Batch 3206, Loss 0.26880326867103577\n",
            "[Training Epoch 60] Batch 3207, Loss 0.24689960479736328\n",
            "[Training Epoch 60] Batch 3208, Loss 0.28370097279548645\n",
            "[Training Epoch 60] Batch 3209, Loss 0.2640424966812134\n",
            "[Training Epoch 60] Batch 3210, Loss 0.2382567822933197\n",
            "[Training Epoch 60] Batch 3211, Loss 0.2735278904438019\n",
            "[Training Epoch 60] Batch 3212, Loss 0.30127134919166565\n",
            "[Training Epoch 60] Batch 3213, Loss 0.25534313917160034\n",
            "[Training Epoch 60] Batch 3214, Loss 0.23898352682590485\n",
            "[Training Epoch 60] Batch 3215, Loss 0.27636587619781494\n",
            "[Training Epoch 60] Batch 3216, Loss 0.26233452558517456\n",
            "[Training Epoch 60] Batch 3217, Loss 0.2520052194595337\n",
            "[Training Epoch 60] Batch 3218, Loss 0.27092301845550537\n",
            "[Training Epoch 60] Batch 3219, Loss 0.25865742564201355\n",
            "[Training Epoch 60] Batch 3220, Loss 0.23964965343475342\n",
            "[Training Epoch 60] Batch 3221, Loss 0.2836887538433075\n",
            "[Training Epoch 60] Batch 3222, Loss 0.2628335952758789\n",
            "[Training Epoch 60] Batch 3223, Loss 0.27745771408081055\n",
            "[Training Epoch 60] Batch 3224, Loss 0.2562379240989685\n",
            "[Training Epoch 60] Batch 3225, Loss 0.2684670686721802\n",
            "[Training Epoch 60] Batch 3226, Loss 0.2594430148601532\n",
            "[Training Epoch 60] Batch 3227, Loss 0.2404315173625946\n",
            "[Training Epoch 60] Batch 3228, Loss 0.25852954387664795\n",
            "[Training Epoch 60] Batch 3229, Loss 0.24777330458164215\n",
            "[Training Epoch 60] Batch 3230, Loss 0.2620375454425812\n",
            "[Training Epoch 60] Batch 3231, Loss 0.26724475622177124\n",
            "[Training Epoch 60] Batch 3232, Loss 0.2664148509502411\n",
            "[Training Epoch 60] Batch 3233, Loss 0.2596507966518402\n",
            "[Training Epoch 60] Batch 3234, Loss 0.2595539093017578\n",
            "[Training Epoch 60] Batch 3235, Loss 0.27979931235313416\n",
            "[Training Epoch 60] Batch 3236, Loss 0.2952248454093933\n",
            "[Training Epoch 60] Batch 3237, Loss 0.2676236927509308\n",
            "[Training Epoch 60] Batch 3238, Loss 0.2387191206216812\n",
            "[Training Epoch 60] Batch 3239, Loss 0.28699901700019836\n",
            "[Training Epoch 60] Batch 3240, Loss 0.2764566242694855\n",
            "[Training Epoch 60] Batch 3241, Loss 0.2557160556316376\n",
            "[Training Epoch 60] Batch 3242, Loss 0.2805108428001404\n",
            "[Training Epoch 60] Batch 3243, Loss 0.2662220001220703\n",
            "[Training Epoch 60] Batch 3244, Loss 0.2972639799118042\n",
            "[Training Epoch 60] Batch 3245, Loss 0.24231910705566406\n",
            "[Training Epoch 60] Batch 3246, Loss 0.2442205846309662\n",
            "[Training Epoch 60] Batch 3247, Loss 0.256475567817688\n",
            "[Training Epoch 60] Batch 3248, Loss 0.30035772919654846\n",
            "[Training Epoch 60] Batch 3249, Loss 0.2742970287799835\n",
            "[Training Epoch 60] Batch 3250, Loss 0.2770324647426605\n",
            "[Training Epoch 60] Batch 3251, Loss 0.2543308138847351\n",
            "[Training Epoch 60] Batch 3252, Loss 0.2630985677242279\n",
            "[Training Epoch 60] Batch 3253, Loss 0.2924695611000061\n",
            "[Training Epoch 60] Batch 3254, Loss 0.2714631259441376\n",
            "[Training Epoch 60] Batch 3255, Loss 0.25062596797943115\n",
            "[Training Epoch 60] Batch 3256, Loss 0.29045772552490234\n",
            "[Training Epoch 60] Batch 3257, Loss 0.26071372628211975\n",
            "[Training Epoch 60] Batch 3258, Loss 0.2487328052520752\n",
            "[Training Epoch 60] Batch 3259, Loss 0.27085578441619873\n",
            "[Training Epoch 60] Batch 3260, Loss 0.27585849165916443\n",
            "[Training Epoch 60] Batch 3261, Loss 0.24204355478286743\n",
            "[Training Epoch 60] Batch 3262, Loss 0.2559477388858795\n",
            "[Training Epoch 60] Batch 3263, Loss 0.2664293944835663\n",
            "[Training Epoch 60] Batch 3264, Loss 0.2558210790157318\n",
            "[Training Epoch 60] Batch 3265, Loss 0.26188915967941284\n",
            "[Training Epoch 60] Batch 3266, Loss 0.29110100865364075\n",
            "[Training Epoch 60] Batch 3267, Loss 0.26557251811027527\n",
            "[Training Epoch 60] Batch 3268, Loss 0.2720367908477783\n",
            "[Training Epoch 60] Batch 3269, Loss 0.27173399925231934\n",
            "[Training Epoch 60] Batch 3270, Loss 0.2572624087333679\n",
            "[Training Epoch 60] Batch 3271, Loss 0.254302978515625\n",
            "[Training Epoch 60] Batch 3272, Loss 0.2964092791080475\n",
            "[Training Epoch 60] Batch 3273, Loss 0.268266499042511\n",
            "[Training Epoch 60] Batch 3274, Loss 0.2744942903518677\n",
            "[Training Epoch 60] Batch 3275, Loss 0.23986172676086426\n",
            "[Training Epoch 60] Batch 3276, Loss 0.2837772071361542\n",
            "[Training Epoch 60] Batch 3277, Loss 0.3028441369533539\n",
            "[Training Epoch 60] Batch 3278, Loss 0.2565798759460449\n",
            "[Training Epoch 60] Batch 3279, Loss 0.24423514306545258\n",
            "[Training Epoch 60] Batch 3280, Loss 0.2735593318939209\n",
            "[Training Epoch 60] Batch 3281, Loss 0.2603059411048889\n",
            "[Training Epoch 60] Batch 3282, Loss 0.2670174241065979\n",
            "[Training Epoch 60] Batch 3283, Loss 0.2865273654460907\n",
            "[Training Epoch 60] Batch 3284, Loss 0.29017388820648193\n",
            "[Training Epoch 60] Batch 3285, Loss 0.2714409828186035\n",
            "[Training Epoch 60] Batch 3286, Loss 0.2596112787723541\n",
            "[Training Epoch 60] Batch 3287, Loss 0.24818864464759827\n",
            "[Training Epoch 60] Batch 3288, Loss 0.28738394379615784\n",
            "[Training Epoch 60] Batch 3289, Loss 0.28773364424705505\n",
            "[Training Epoch 60] Batch 3290, Loss 0.24104943871498108\n",
            "[Training Epoch 60] Batch 3291, Loss 0.27026981115341187\n",
            "[Training Epoch 60] Batch 3292, Loss 0.24920573830604553\n",
            "[Training Epoch 60] Batch 3293, Loss 0.2757805585861206\n",
            "[Training Epoch 60] Batch 3294, Loss 0.2653922438621521\n",
            "[Training Epoch 60] Batch 3295, Loss 0.27907702326774597\n",
            "[Training Epoch 60] Batch 3296, Loss 0.26782190799713135\n",
            "[Training Epoch 60] Batch 3297, Loss 0.2685622572898865\n",
            "[Training Epoch 60] Batch 3298, Loss 0.2829066812992096\n",
            "[Training Epoch 60] Batch 3299, Loss 0.2768106460571289\n",
            "[Training Epoch 60] Batch 3300, Loss 0.25128796696662903\n",
            "[Training Epoch 60] Batch 3301, Loss 0.2499539852142334\n",
            "[Training Epoch 60] Batch 3302, Loss 0.26908186078071594\n",
            "[Training Epoch 60] Batch 3303, Loss 0.27661946415901184\n",
            "[Training Epoch 60] Batch 3304, Loss 0.26353079080581665\n",
            "[Training Epoch 60] Batch 3305, Loss 0.2924977242946625\n",
            "[Training Epoch 60] Batch 3306, Loss 0.2700147032737732\n",
            "[Training Epoch 60] Batch 3307, Loss 0.2553328275680542\n",
            "[Training Epoch 60] Batch 3308, Loss 0.2624097466468811\n",
            "[Training Epoch 60] Batch 3309, Loss 0.2738323509693146\n",
            "[Training Epoch 60] Batch 3310, Loss 0.2378312051296234\n",
            "[Training Epoch 60] Batch 3311, Loss 0.2550697922706604\n",
            "[Training Epoch 60] Batch 3312, Loss 0.25503435730934143\n",
            "[Training Epoch 60] Batch 3313, Loss 0.27130794525146484\n",
            "[Training Epoch 60] Batch 3314, Loss 0.2746635377407074\n",
            "[Training Epoch 60] Batch 3315, Loss 0.2519879639148712\n",
            "[Training Epoch 60] Batch 3316, Loss 0.26557135581970215\n",
            "[Training Epoch 60] Batch 3317, Loss 0.28675469756126404\n",
            "[Training Epoch 60] Batch 3318, Loss 0.2729355990886688\n",
            "[Training Epoch 60] Batch 3319, Loss 0.27055537700653076\n",
            "[Training Epoch 60] Batch 3320, Loss 0.26730942726135254\n",
            "[Training Epoch 60] Batch 3321, Loss 0.27661699056625366\n",
            "[Training Epoch 60] Batch 3322, Loss 0.2680969536304474\n",
            "[Training Epoch 60] Batch 3323, Loss 0.2507920265197754\n",
            "[Training Epoch 60] Batch 3324, Loss 0.2569912075996399\n",
            "[Training Epoch 60] Batch 3325, Loss 0.2698585093021393\n",
            "[Training Epoch 60] Batch 3326, Loss 0.2773599922657013\n",
            "[Training Epoch 60] Batch 3327, Loss 0.25587448477745056\n",
            "[Training Epoch 60] Batch 3328, Loss 0.26339882612228394\n",
            "[Training Epoch 60] Batch 3329, Loss 0.27087244391441345\n",
            "[Training Epoch 60] Batch 3330, Loss 0.2721520662307739\n",
            "[Training Epoch 60] Batch 3331, Loss 0.2402525395154953\n",
            "[Training Epoch 60] Batch 3332, Loss 0.255515992641449\n",
            "[Training Epoch 60] Batch 3333, Loss 0.26686087250709534\n",
            "[Training Epoch 60] Batch 3334, Loss 0.24224853515625\n",
            "[Training Epoch 60] Batch 3335, Loss 0.27218520641326904\n",
            "[Training Epoch 60] Batch 3336, Loss 0.2802900969982147\n",
            "[Training Epoch 60] Batch 3337, Loss 0.23669566214084625\n",
            "[Training Epoch 60] Batch 3338, Loss 0.25796666741371155\n",
            "[Training Epoch 60] Batch 3339, Loss 0.2724454402923584\n",
            "[Training Epoch 60] Batch 3340, Loss 0.26277944445610046\n",
            "[Training Epoch 60] Batch 3341, Loss 0.318604052066803\n",
            "[Training Epoch 60] Batch 3342, Loss 0.25022783875465393\n",
            "[Training Epoch 60] Batch 3343, Loss 0.28791749477386475\n",
            "[Training Epoch 60] Batch 3344, Loss 0.2638925015926361\n",
            "[Training Epoch 60] Batch 3345, Loss 0.24255339801311493\n",
            "[Training Epoch 60] Batch 3346, Loss 0.286336749792099\n",
            "[Training Epoch 60] Batch 3347, Loss 0.26813721656799316\n",
            "[Training Epoch 60] Batch 3348, Loss 0.2669161558151245\n",
            "[Training Epoch 60] Batch 3349, Loss 0.2492891252040863\n",
            "[Training Epoch 60] Batch 3350, Loss 0.289937287569046\n",
            "[Training Epoch 60] Batch 3351, Loss 0.27414679527282715\n",
            "[Training Epoch 60] Batch 3352, Loss 0.26102906465530396\n",
            "[Training Epoch 60] Batch 3353, Loss 0.2720370292663574\n",
            "[Training Epoch 60] Batch 3354, Loss 0.27381280064582825\n",
            "[Training Epoch 60] Batch 3355, Loss 0.28500232100486755\n",
            "[Training Epoch 60] Batch 3356, Loss 0.27620476484298706\n",
            "[Training Epoch 60] Batch 3357, Loss 0.27888792753219604\n",
            "[Training Epoch 60] Batch 3358, Loss 0.26379233598709106\n",
            "[Training Epoch 60] Batch 3359, Loss 0.26715829968452454\n",
            "[Training Epoch 60] Batch 3360, Loss 0.30912429094314575\n",
            "[Training Epoch 60] Batch 3361, Loss 0.22458286583423615\n",
            "[Training Epoch 60] Batch 3362, Loss 0.26808303594589233\n",
            "[Training Epoch 60] Batch 3363, Loss 0.27792325615882874\n",
            "[Training Epoch 60] Batch 3364, Loss 0.22771629691123962\n",
            "[Training Epoch 60] Batch 3365, Loss 0.266722172498703\n",
            "[Training Epoch 60] Batch 3366, Loss 0.26124852895736694\n",
            "[Training Epoch 60] Batch 3367, Loss 0.26265159249305725\n",
            "[Training Epoch 60] Batch 3368, Loss 0.27684637904167175\n",
            "[Training Epoch 60] Batch 3369, Loss 0.2611919641494751\n",
            "[Training Epoch 60] Batch 3370, Loss 0.2702820599079132\n",
            "[Training Epoch 60] Batch 3371, Loss 0.25191494822502136\n",
            "[Training Epoch 60] Batch 3372, Loss 0.2709389626979828\n",
            "[Training Epoch 60] Batch 3373, Loss 0.26217490434646606\n",
            "[Training Epoch 60] Batch 3374, Loss 0.29105857014656067\n",
            "[Training Epoch 60] Batch 3375, Loss 0.30640918016433716\n",
            "[Training Epoch 60] Batch 3376, Loss 0.25075188279151917\n",
            "[Training Epoch 60] Batch 3377, Loss 0.23563992977142334\n",
            "[Training Epoch 60] Batch 3378, Loss 0.28054487705230713\n",
            "[Training Epoch 60] Batch 3379, Loss 0.270669162273407\n",
            "[Training Epoch 60] Batch 3380, Loss 0.2535657286643982\n",
            "[Training Epoch 60] Batch 3381, Loss 0.2772153913974762\n",
            "[Training Epoch 60] Batch 3382, Loss 0.2561075985431671\n",
            "[Training Epoch 60] Batch 3383, Loss 0.28943631052970886\n",
            "[Training Epoch 60] Batch 3384, Loss 0.25132232904434204\n",
            "[Training Epoch 60] Batch 3385, Loss 0.27753424644470215\n",
            "[Training Epoch 60] Batch 3386, Loss 0.26415425539016724\n",
            "[Training Epoch 60] Batch 3387, Loss 0.2917229235172272\n",
            "[Training Epoch 60] Batch 3388, Loss 0.26189523935317993\n",
            "[Training Epoch 60] Batch 3389, Loss 0.2721920609474182\n",
            "[Training Epoch 60] Batch 3390, Loss 0.26725634932518005\n",
            "[Training Epoch 60] Batch 3391, Loss 0.2667508125305176\n",
            "[Training Epoch 60] Batch 3392, Loss 0.2790583372116089\n",
            "[Training Epoch 60] Batch 3393, Loss 0.2544509172439575\n",
            "[Training Epoch 60] Batch 3394, Loss 0.25117480754852295\n",
            "[Training Epoch 60] Batch 3395, Loss 0.2451578974723816\n",
            "[Training Epoch 60] Batch 3396, Loss 0.2550134062767029\n",
            "[Training Epoch 60] Batch 3397, Loss 0.2855473756790161\n",
            "[Training Epoch 60] Batch 3398, Loss 0.2772608995437622\n",
            "[Training Epoch 60] Batch 3399, Loss 0.2559499740600586\n",
            "[Training Epoch 60] Batch 3400, Loss 0.25620660185813904\n",
            "[Training Epoch 60] Batch 3401, Loss 0.2568981647491455\n",
            "[Training Epoch 60] Batch 3402, Loss 0.27367880940437317\n",
            "[Training Epoch 60] Batch 3403, Loss 0.29267191886901855\n",
            "[Training Epoch 60] Batch 3404, Loss 0.2767977714538574\n",
            "[Training Epoch 60] Batch 3405, Loss 0.28662604093551636\n",
            "[Training Epoch 60] Batch 3406, Loss 0.2647450864315033\n",
            "[Training Epoch 60] Batch 3407, Loss 0.26222240924835205\n",
            "[Training Epoch 60] Batch 3408, Loss 0.27597326040267944\n",
            "[Training Epoch 60] Batch 3409, Loss 0.26732510328292847\n",
            "[Training Epoch 60] Batch 3410, Loss 0.25553783774375916\n",
            "[Training Epoch 60] Batch 3411, Loss 0.2711605131626129\n",
            "[Training Epoch 60] Batch 3412, Loss 0.26817452907562256\n",
            "[Training Epoch 60] Batch 3413, Loss 0.2697846293449402\n",
            "[Training Epoch 60] Batch 3414, Loss 0.25858381390571594\n",
            "[Training Epoch 60] Batch 3415, Loss 0.28916242718696594\n",
            "[Training Epoch 60] Batch 3416, Loss 0.24350938200950623\n",
            "[Training Epoch 60] Batch 3417, Loss 0.27676570415496826\n",
            "[Training Epoch 60] Batch 3418, Loss 0.2629110813140869\n",
            "[Training Epoch 60] Batch 3419, Loss 0.27108582854270935\n",
            "[Training Epoch 60] Batch 3420, Loss 0.27257388830184937\n",
            "[Training Epoch 60] Batch 3421, Loss 0.26089397072792053\n",
            "[Training Epoch 60] Batch 3422, Loss 0.2982928156852722\n",
            "[Training Epoch 60] Batch 3423, Loss 0.25550276041030884\n",
            "[Training Epoch 60] Batch 3424, Loss 0.2682568430900574\n",
            "[Training Epoch 60] Batch 3425, Loss 0.2675017714500427\n",
            "[Training Epoch 60] Batch 3426, Loss 0.2574070990085602\n",
            "[Training Epoch 60] Batch 3427, Loss 0.3061036765575409\n",
            "[Training Epoch 60] Batch 3428, Loss 0.24254393577575684\n",
            "[Training Epoch 60] Batch 3429, Loss 0.2740534842014313\n",
            "[Training Epoch 60] Batch 3430, Loss 0.26162633299827576\n",
            "[Training Epoch 60] Batch 3431, Loss 0.2583189010620117\n",
            "[Training Epoch 60] Batch 3432, Loss 0.23899781703948975\n",
            "[Training Epoch 60] Batch 3433, Loss 0.23974958062171936\n",
            "[Training Epoch 60] Batch 3434, Loss 0.2958076596260071\n",
            "[Training Epoch 60] Batch 3435, Loss 0.2702014446258545\n",
            "[Training Epoch 60] Batch 3436, Loss 0.2306239902973175\n",
            "[Training Epoch 60] Batch 3437, Loss 0.272648423910141\n",
            "[Training Epoch 60] Batch 3438, Loss 0.26999878883361816\n",
            "[Training Epoch 60] Batch 3439, Loss 0.2511889934539795\n",
            "[Training Epoch 60] Batch 3440, Loss 0.25198620557785034\n",
            "[Training Epoch 60] Batch 3441, Loss 0.26365652680397034\n",
            "[Training Epoch 60] Batch 3442, Loss 0.2723860442638397\n",
            "[Training Epoch 60] Batch 3443, Loss 0.25196757912635803\n",
            "[Training Epoch 60] Batch 3444, Loss 0.28126662969589233\n",
            "[Training Epoch 60] Batch 3445, Loss 0.26736128330230713\n",
            "[Training Epoch 60] Batch 3446, Loss 0.26726311445236206\n",
            "[Training Epoch 60] Batch 3447, Loss 0.2863863408565521\n",
            "[Training Epoch 60] Batch 3448, Loss 0.2627524733543396\n",
            "[Training Epoch 60] Batch 3449, Loss 0.26822030544281006\n",
            "[Training Epoch 60] Batch 3450, Loss 0.2671949565410614\n",
            "[Training Epoch 60] Batch 3451, Loss 0.28754132986068726\n",
            "[Training Epoch 60] Batch 3452, Loss 0.2751636207103729\n",
            "[Training Epoch 60] Batch 3453, Loss 0.2856944799423218\n",
            "[Training Epoch 60] Batch 3454, Loss 0.28875985741615295\n",
            "[Training Epoch 60] Batch 3455, Loss 0.28625038266181946\n",
            "[Training Epoch 60] Batch 3456, Loss 0.23026889562606812\n",
            "[Training Epoch 60] Batch 3457, Loss 0.25885671377182007\n",
            "[Training Epoch 60] Batch 3458, Loss 0.28031569719314575\n",
            "[Training Epoch 60] Batch 3459, Loss 0.27205222845077515\n",
            "[Training Epoch 60] Batch 3460, Loss 0.24651992321014404\n",
            "[Training Epoch 60] Batch 3461, Loss 0.30610769987106323\n",
            "[Training Epoch 60] Batch 3462, Loss 0.273427277803421\n",
            "[Training Epoch 60] Batch 3463, Loss 0.2893877625465393\n",
            "[Training Epoch 60] Batch 3464, Loss 0.28016990423202515\n",
            "[Training Epoch 60] Batch 3465, Loss 0.28479111194610596\n",
            "[Training Epoch 60] Batch 3466, Loss 0.25161653757095337\n",
            "[Training Epoch 60] Batch 3467, Loss 0.2816663980484009\n",
            "[Training Epoch 60] Batch 3468, Loss 0.2666056752204895\n",
            "[Training Epoch 60] Batch 3469, Loss 0.26021814346313477\n",
            "[Training Epoch 60] Batch 3470, Loss 0.2840336859226227\n",
            "[Training Epoch 60] Batch 3471, Loss 0.2646608352661133\n",
            "[Training Epoch 60] Batch 3472, Loss 0.2633325159549713\n",
            "[Training Epoch 60] Batch 3473, Loss 0.2645002603530884\n",
            "[Training Epoch 60] Batch 3474, Loss 0.25030604004859924\n",
            "[Training Epoch 60] Batch 3475, Loss 0.2546798288822174\n",
            "[Training Epoch 60] Batch 3476, Loss 0.2503625750541687\n",
            "[Training Epoch 60] Batch 3477, Loss 0.264204204082489\n",
            "[Training Epoch 60] Batch 3478, Loss 0.26143476366996765\n",
            "[Training Epoch 60] Batch 3479, Loss 0.29390162229537964\n",
            "[Training Epoch 60] Batch 3480, Loss 0.25598710775375366\n",
            "[Training Epoch 60] Batch 3481, Loss 0.2784908413887024\n",
            "[Training Epoch 60] Batch 3482, Loss 0.2812100052833557\n",
            "[Training Epoch 60] Batch 3483, Loss 0.26949673891067505\n",
            "[Training Epoch 60] Batch 3484, Loss 0.26796019077301025\n",
            "[Training Epoch 60] Batch 3485, Loss 0.26868152618408203\n",
            "[Training Epoch 60] Batch 3486, Loss 0.26481160521507263\n",
            "[Training Epoch 60] Batch 3487, Loss 0.25492340326309204\n",
            "[Training Epoch 60] Batch 3488, Loss 0.2748899757862091\n",
            "[Training Epoch 60] Batch 3489, Loss 0.2544827461242676\n",
            "[Training Epoch 60] Batch 3490, Loss 0.26218876242637634\n",
            "[Training Epoch 60] Batch 3491, Loss 0.2662612199783325\n",
            "[Training Epoch 60] Batch 3492, Loss 0.26703089475631714\n",
            "[Training Epoch 60] Batch 3493, Loss 0.26964274048805237\n",
            "[Training Epoch 60] Batch 3494, Loss 0.2360461801290512\n",
            "[Training Epoch 60] Batch 3495, Loss 0.27680960297584534\n",
            "[Training Epoch 60] Batch 3496, Loss 0.28253868222236633\n",
            "[Training Epoch 60] Batch 3497, Loss 0.26360732316970825\n",
            "[Training Epoch 60] Batch 3498, Loss 0.26832470297813416\n",
            "[Training Epoch 60] Batch 3499, Loss 0.2570697069168091\n",
            "[Training Epoch 60] Batch 3500, Loss 0.2729487121105194\n",
            "[Training Epoch 60] Batch 3501, Loss 0.27727997303009033\n",
            "[Training Epoch 60] Batch 3502, Loss 0.2735392451286316\n",
            "[Training Epoch 60] Batch 3503, Loss 0.26495257019996643\n",
            "[Training Epoch 60] Batch 3504, Loss 0.27030858397483826\n",
            "[Training Epoch 60] Batch 3505, Loss 0.2598459720611572\n",
            "[Training Epoch 60] Batch 3506, Loss 0.27607202529907227\n",
            "[Training Epoch 60] Batch 3507, Loss 0.27630579471588135\n",
            "[Training Epoch 60] Batch 3508, Loss 0.25219297409057617\n",
            "[Training Epoch 60] Batch 3509, Loss 0.27571752667427063\n",
            "[Training Epoch 60] Batch 3510, Loss 0.2914830446243286\n",
            "[Training Epoch 60] Batch 3511, Loss 0.2731229066848755\n",
            "[Training Epoch 60] Batch 3512, Loss 0.2742660343647003\n",
            "[Training Epoch 60] Batch 3513, Loss 0.28704988956451416\n",
            "[Training Epoch 60] Batch 3514, Loss 0.2739080488681793\n",
            "[Training Epoch 60] Batch 3515, Loss 0.27290773391723633\n",
            "[Training Epoch 60] Batch 3516, Loss 0.24604114890098572\n",
            "[Training Epoch 60] Batch 3517, Loss 0.27403759956359863\n",
            "[Training Epoch 60] Batch 3518, Loss 0.24666720628738403\n",
            "[Training Epoch 60] Batch 3519, Loss 0.28337395191192627\n",
            "[Training Epoch 60] Batch 3520, Loss 0.2532058358192444\n",
            "[Training Epoch 60] Batch 3521, Loss 0.24504679441452026\n",
            "[Training Epoch 60] Batch 3522, Loss 0.24720735847949982\n",
            "[Training Epoch 60] Batch 3523, Loss 0.306413471698761\n",
            "[Training Epoch 60] Batch 3524, Loss 0.26542285084724426\n",
            "[Training Epoch 60] Batch 3525, Loss 0.2687743008136749\n",
            "[Training Epoch 60] Batch 3526, Loss 0.2829018533229828\n",
            "[Training Epoch 60] Batch 3527, Loss 0.24976208806037903\n",
            "[Training Epoch 60] Batch 3528, Loss 0.2516137659549713\n",
            "[Training Epoch 60] Batch 3529, Loss 0.286800354719162\n",
            "[Training Epoch 60] Batch 3530, Loss 0.25281473994255066\n",
            "[Training Epoch 60] Batch 3531, Loss 0.26595813035964966\n",
            "[Training Epoch 60] Batch 3532, Loss 0.24140597879886627\n",
            "[Training Epoch 60] Batch 3533, Loss 0.2984292209148407\n",
            "[Training Epoch 60] Batch 3534, Loss 0.2790125906467438\n",
            "[Training Epoch 60] Batch 3535, Loss 0.24503430724143982\n",
            "[Training Epoch 60] Batch 3536, Loss 0.27804774045944214\n",
            "[Training Epoch 60] Batch 3537, Loss 0.25775057077407837\n",
            "[Training Epoch 60] Batch 3538, Loss 0.27317455410957336\n",
            "[Training Epoch 60] Batch 3539, Loss 0.2688024938106537\n",
            "[Training Epoch 60] Batch 3540, Loss 0.3126209080219269\n",
            "[Training Epoch 60] Batch 3541, Loss 0.28661006689071655\n",
            "[Training Epoch 60] Batch 3542, Loss 0.2618388533592224\n",
            "[Training Epoch 60] Batch 3543, Loss 0.26223865151405334\n",
            "[Training Epoch 60] Batch 3544, Loss 0.2573075592517853\n",
            "[Training Epoch 60] Batch 3545, Loss 0.2785887122154236\n",
            "[Training Epoch 60] Batch 3546, Loss 0.2792902886867523\n",
            "[Training Epoch 60] Batch 3547, Loss 0.26820382475852966\n",
            "[Training Epoch 60] Batch 3548, Loss 0.2727567255496979\n",
            "[Training Epoch 60] Batch 3549, Loss 0.2603882849216461\n",
            "[Training Epoch 60] Batch 3550, Loss 0.23818935453891754\n",
            "[Training Epoch 60] Batch 3551, Loss 0.2813338041305542\n",
            "[Training Epoch 60] Batch 3552, Loss 0.26418331265449524\n",
            "[Training Epoch 60] Batch 3553, Loss 0.2496226280927658\n",
            "[Training Epoch 60] Batch 3554, Loss 0.28698673844337463\n",
            "[Training Epoch 60] Batch 3555, Loss 0.2940804958343506\n",
            "[Training Epoch 60] Batch 3556, Loss 0.24652063846588135\n",
            "[Training Epoch 60] Batch 3557, Loss 0.25681599974632263\n",
            "[Training Epoch 60] Batch 3558, Loss 0.2661115229129791\n",
            "[Training Epoch 60] Batch 3559, Loss 0.2638024389743805\n",
            "[Training Epoch 60] Batch 3560, Loss 0.27854758501052856\n",
            "[Training Epoch 60] Batch 3561, Loss 0.278655469417572\n",
            "[Training Epoch 60] Batch 3562, Loss 0.25967374444007874\n",
            "[Training Epoch 60] Batch 3563, Loss 0.27463972568511963\n",
            "[Training Epoch 60] Batch 3564, Loss 0.23394885659217834\n",
            "[Training Epoch 60] Batch 3565, Loss 0.26686733961105347\n",
            "[Training Epoch 60] Batch 3566, Loss 0.3004975914955139\n",
            "[Training Epoch 60] Batch 3567, Loss 0.27819380164146423\n",
            "[Training Epoch 60] Batch 3568, Loss 0.23969396948814392\n",
            "[Training Epoch 60] Batch 3569, Loss 0.2659916281700134\n",
            "[Training Epoch 60] Batch 3570, Loss 0.2719181478023529\n",
            "[Training Epoch 60] Batch 3571, Loss 0.2849009335041046\n",
            "[Training Epoch 60] Batch 3572, Loss 0.2719571888446808\n",
            "[Training Epoch 60] Batch 3573, Loss 0.2555873692035675\n",
            "[Training Epoch 60] Batch 3574, Loss 0.2744923532009125\n",
            "[Training Epoch 60] Batch 3575, Loss 0.2596408724784851\n",
            "[Training Epoch 60] Batch 3576, Loss 0.24951998889446259\n",
            "[Training Epoch 60] Batch 3577, Loss 0.24879775941371918\n",
            "[Training Epoch 60] Batch 3578, Loss 0.23887468874454498\n",
            "[Training Epoch 60] Batch 3579, Loss 0.2665078341960907\n",
            "[Training Epoch 60] Batch 3580, Loss 0.27861812710762024\n",
            "[Training Epoch 60] Batch 3581, Loss 0.28942620754241943\n",
            "[Training Epoch 60] Batch 3582, Loss 0.23985618352890015\n",
            "[Training Epoch 60] Batch 3583, Loss 0.27493971586227417\n",
            "[Training Epoch 60] Batch 3584, Loss 0.25538596510887146\n",
            "[Training Epoch 60] Batch 3585, Loss 0.23735816776752472\n",
            "[Training Epoch 60] Batch 3586, Loss 0.2815089523792267\n",
            "[Training Epoch 60] Batch 3587, Loss 0.25564903020858765\n",
            "[Training Epoch 60] Batch 3588, Loss 0.2811613082885742\n",
            "[Training Epoch 60] Batch 3589, Loss 0.2818136215209961\n",
            "[Training Epoch 60] Batch 3590, Loss 0.2611966133117676\n",
            "[Training Epoch 60] Batch 3591, Loss 0.2445073127746582\n",
            "[Training Epoch 60] Batch 3592, Loss 0.30136334896087646\n",
            "[Training Epoch 60] Batch 3593, Loss 0.243477001786232\n",
            "[Training Epoch 60] Batch 3594, Loss 0.29363298416137695\n",
            "[Training Epoch 60] Batch 3595, Loss 0.26010003685951233\n",
            "[Training Epoch 60] Batch 3596, Loss 0.2907409071922302\n",
            "[Training Epoch 60] Batch 3597, Loss 0.26446789503097534\n",
            "[Training Epoch 60] Batch 3598, Loss 0.3063737154006958\n",
            "[Training Epoch 60] Batch 3599, Loss 0.30238839983940125\n",
            "[Training Epoch 60] Batch 3600, Loss 0.25587689876556396\n",
            "[Training Epoch 60] Batch 3601, Loss 0.25326523184776306\n",
            "[Training Epoch 60] Batch 3602, Loss 0.27224263548851013\n",
            "[Training Epoch 60] Batch 3603, Loss 0.2604106366634369\n",
            "[Training Epoch 60] Batch 3604, Loss 0.27838945388793945\n",
            "[Training Epoch 60] Batch 3605, Loss 0.2545022964477539\n",
            "[Training Epoch 60] Batch 3606, Loss 0.23371699452400208\n",
            "[Training Epoch 60] Batch 3607, Loss 0.2756064236164093\n",
            "[Training Epoch 60] Batch 3608, Loss 0.24407921731472015\n",
            "[Training Epoch 60] Batch 3609, Loss 0.2822878956794739\n",
            "[Training Epoch 60] Batch 3610, Loss 0.31112000346183777\n",
            "[Training Epoch 60] Batch 3611, Loss 0.25949224829673767\n",
            "[Training Epoch 60] Batch 3612, Loss 0.2757004499435425\n",
            "[Training Epoch 60] Batch 3613, Loss 0.24990086257457733\n",
            "[Training Epoch 60] Batch 3614, Loss 0.2631496787071228\n",
            "[Training Epoch 60] Batch 3615, Loss 0.2694460451602936\n",
            "[Training Epoch 60] Batch 3616, Loss 0.24181941151618958\n",
            "[Training Epoch 60] Batch 3617, Loss 0.28717613220214844\n",
            "[Training Epoch 60] Batch 3618, Loss 0.2704923450946808\n",
            "[Training Epoch 60] Batch 3619, Loss 0.2613948881626129\n",
            "[Training Epoch 60] Batch 3620, Loss 0.24053774774074554\n",
            "[Training Epoch 60] Batch 3621, Loss 0.24523493647575378\n",
            "[Training Epoch 60] Batch 3622, Loss 0.26080983877182007\n",
            "[Training Epoch 60] Batch 3623, Loss 0.2610866129398346\n",
            "[Training Epoch 60] Batch 3624, Loss 0.24528445303440094\n",
            "[Training Epoch 60] Batch 3625, Loss 0.2763427197933197\n",
            "[Training Epoch 60] Batch 3626, Loss 0.2589592933654785\n",
            "[Training Epoch 60] Batch 3627, Loss 0.251516729593277\n",
            "[Training Epoch 60] Batch 3628, Loss 0.2642061412334442\n",
            "[Training Epoch 60] Batch 3629, Loss 0.2856685519218445\n",
            "[Training Epoch 60] Batch 3630, Loss 0.26379671692848206\n",
            "[Training Epoch 60] Batch 3631, Loss 0.27597537636756897\n",
            "[Training Epoch 60] Batch 3632, Loss 0.2481217235326767\n",
            "[Training Epoch 60] Batch 3633, Loss 0.27595266699790955\n",
            "[Training Epoch 60] Batch 3634, Loss 0.2754628360271454\n",
            "[Training Epoch 60] Batch 3635, Loss 0.2477683126926422\n",
            "[Training Epoch 60] Batch 3636, Loss 0.293215274810791\n",
            "[Training Epoch 60] Batch 3637, Loss 0.27977436780929565\n",
            "[Training Epoch 60] Batch 3638, Loss 0.2535531222820282\n",
            "[Training Epoch 60] Batch 3639, Loss 0.2802821695804596\n",
            "[Training Epoch 60] Batch 3640, Loss 0.2518249452114105\n",
            "[Training Epoch 60] Batch 3641, Loss 0.260158509016037\n",
            "[Training Epoch 60] Batch 3642, Loss 0.2836307883262634\n",
            "[Training Epoch 60] Batch 3643, Loss 0.27173522114753723\n",
            "[Training Epoch 60] Batch 3644, Loss 0.25883328914642334\n",
            "[Training Epoch 60] Batch 3645, Loss 0.2642935514450073\n",
            "[Training Epoch 60] Batch 3646, Loss 0.24875877797603607\n",
            "[Training Epoch 60] Batch 3647, Loss 0.27452653646469116\n",
            "[Training Epoch 60] Batch 3648, Loss 0.2571534812450409\n",
            "[Training Epoch 60] Batch 3649, Loss 0.269257515668869\n",
            "[Training Epoch 60] Batch 3650, Loss 0.27277055382728577\n",
            "[Training Epoch 60] Batch 3651, Loss 0.217802494764328\n",
            "[Training Epoch 60] Batch 3652, Loss 0.2539973258972168\n",
            "[Training Epoch 60] Batch 3653, Loss 0.2783730626106262\n",
            "[Training Epoch 60] Batch 3654, Loss 0.2559761106967926\n",
            "[Training Epoch 60] Batch 3655, Loss 0.2494698166847229\n",
            "[Training Epoch 60] Batch 3656, Loss 0.2875820994377136\n",
            "[Training Epoch 60] Batch 3657, Loss 0.2882460951805115\n",
            "[Training Epoch 60] Batch 3658, Loss 0.2567405104637146\n",
            "[Training Epoch 60] Batch 3659, Loss 0.2408621609210968\n",
            "[Training Epoch 60] Batch 3660, Loss 0.2659643888473511\n",
            "[Training Epoch 60] Batch 3661, Loss 0.28603577613830566\n",
            "[Training Epoch 60] Batch 3662, Loss 0.266452819108963\n",
            "[Training Epoch 60] Batch 3663, Loss 0.2977539300918579\n",
            "[Training Epoch 60] Batch 3664, Loss 0.2663697302341461\n",
            "[Training Epoch 60] Batch 3665, Loss 0.22967183589935303\n",
            "[Training Epoch 60] Batch 3666, Loss 0.24531492590904236\n",
            "[Training Epoch 60] Batch 3667, Loss 0.24353164434432983\n",
            "[Training Epoch 60] Batch 3668, Loss 0.28281599283218384\n",
            "[Training Epoch 60] Batch 3669, Loss 0.27852916717529297\n",
            "[Training Epoch 60] Batch 3670, Loss 0.28440892696380615\n",
            "[Training Epoch 60] Batch 3671, Loss 0.2615780532360077\n",
            "[Training Epoch 60] Batch 3672, Loss 0.2647782862186432\n",
            "[Training Epoch 60] Batch 3673, Loss 0.26757460832595825\n",
            "[Training Epoch 60] Batch 3674, Loss 0.24913285672664642\n",
            "[Training Epoch 60] Batch 3675, Loss 0.2787904143333435\n",
            "[Training Epoch 60] Batch 3676, Loss 0.22701650857925415\n",
            "[Training Epoch 60] Batch 3677, Loss 0.25197863578796387\n",
            "[Training Epoch 60] Batch 3678, Loss 0.280886709690094\n",
            "[Training Epoch 60] Batch 3679, Loss 0.24553464353084564\n",
            "[Training Epoch 60] Batch 3680, Loss 0.25356218218803406\n",
            "[Training Epoch 60] Batch 3681, Loss 0.270469069480896\n",
            "[Training Epoch 60] Batch 3682, Loss 0.24632278084754944\n",
            "[Training Epoch 60] Batch 3683, Loss 0.2913661003112793\n",
            "[Training Epoch 60] Batch 3684, Loss 0.2636336386203766\n",
            "[Training Epoch 60] Batch 3685, Loss 0.2819456458091736\n",
            "[Training Epoch 60] Batch 3686, Loss 0.28306788206100464\n",
            "[Training Epoch 60] Batch 3687, Loss 0.2586682140827179\n",
            "[Training Epoch 60] Batch 3688, Loss 0.2694993019104004\n",
            "[Training Epoch 60] Batch 3689, Loss 0.26633477210998535\n",
            "[Training Epoch 60] Batch 3690, Loss 0.26097381114959717\n",
            "[Training Epoch 60] Batch 3691, Loss 0.2718966603279114\n",
            "[Training Epoch 60] Batch 3692, Loss 0.22754821181297302\n",
            "[Training Epoch 60] Batch 3693, Loss 0.28300413489341736\n",
            "[Training Epoch 60] Batch 3694, Loss 0.2526596784591675\n",
            "[Training Epoch 60] Batch 3695, Loss 0.2668115496635437\n",
            "[Training Epoch 60] Batch 3696, Loss 0.2828609347343445\n",
            "[Training Epoch 60] Batch 3697, Loss 0.2829304039478302\n",
            "[Training Epoch 60] Batch 3698, Loss 0.2793360948562622\n",
            "[Training Epoch 60] Batch 3699, Loss 0.2490502893924713\n",
            "[Training Epoch 60] Batch 3700, Loss 0.2597658932209015\n",
            "[Training Epoch 60] Batch 3701, Loss 0.24571770429611206\n",
            "[Training Epoch 60] Batch 3702, Loss 0.262131005525589\n",
            "[Training Epoch 60] Batch 3703, Loss 0.2549528479576111\n",
            "[Training Epoch 60] Batch 3704, Loss 0.27490895986557007\n",
            "[Training Epoch 60] Batch 3705, Loss 0.26462802290916443\n",
            "[Training Epoch 60] Batch 3706, Loss 0.27042028307914734\n",
            "[Training Epoch 60] Batch 3707, Loss 0.27824556827545166\n",
            "[Training Epoch 60] Batch 3708, Loss 0.28988224267959595\n",
            "[Training Epoch 60] Batch 3709, Loss 0.26593801379203796\n",
            "[Training Epoch 60] Batch 3710, Loss 0.29065215587615967\n",
            "[Training Epoch 60] Batch 3711, Loss 0.24544581770896912\n",
            "[Training Epoch 60] Batch 3712, Loss 0.24150320887565613\n",
            "[Training Epoch 60] Batch 3713, Loss 0.2792087197303772\n",
            "[Training Epoch 60] Batch 3714, Loss 0.2547263503074646\n",
            "[Training Epoch 60] Batch 3715, Loss 0.25856757164001465\n",
            "[Training Epoch 60] Batch 3716, Loss 0.25106126070022583\n",
            "[Training Epoch 60] Batch 3717, Loss 0.2861056923866272\n",
            "[Training Epoch 60] Batch 3718, Loss 0.2773434817790985\n",
            "[Training Epoch 60] Batch 3719, Loss 0.24773061275482178\n",
            "[Training Epoch 60] Batch 3720, Loss 0.2623681128025055\n",
            "[Training Epoch 60] Batch 3721, Loss 0.2359209954738617\n",
            "[Training Epoch 60] Batch 3722, Loss 0.2691623270511627\n",
            "[Training Epoch 60] Batch 3723, Loss 0.2725146412849426\n",
            "[Training Epoch 60] Batch 3724, Loss 0.2586357295513153\n",
            "[Training Epoch 60] Batch 3725, Loss 0.290769100189209\n",
            "[Training Epoch 60] Batch 3726, Loss 0.2541361153125763\n",
            "[Training Epoch 60] Batch 3727, Loss 0.26114949584007263\n",
            "[Training Epoch 60] Batch 3728, Loss 0.23777009546756744\n",
            "[Training Epoch 60] Batch 3729, Loss 0.2574540972709656\n",
            "[Training Epoch 60] Batch 3730, Loss 0.304463267326355\n",
            "[Training Epoch 60] Batch 3731, Loss 0.23535287380218506\n",
            "[Training Epoch 60] Batch 3732, Loss 0.26785850524902344\n",
            "[Training Epoch 60] Batch 3733, Loss 0.2633930742740631\n",
            "[Training Epoch 60] Batch 3734, Loss 0.24395699799060822\n",
            "[Training Epoch 60] Batch 3735, Loss 0.24829119443893433\n",
            "[Training Epoch 60] Batch 3736, Loss 0.25927919149398804\n",
            "[Training Epoch 60] Batch 3737, Loss 0.2381575107574463\n",
            "[Training Epoch 60] Batch 3738, Loss 0.26368311047554016\n",
            "[Training Epoch 60] Batch 3739, Loss 0.26386380195617676\n",
            "[Training Epoch 60] Batch 3740, Loss 0.2386600822210312\n",
            "[Training Epoch 60] Batch 3741, Loss 0.2525920867919922\n",
            "[Training Epoch 60] Batch 3742, Loss 0.2567253112792969\n",
            "[Training Epoch 60] Batch 3743, Loss 0.27124959230422974\n",
            "[Training Epoch 60] Batch 3744, Loss 0.25344276428222656\n",
            "[Training Epoch 60] Batch 3745, Loss 0.2680175006389618\n",
            "[Training Epoch 60] Batch 3746, Loss 0.24066172540187836\n",
            "[Training Epoch 60] Batch 3747, Loss 0.2547907531261444\n",
            "[Training Epoch 60] Batch 3748, Loss 0.27587196230888367\n",
            "[Training Epoch 60] Batch 3749, Loss 0.24605944752693176\n",
            "[Training Epoch 60] Batch 3750, Loss 0.2598082423210144\n",
            "[Training Epoch 60] Batch 3751, Loss 0.24259330332279205\n",
            "[Training Epoch 60] Batch 3752, Loss 0.2345026135444641\n",
            "[Training Epoch 60] Batch 3753, Loss 0.2647305428981781\n",
            "[Training Epoch 60] Batch 3754, Loss 0.26410719752311707\n",
            "[Training Epoch 60] Batch 3755, Loss 0.2649138569831848\n",
            "[Training Epoch 60] Batch 3756, Loss 0.26591116189956665\n",
            "[Training Epoch 60] Batch 3757, Loss 0.2888839840888977\n",
            "[Training Epoch 60] Batch 3758, Loss 0.25179386138916016\n",
            "[Training Epoch 60] Batch 3759, Loss 0.28190454840660095\n",
            "[Training Epoch 60] Batch 3760, Loss 0.23771266639232635\n",
            "[Training Epoch 60] Batch 3761, Loss 0.2877187132835388\n",
            "[Training Epoch 60] Batch 3762, Loss 0.23921293020248413\n",
            "[Training Epoch 60] Batch 3763, Loss 0.2693783640861511\n",
            "[Training Epoch 60] Batch 3764, Loss 0.2493448257446289\n",
            "[Training Epoch 60] Batch 3765, Loss 0.2725975513458252\n",
            "[Training Epoch 60] Batch 3766, Loss 0.2441297471523285\n",
            "[Training Epoch 60] Batch 3767, Loss 0.24763402342796326\n",
            "[Training Epoch 60] Batch 3768, Loss 0.2837803065776825\n",
            "[Training Epoch 60] Batch 3769, Loss 0.21226325631141663\n",
            "[Training Epoch 60] Batch 3770, Loss 0.2737601399421692\n",
            "[Training Epoch 60] Batch 3771, Loss 0.24636641144752502\n",
            "[Training Epoch 60] Batch 3772, Loss 0.256326824426651\n",
            "[Training Epoch 60] Batch 3773, Loss 0.25947728753089905\n",
            "[Training Epoch 60] Batch 3774, Loss 0.2652486562728882\n",
            "[Training Epoch 60] Batch 3775, Loss 0.2674068808555603\n",
            "[Training Epoch 60] Batch 3776, Loss 0.2558096647262573\n",
            "[Training Epoch 60] Batch 3777, Loss 0.25713998079299927\n",
            "[Training Epoch 60] Batch 3778, Loss 0.25915810465812683\n",
            "[Training Epoch 60] Batch 3779, Loss 0.25661084055900574\n",
            "[Training Epoch 60] Batch 3780, Loss 0.27641579508781433\n",
            "[Training Epoch 60] Batch 3781, Loss 0.2814559042453766\n",
            "[Training Epoch 60] Batch 3782, Loss 0.261193186044693\n",
            "[Training Epoch 60] Batch 3783, Loss 0.2750542163848877\n",
            "[Training Epoch 60] Batch 3784, Loss 0.2820203900337219\n",
            "[Training Epoch 60] Batch 3785, Loss 0.2718164324760437\n",
            "[Training Epoch 60] Batch 3786, Loss 0.2690410912036896\n",
            "[Training Epoch 60] Batch 3787, Loss 0.28418615460395813\n",
            "[Training Epoch 60] Batch 3788, Loss 0.2751888334751129\n",
            "[Training Epoch 60] Batch 3789, Loss 0.2764667868614197\n",
            "[Training Epoch 60] Batch 3790, Loss 0.22010710835456848\n",
            "[Training Epoch 60] Batch 3791, Loss 0.2556245028972626\n",
            "[Training Epoch 60] Batch 3792, Loss 0.24434402585029602\n",
            "[Training Epoch 60] Batch 3793, Loss 0.24894824624061584\n",
            "[Training Epoch 60] Batch 3794, Loss 0.2567305266857147\n",
            "[Training Epoch 60] Batch 3795, Loss 0.29177844524383545\n",
            "[Training Epoch 60] Batch 3796, Loss 0.2630082964897156\n",
            "[Training Epoch 60] Batch 3797, Loss 0.27677181363105774\n",
            "[Training Epoch 60] Batch 3798, Loss 0.2687195837497711\n",
            "[Training Epoch 60] Batch 3799, Loss 0.29173189401626587\n",
            "[Training Epoch 60] Batch 3800, Loss 0.2602387070655823\n",
            "[Training Epoch 60] Batch 3801, Loss 0.27019935846328735\n",
            "[Training Epoch 60] Batch 3802, Loss 0.27539825439453125\n",
            "[Training Epoch 60] Batch 3803, Loss 0.24745537340641022\n",
            "[Training Epoch 60] Batch 3804, Loss 0.28243017196655273\n",
            "[Training Epoch 60] Batch 3805, Loss 0.2429552525281906\n",
            "[Training Epoch 60] Batch 3806, Loss 0.2613212466239929\n",
            "[Training Epoch 60] Batch 3807, Loss 0.26369673013687134\n",
            "[Training Epoch 60] Batch 3808, Loss 0.27233079075813293\n",
            "[Training Epoch 60] Batch 3809, Loss 0.25529715418815613\n",
            "[Training Epoch 60] Batch 3810, Loss 0.2924255132675171\n",
            "[Training Epoch 60] Batch 3811, Loss 0.2682190537452698\n",
            "[Training Epoch 60] Batch 3812, Loss 0.2844972014427185\n",
            "[Training Epoch 60] Batch 3813, Loss 0.26878809928894043\n",
            "[Training Epoch 60] Batch 3814, Loss 0.2377956509590149\n",
            "[Training Epoch 60] Batch 3815, Loss 0.2805892825126648\n",
            "[Training Epoch 60] Batch 3816, Loss 0.2778864800930023\n",
            "[Training Epoch 60] Batch 3817, Loss 0.2583041191101074\n",
            "[Training Epoch 60] Batch 3818, Loss 0.2797439992427826\n",
            "[Training Epoch 60] Batch 3819, Loss 0.2968948185443878\n",
            "[Training Epoch 60] Batch 3820, Loss 0.2750305235385895\n",
            "[Training Epoch 60] Batch 3821, Loss 0.2840478718280792\n",
            "[Training Epoch 60] Batch 3822, Loss 0.2765921950340271\n",
            "[Training Epoch 60] Batch 3823, Loss 0.28288590908050537\n",
            "[Training Epoch 60] Batch 3824, Loss 0.27064740657806396\n",
            "[Training Epoch 60] Batch 3825, Loss 0.27006787061691284\n",
            "[Training Epoch 60] Batch 3826, Loss 0.26021862030029297\n",
            "[Training Epoch 60] Batch 3827, Loss 0.3123510777950287\n",
            "[Training Epoch 60] Batch 3828, Loss 0.27815693616867065\n",
            "[Training Epoch 60] Batch 3829, Loss 0.2531544268131256\n",
            "[Training Epoch 60] Batch 3830, Loss 0.2594970464706421\n",
            "[Training Epoch 60] Batch 3831, Loss 0.2821410894393921\n",
            "[Training Epoch 60] Batch 3832, Loss 0.27025163173675537\n",
            "[Training Epoch 60] Batch 3833, Loss 0.26150476932525635\n",
            "[Training Epoch 60] Batch 3834, Loss 0.23464910686016083\n",
            "[Training Epoch 60] Batch 3835, Loss 0.29216712713241577\n",
            "[Training Epoch 60] Batch 3836, Loss 0.26583728194236755\n",
            "[Training Epoch 60] Batch 3837, Loss 0.2418450564146042\n",
            "[Training Epoch 60] Batch 3838, Loss 0.25601205229759216\n",
            "[Training Epoch 60] Batch 3839, Loss 0.2742688059806824\n",
            "[Training Epoch 60] Batch 3840, Loss 0.2593066394329071\n",
            "[Training Epoch 60] Batch 3841, Loss 0.2702164053916931\n",
            "[Training Epoch 60] Batch 3842, Loss 0.23606760799884796\n",
            "[Training Epoch 60] Batch 3843, Loss 0.2799876928329468\n",
            "[Training Epoch 60] Batch 3844, Loss 0.26673176884651184\n",
            "[Training Epoch 60] Batch 3845, Loss 0.26241111755371094\n",
            "[Training Epoch 60] Batch 3846, Loss 0.2661244869232178\n",
            "[Training Epoch 60] Batch 3847, Loss 0.2832878828048706\n",
            "[Training Epoch 60] Batch 3848, Loss 0.23943594098091125\n",
            "[Training Epoch 60] Batch 3849, Loss 0.25458618998527527\n",
            "[Training Epoch 60] Batch 3850, Loss 0.2660691738128662\n",
            "[Training Epoch 60] Batch 3851, Loss 0.27357709407806396\n",
            "[Training Epoch 60] Batch 3852, Loss 0.2501542568206787\n",
            "[Training Epoch 60] Batch 3853, Loss 0.2721085250377655\n",
            "[Training Epoch 60] Batch 3854, Loss 0.25461345911026\n",
            "[Training Epoch 60] Batch 3855, Loss 0.2604171633720398\n",
            "[Training Epoch 60] Batch 3856, Loss 0.2829749584197998\n",
            "[Training Epoch 60] Batch 3857, Loss 0.24774235486984253\n",
            "[Training Epoch 60] Batch 3858, Loss 0.3017861843109131\n",
            "[Training Epoch 60] Batch 3859, Loss 0.2914814352989197\n",
            "[Training Epoch 60] Batch 3860, Loss 0.26170215010643005\n",
            "[Training Epoch 60] Batch 3861, Loss 0.2566397190093994\n",
            "[Training Epoch 60] Batch 3862, Loss 0.26773127913475037\n",
            "[Training Epoch 60] Batch 3863, Loss 0.2548561096191406\n",
            "[Training Epoch 60] Batch 3864, Loss 0.2866874933242798\n",
            "[Training Epoch 60] Batch 3865, Loss 0.25495731830596924\n",
            "[Training Epoch 60] Batch 3866, Loss 0.27280908823013306\n",
            "[Training Epoch 60] Batch 3867, Loss 0.26269397139549255\n",
            "[Training Epoch 60] Batch 3868, Loss 0.26964622735977173\n",
            "[Training Epoch 60] Batch 3869, Loss 0.26003071665763855\n",
            "[Training Epoch 60] Batch 3870, Loss 0.2485438585281372\n",
            "[Training Epoch 60] Batch 3871, Loss 0.27540379762649536\n",
            "[Training Epoch 60] Batch 3872, Loss 0.2811216115951538\n",
            "[Training Epoch 60] Batch 3873, Loss 0.2549261450767517\n",
            "[Training Epoch 60] Batch 3874, Loss 0.2540890872478485\n",
            "[Training Epoch 60] Batch 3875, Loss 0.26158154010772705\n",
            "[Training Epoch 60] Batch 3876, Loss 0.2700099050998688\n",
            "[Training Epoch 60] Batch 3877, Loss 0.2906464636325836\n",
            "[Training Epoch 60] Batch 3878, Loss 0.2433084398508072\n",
            "[Training Epoch 60] Batch 3879, Loss 0.2505580484867096\n",
            "[Training Epoch 60] Batch 3880, Loss 0.25399860739707947\n",
            "[Training Epoch 60] Batch 3881, Loss 0.2450878918170929\n",
            "[Training Epoch 60] Batch 3882, Loss 0.27271556854248047\n",
            "[Training Epoch 60] Batch 3883, Loss 0.2605222165584564\n",
            "[Training Epoch 60] Batch 3884, Loss 0.24645069241523743\n",
            "[Training Epoch 60] Batch 3885, Loss 0.2901651859283447\n",
            "[Training Epoch 60] Batch 3886, Loss 0.283284991979599\n",
            "[Training Epoch 60] Batch 3887, Loss 0.27093562483787537\n",
            "[Training Epoch 60] Batch 3888, Loss 0.28861963748931885\n",
            "[Training Epoch 60] Batch 3889, Loss 0.2716771364212036\n",
            "[Training Epoch 60] Batch 3890, Loss 0.2625162601470947\n",
            "[Training Epoch 60] Batch 3891, Loss 0.28212642669677734\n",
            "[Training Epoch 60] Batch 3892, Loss 0.2709982097148895\n",
            "[Training Epoch 60] Batch 3893, Loss 0.2963091731071472\n",
            "[Training Epoch 60] Batch 3894, Loss 0.2657168507575989\n",
            "[Training Epoch 60] Batch 3895, Loss 0.291966050863266\n",
            "[Training Epoch 60] Batch 3896, Loss 0.26246050000190735\n",
            "[Training Epoch 60] Batch 3897, Loss 0.26544827222824097\n",
            "[Training Epoch 60] Batch 3898, Loss 0.2717750370502472\n",
            "[Training Epoch 60] Batch 3899, Loss 0.29660564661026\n",
            "[Training Epoch 60] Batch 3900, Loss 0.2424834817647934\n",
            "[Training Epoch 60] Batch 3901, Loss 0.25406497716903687\n",
            "[Training Epoch 60] Batch 3902, Loss 0.24283167719841003\n",
            "[Training Epoch 60] Batch 3903, Loss 0.2580511271953583\n",
            "[Training Epoch 60] Batch 3904, Loss 0.2557297646999359\n",
            "[Training Epoch 60] Batch 3905, Loss 0.2678574323654175\n",
            "[Training Epoch 60] Batch 3906, Loss 0.2755792438983917\n",
            "[Training Epoch 60] Batch 3907, Loss 0.2807297706604004\n",
            "[Training Epoch 60] Batch 3908, Loss 0.2747173607349396\n",
            "[Training Epoch 60] Batch 3909, Loss 0.26893508434295654\n",
            "[Training Epoch 60] Batch 3910, Loss 0.29478201270103455\n",
            "[Training Epoch 60] Batch 3911, Loss 0.26154711842536926\n",
            "[Training Epoch 60] Batch 3912, Loss 0.2480231523513794\n",
            "[Training Epoch 60] Batch 3913, Loss 0.24524712562561035\n",
            "[Training Epoch 60] Batch 3914, Loss 0.2488580346107483\n",
            "[Training Epoch 60] Batch 3915, Loss 0.2599816620349884\n",
            "[Training Epoch 60] Batch 3916, Loss 0.2621963322162628\n",
            "[Training Epoch 60] Batch 3917, Loss 0.25220561027526855\n",
            "[Training Epoch 60] Batch 3918, Loss 0.27567142248153687\n",
            "[Training Epoch 60] Batch 3919, Loss 0.27193784713745117\n",
            "[Training Epoch 60] Batch 3920, Loss 0.24006082117557526\n",
            "[Training Epoch 60] Batch 3921, Loss 0.27344420552253723\n",
            "[Training Epoch 60] Batch 3922, Loss 0.29888224601745605\n",
            "[Training Epoch 60] Batch 3923, Loss 0.25814124941825867\n",
            "[Training Epoch 60] Batch 3924, Loss 0.26312342286109924\n",
            "[Training Epoch 60] Batch 3925, Loss 0.268241822719574\n",
            "[Training Epoch 60] Batch 3926, Loss 0.26084911823272705\n",
            "[Training Epoch 60] Batch 3927, Loss 0.26509833335876465\n",
            "[Training Epoch 60] Batch 3928, Loss 0.2565608024597168\n",
            "[Training Epoch 60] Batch 3929, Loss 0.2940126061439514\n",
            "[Training Epoch 60] Batch 3930, Loss 0.2643827497959137\n",
            "[Training Epoch 60] Batch 3931, Loss 0.25642624497413635\n",
            "[Training Epoch 60] Batch 3932, Loss 0.2361876517534256\n",
            "[Training Epoch 60] Batch 3933, Loss 0.2720569670200348\n",
            "[Training Epoch 60] Batch 3934, Loss 0.25508734583854675\n",
            "[Training Epoch 60] Batch 3935, Loss 0.2899719774723053\n",
            "[Training Epoch 60] Batch 3936, Loss 0.27296775579452515\n",
            "[Training Epoch 60] Batch 3937, Loss 0.26294469833374023\n",
            "[Training Epoch 60] Batch 3938, Loss 0.2766111493110657\n",
            "[Training Epoch 60] Batch 3939, Loss 0.27633532881736755\n",
            "[Training Epoch 60] Batch 3940, Loss 0.27898845076560974\n",
            "[Training Epoch 60] Batch 3941, Loss 0.25998106598854065\n",
            "[Training Epoch 60] Batch 3942, Loss 0.25335830450057983\n",
            "[Training Epoch 60] Batch 3943, Loss 0.27371251583099365\n",
            "[Training Epoch 60] Batch 3944, Loss 0.26215067505836487\n",
            "[Training Epoch 60] Batch 3945, Loss 0.2759077847003937\n",
            "[Training Epoch 60] Batch 3946, Loss 0.26648861169815063\n",
            "[Training Epoch 60] Batch 3947, Loss 0.2594913840293884\n",
            "[Training Epoch 60] Batch 3948, Loss 0.27674469351768494\n",
            "[Training Epoch 60] Batch 3949, Loss 0.30118465423583984\n",
            "[Training Epoch 60] Batch 3950, Loss 0.25185132026672363\n",
            "[Training Epoch 60] Batch 3951, Loss 0.2506539821624756\n",
            "[Training Epoch 60] Batch 3952, Loss 0.26800328493118286\n",
            "[Training Epoch 60] Batch 3953, Loss 0.2395940124988556\n",
            "[Training Epoch 60] Batch 3954, Loss 0.2519448399543762\n",
            "[Training Epoch 60] Batch 3955, Loss 0.2740366756916046\n",
            "[Training Epoch 60] Batch 3956, Loss 0.26161566376686096\n",
            "[Training Epoch 60] Batch 3957, Loss 0.23988956212997437\n",
            "[Training Epoch 60] Batch 3958, Loss 0.28479206562042236\n",
            "[Training Epoch 60] Batch 3959, Loss 0.26269808411598206\n",
            "[Training Epoch 60] Batch 3960, Loss 0.28945139050483704\n",
            "[Training Epoch 60] Batch 3961, Loss 0.26916220784187317\n",
            "[Training Epoch 60] Batch 3962, Loss 0.24886974692344666\n",
            "[Training Epoch 60] Batch 3963, Loss 0.2952885031700134\n",
            "[Training Epoch 60] Batch 3964, Loss 0.2595425248146057\n",
            "[Training Epoch 60] Batch 3965, Loss 0.2523654103279114\n",
            "[Training Epoch 60] Batch 3966, Loss 0.25943654775619507\n",
            "[Training Epoch 60] Batch 3967, Loss 0.2563263773918152\n",
            "[Training Epoch 60] Batch 3968, Loss 0.26751333475112915\n",
            "[Training Epoch 60] Batch 3969, Loss 0.26630932092666626\n",
            "[Training Epoch 60] Batch 3970, Loss 0.2844286262989044\n",
            "[Training Epoch 60] Batch 3971, Loss 0.23883631825447083\n",
            "[Training Epoch 60] Batch 3972, Loss 0.26939088106155396\n",
            "[Training Epoch 60] Batch 3973, Loss 0.2949041724205017\n",
            "[Training Epoch 60] Batch 3974, Loss 0.2597230076789856\n",
            "[Training Epoch 60] Batch 3975, Loss 0.26685115694999695\n",
            "[Training Epoch 60] Batch 3976, Loss 0.2610911428928375\n",
            "[Training Epoch 60] Batch 3977, Loss 0.2806965708732605\n",
            "[Training Epoch 60] Batch 3978, Loss 0.2793574631214142\n",
            "[Training Epoch 60] Batch 3979, Loss 0.2536109983921051\n",
            "[Training Epoch 60] Batch 3980, Loss 0.2572876513004303\n",
            "[Training Epoch 60] Batch 3981, Loss 0.2691590189933777\n",
            "[Training Epoch 60] Batch 3982, Loss 0.25989198684692383\n",
            "[Training Epoch 60] Batch 3983, Loss 0.273922860622406\n",
            "[Training Epoch 60] Batch 3984, Loss 0.25902900099754333\n",
            "[Training Epoch 60] Batch 3985, Loss 0.278998464345932\n",
            "[Training Epoch 60] Batch 3986, Loss 0.26778316497802734\n",
            "[Training Epoch 60] Batch 3987, Loss 0.28047749400138855\n",
            "[Training Epoch 60] Batch 3988, Loss 0.24845212697982788\n",
            "[Training Epoch 60] Batch 3989, Loss 0.2564058303833008\n",
            "[Training Epoch 60] Batch 3990, Loss 0.29708850383758545\n",
            "[Training Epoch 60] Batch 3991, Loss 0.26423555612564087\n",
            "[Training Epoch 60] Batch 3992, Loss 0.27393266558647156\n",
            "[Training Epoch 60] Batch 3993, Loss 0.2500598132610321\n",
            "[Training Epoch 60] Batch 3994, Loss 0.27889692783355713\n",
            "[Training Epoch 60] Batch 3995, Loss 0.2557513415813446\n",
            "[Training Epoch 60] Batch 3996, Loss 0.24983154237270355\n",
            "[Training Epoch 60] Batch 3997, Loss 0.24049140512943268\n",
            "[Training Epoch 60] Batch 3998, Loss 0.27173617482185364\n",
            "[Training Epoch 60] Batch 3999, Loss 0.2660163938999176\n",
            "[Training Epoch 60] Batch 4000, Loss 0.24798598885536194\n",
            "[Training Epoch 60] Batch 4001, Loss 0.28572702407836914\n",
            "[Training Epoch 60] Batch 4002, Loss 0.27741819620132446\n",
            "[Training Epoch 60] Batch 4003, Loss 0.25785917043685913\n",
            "[Training Epoch 60] Batch 4004, Loss 0.26520541310310364\n",
            "[Training Epoch 60] Batch 4005, Loss 0.27336597442626953\n",
            "[Training Epoch 60] Batch 4006, Loss 0.275465190410614\n",
            "[Training Epoch 60] Batch 4007, Loss 0.24933584034442902\n",
            "[Training Epoch 60] Batch 4008, Loss 0.26062190532684326\n",
            "[Training Epoch 60] Batch 4009, Loss 0.25181224942207336\n",
            "[Training Epoch 60] Batch 4010, Loss 0.2608185410499573\n",
            "[Training Epoch 60] Batch 4011, Loss 0.2828494906425476\n",
            "[Training Epoch 60] Batch 4012, Loss 0.2686655819416046\n",
            "[Training Epoch 60] Batch 4013, Loss 0.25215888023376465\n",
            "[Training Epoch 60] Batch 4014, Loss 0.25624769926071167\n",
            "[Training Epoch 60] Batch 4015, Loss 0.28750482201576233\n",
            "[Training Epoch 60] Batch 4016, Loss 0.2667819559574127\n",
            "[Training Epoch 60] Batch 4017, Loss 0.27187612652778625\n",
            "[Training Epoch 60] Batch 4018, Loss 0.24301885068416595\n",
            "[Training Epoch 60] Batch 4019, Loss 0.24571457505226135\n",
            "[Training Epoch 60] Batch 4020, Loss 0.25545215606689453\n",
            "[Training Epoch 60] Batch 4021, Loss 0.22904852032661438\n",
            "[Training Epoch 60] Batch 4022, Loss 0.2492106407880783\n",
            "[Training Epoch 60] Batch 4023, Loss 0.2808469533920288\n",
            "[Training Epoch 60] Batch 4024, Loss 0.2582789659500122\n",
            "[Training Epoch 60] Batch 4025, Loss 0.2930704653263092\n",
            "[Training Epoch 60] Batch 4026, Loss 0.2710363566875458\n",
            "[Training Epoch 60] Batch 4027, Loss 0.24659360945224762\n",
            "[Training Epoch 60] Batch 4028, Loss 0.28042563796043396\n",
            "[Training Epoch 60] Batch 4029, Loss 0.31520548462867737\n",
            "[Training Epoch 60] Batch 4030, Loss 0.2678021490573883\n",
            "[Training Epoch 60] Batch 4031, Loss 0.25896674394607544\n",
            "[Training Epoch 60] Batch 4032, Loss 0.24812136590480804\n",
            "[Training Epoch 60] Batch 4033, Loss 0.25659888982772827\n",
            "[Training Epoch 60] Batch 4034, Loss 0.2954971194267273\n",
            "[Training Epoch 60] Batch 4035, Loss 0.2690180838108063\n",
            "[Training Epoch 60] Batch 4036, Loss 0.27692416310310364\n",
            "[Training Epoch 60] Batch 4037, Loss 0.2581639289855957\n",
            "[Training Epoch 60] Batch 4038, Loss 0.2764926850795746\n",
            "[Training Epoch 60] Batch 4039, Loss 0.26088324189186096\n",
            "[Training Epoch 60] Batch 4040, Loss 0.25779274106025696\n",
            "[Training Epoch 60] Batch 4041, Loss 0.24590954184532166\n",
            "[Training Epoch 60] Batch 4042, Loss 0.24353627860546112\n",
            "[Training Epoch 60] Batch 4043, Loss 0.28562241792678833\n",
            "[Training Epoch 60] Batch 4044, Loss 0.2512059807777405\n",
            "[Training Epoch 60] Batch 4045, Loss 0.24984654784202576\n",
            "[Training Epoch 60] Batch 4046, Loss 0.26011574268341064\n",
            "[Training Epoch 60] Batch 4047, Loss 0.2780398428440094\n",
            "[Training Epoch 60] Batch 4048, Loss 0.2752569019794464\n",
            "[Training Epoch 60] Batch 4049, Loss 0.2566782236099243\n",
            "[Training Epoch 60] Batch 4050, Loss 0.28962063789367676\n",
            "[Training Epoch 60] Batch 4051, Loss 0.2796061933040619\n",
            "[Training Epoch 60] Batch 4052, Loss 0.26750731468200684\n",
            "[Training Epoch 60] Batch 4053, Loss 0.239131361246109\n",
            "[Training Epoch 60] Batch 4054, Loss 0.25089871883392334\n",
            "[Training Epoch 60] Batch 4055, Loss 0.27804917097091675\n",
            "[Training Epoch 60] Batch 4056, Loss 0.26378124952316284\n",
            "[Training Epoch 60] Batch 4057, Loss 0.2957216799259186\n",
            "[Training Epoch 60] Batch 4058, Loss 0.2854086756706238\n",
            "[Training Epoch 60] Batch 4059, Loss 0.284388929605484\n",
            "[Training Epoch 60] Batch 4060, Loss 0.2563270926475525\n",
            "[Training Epoch 60] Batch 4061, Loss 0.27808618545532227\n",
            "[Training Epoch 60] Batch 4062, Loss 0.289792537689209\n",
            "[Training Epoch 60] Batch 4063, Loss 0.2523934841156006\n",
            "[Training Epoch 60] Batch 4064, Loss 0.2611614465713501\n",
            "[Training Epoch 60] Batch 4065, Loss 0.2582566440105438\n",
            "[Training Epoch 60] Batch 4066, Loss 0.2705134451389313\n",
            "[Training Epoch 60] Batch 4067, Loss 0.263759970664978\n",
            "[Training Epoch 60] Batch 4068, Loss 0.2432681769132614\n",
            "[Training Epoch 60] Batch 4069, Loss 0.25409072637557983\n",
            "[Training Epoch 60] Batch 4070, Loss 0.2532147765159607\n",
            "[Training Epoch 60] Batch 4071, Loss 0.2606393098831177\n",
            "[Training Epoch 60] Batch 4072, Loss 0.2501145899295807\n",
            "[Training Epoch 60] Batch 4073, Loss 0.2616814374923706\n",
            "[Training Epoch 60] Batch 4074, Loss 0.24949756264686584\n",
            "[Training Epoch 60] Batch 4075, Loss 0.26269447803497314\n",
            "[Training Epoch 60] Batch 4076, Loss 0.26613157987594604\n",
            "[Training Epoch 60] Batch 4077, Loss 0.2565043866634369\n",
            "[Training Epoch 60] Batch 4078, Loss 0.2733774185180664\n",
            "[Training Epoch 60] Batch 4079, Loss 0.27654901146888733\n",
            "[Training Epoch 60] Batch 4080, Loss 0.274791419506073\n",
            "[Training Epoch 60] Batch 4081, Loss 0.23920653760433197\n",
            "[Training Epoch 60] Batch 4082, Loss 0.28519487380981445\n",
            "[Training Epoch 60] Batch 4083, Loss 0.264446884393692\n",
            "[Training Epoch 60] Batch 4084, Loss 0.2755202651023865\n",
            "[Training Epoch 60] Batch 4085, Loss 0.2548881471157074\n",
            "[Training Epoch 60] Batch 4086, Loss 0.28876450657844543\n",
            "[Training Epoch 60] Batch 4087, Loss 0.2606944441795349\n",
            "[Training Epoch 60] Batch 4088, Loss 0.2599039077758789\n",
            "[Training Epoch 60] Batch 4089, Loss 0.2725200057029724\n",
            "[Training Epoch 60] Batch 4090, Loss 0.2523631155490875\n",
            "[Training Epoch 60] Batch 4091, Loss 0.25620001554489136\n",
            "[Training Epoch 60] Batch 4092, Loss 0.24994510412216187\n",
            "[Training Epoch 60] Batch 4093, Loss 0.27572566270828247\n",
            "[Training Epoch 60] Batch 4094, Loss 0.26262301206588745\n",
            "[Training Epoch 60] Batch 4095, Loss 0.27269822359085083\n",
            "[Training Epoch 60] Batch 4096, Loss 0.24943064153194427\n",
            "[Training Epoch 60] Batch 4097, Loss 0.29080432653427124\n",
            "[Training Epoch 60] Batch 4098, Loss 0.2346923053264618\n",
            "[Training Epoch 60] Batch 4099, Loss 0.28923314809799194\n",
            "[Training Epoch 60] Batch 4100, Loss 0.2609313130378723\n",
            "[Training Epoch 60] Batch 4101, Loss 0.24689172208309174\n",
            "[Training Epoch 60] Batch 4102, Loss 0.27591195702552795\n",
            "[Training Epoch 60] Batch 4103, Loss 0.2799493670463562\n",
            "[Training Epoch 60] Batch 4104, Loss 0.28347620368003845\n",
            "[Training Epoch 60] Batch 4105, Loss 0.28453588485717773\n",
            "[Training Epoch 60] Batch 4106, Loss 0.3001793622970581\n",
            "[Training Epoch 60] Batch 4107, Loss 0.2762386202812195\n",
            "[Training Epoch 60] Batch 4108, Loss 0.23703274130821228\n",
            "[Training Epoch 60] Batch 4109, Loss 0.2650618255138397\n",
            "[Training Epoch 60] Batch 4110, Loss 0.27345311641693115\n",
            "[Training Epoch 60] Batch 4111, Loss 0.2732299268245697\n",
            "[Training Epoch 60] Batch 4112, Loss 0.24487486481666565\n",
            "[Training Epoch 60] Batch 4113, Loss 0.25785306096076965\n",
            "[Training Epoch 60] Batch 4114, Loss 0.24326454102993011\n",
            "[Training Epoch 60] Batch 4115, Loss 0.26664602756500244\n",
            "[Training Epoch 60] Batch 4116, Loss 0.2588198482990265\n",
            "[Training Epoch 60] Batch 4117, Loss 0.2481408417224884\n",
            "[Training Epoch 60] Batch 4118, Loss 0.2615213394165039\n",
            "[Training Epoch 60] Batch 4119, Loss 0.26398342847824097\n",
            "[Training Epoch 60] Batch 4120, Loss 0.2653815448284149\n",
            "[Training Epoch 60] Batch 4121, Loss 0.2837792634963989\n",
            "[Training Epoch 60] Batch 4122, Loss 0.24042953550815582\n",
            "[Training Epoch 60] Batch 4123, Loss 0.26491957902908325\n",
            "[Training Epoch 60] Batch 4124, Loss 0.27482160925865173\n",
            "[Training Epoch 60] Batch 4125, Loss 0.2677296996116638\n",
            "[Training Epoch 60] Batch 4126, Loss 0.27914193272590637\n",
            "[Training Epoch 60] Batch 4127, Loss 0.22534354031085968\n",
            "[Training Epoch 60] Batch 4128, Loss 0.2839326858520508\n",
            "[Training Epoch 60] Batch 4129, Loss 0.26226919889450073\n",
            "[Training Epoch 60] Batch 4130, Loss 0.24574881792068481\n",
            "[Training Epoch 60] Batch 4131, Loss 0.2583005130290985\n",
            "[Training Epoch 60] Batch 4132, Loss 0.2551208734512329\n",
            "[Training Epoch 60] Batch 4133, Loss 0.27637484669685364\n",
            "[Training Epoch 60] Batch 4134, Loss 0.2607129216194153\n",
            "[Training Epoch 60] Batch 4135, Loss 0.2466176152229309\n",
            "[Training Epoch 60] Batch 4136, Loss 0.31097155809402466\n",
            "[Training Epoch 60] Batch 4137, Loss 0.2846592664718628\n",
            "[Training Epoch 60] Batch 4138, Loss 0.25475212931632996\n",
            "[Training Epoch 60] Batch 4139, Loss 0.25207680463790894\n",
            "[Training Epoch 60] Batch 4140, Loss 0.2727040648460388\n",
            "[Training Epoch 60] Batch 4141, Loss 0.27550196647644043\n",
            "[Training Epoch 60] Batch 4142, Loss 0.24909111857414246\n",
            "[Training Epoch 60] Batch 4143, Loss 0.26246175169944763\n",
            "[Training Epoch 60] Batch 4144, Loss 0.260206401348114\n",
            "[Training Epoch 60] Batch 4145, Loss 0.25760889053344727\n",
            "[Training Epoch 60] Batch 4146, Loss 0.26090162992477417\n",
            "[Training Epoch 60] Batch 4147, Loss 0.251084566116333\n",
            "[Training Epoch 60] Batch 4148, Loss 0.2525966167449951\n",
            "[Training Epoch 60] Batch 4149, Loss 0.2581283450126648\n",
            "[Training Epoch 60] Batch 4150, Loss 0.2618674337863922\n",
            "[Training Epoch 60] Batch 4151, Loss 0.27158281207084656\n",
            "[Training Epoch 60] Batch 4152, Loss 0.294671893119812\n",
            "[Training Epoch 60] Batch 4153, Loss 0.29558491706848145\n",
            "[Training Epoch 60] Batch 4154, Loss 0.2609604299068451\n",
            "[Training Epoch 60] Batch 4155, Loss 0.271321564912796\n",
            "[Training Epoch 60] Batch 4156, Loss 0.25920921564102173\n",
            "[Training Epoch 60] Batch 4157, Loss 0.2536594867706299\n",
            "[Training Epoch 60] Batch 4158, Loss 0.26714473962783813\n",
            "[Training Epoch 60] Batch 4159, Loss 0.29004037380218506\n",
            "[Training Epoch 60] Batch 4160, Loss 0.27505746483802795\n",
            "[Training Epoch 60] Batch 4161, Loss 0.2671447992324829\n",
            "[Training Epoch 60] Batch 4162, Loss 0.24179187417030334\n",
            "[Training Epoch 60] Batch 4163, Loss 0.2511715888977051\n",
            "[Training Epoch 60] Batch 4164, Loss 0.2704157531261444\n",
            "[Training Epoch 60] Batch 4165, Loss 0.26863527297973633\n",
            "[Training Epoch 60] Batch 4166, Loss 0.2847103476524353\n",
            "[Training Epoch 60] Batch 4167, Loss 0.2502247095108032\n",
            "[Training Epoch 60] Batch 4168, Loss 0.24484676122665405\n",
            "[Training Epoch 60] Batch 4169, Loss 0.24733157455921173\n",
            "[Training Epoch 60] Batch 4170, Loss 0.3066664934158325\n",
            "[Training Epoch 60] Batch 4171, Loss 0.28335386514663696\n",
            "[Training Epoch 60] Batch 4172, Loss 0.259361207485199\n",
            "[Training Epoch 60] Batch 4173, Loss 0.2759731113910675\n",
            "[Training Epoch 60] Batch 4174, Loss 0.26177775859832764\n",
            "[Training Epoch 60] Batch 4175, Loss 0.27603182196617126\n",
            "[Training Epoch 60] Batch 4176, Loss 0.262279212474823\n",
            "[Training Epoch 60] Batch 4177, Loss 0.2598723769187927\n",
            "[Training Epoch 60] Batch 4178, Loss 0.2825477123260498\n",
            "[Training Epoch 60] Batch 4179, Loss 0.24067939817905426\n",
            "[Training Epoch 60] Batch 4180, Loss 0.28710517287254333\n",
            "[Training Epoch 60] Batch 4181, Loss 0.2726881206035614\n",
            "[Training Epoch 60] Batch 4182, Loss 0.2631831765174866\n",
            "[Training Epoch 60] Batch 4183, Loss 0.2470148354768753\n",
            "[Training Epoch 60] Batch 4184, Loss 0.2538186013698578\n",
            "[Training Epoch 60] Batch 4185, Loss 0.24183207750320435\n",
            "[Training Epoch 60] Batch 4186, Loss 0.27388420701026917\n",
            "[Training Epoch 60] Batch 4187, Loss 0.25938570499420166\n",
            "[Training Epoch 60] Batch 4188, Loss 0.2906213700771332\n",
            "[Training Epoch 60] Batch 4189, Loss 0.28113481402397156\n",
            "[Training Epoch 60] Batch 4190, Loss 0.24939346313476562\n",
            "[Training Epoch 60] Batch 4191, Loss 0.275403767824173\n",
            "[Training Epoch 60] Batch 4192, Loss 0.26358872652053833\n",
            "[Training Epoch 60] Batch 4193, Loss 0.26726895570755005\n",
            "[Training Epoch 60] Batch 4194, Loss 0.2526732385158539\n",
            "[Training Epoch 60] Batch 4195, Loss 0.2370818853378296\n",
            "[Training Epoch 60] Batch 4196, Loss 0.25588148832321167\n",
            "[Training Epoch 60] Batch 4197, Loss 0.25943562388420105\n",
            "[Training Epoch 60] Batch 4198, Loss 0.21553359925746918\n",
            "[Training Epoch 60] Batch 4199, Loss 0.26257795095443726\n",
            "[Training Epoch 60] Batch 4200, Loss 0.26726871728897095\n",
            "[Training Epoch 60] Batch 4201, Loss 0.243881955742836\n",
            "[Training Epoch 60] Batch 4202, Loss 0.25391995906829834\n",
            "[Training Epoch 60] Batch 4203, Loss 0.2507975995540619\n",
            "[Training Epoch 60] Batch 4204, Loss 0.2503228485584259\n",
            "[Training Epoch 60] Batch 4205, Loss 0.26124775409698486\n",
            "[Training Epoch 60] Batch 4206, Loss 0.24132105708122253\n",
            "[Training Epoch 60] Batch 4207, Loss 0.291313111782074\n",
            "[Training Epoch 60] Batch 4208, Loss 0.25568366050720215\n",
            "[Training Epoch 60] Batch 4209, Loss 0.2984292507171631\n",
            "[Training Epoch 60] Batch 4210, Loss 0.2658407390117645\n",
            "[Training Epoch 60] Batch 4211, Loss 0.265428751707077\n",
            "[Training Epoch 60] Batch 4212, Loss 0.26279670000076294\n",
            "[Training Epoch 60] Batch 4213, Loss 0.24130579829216003\n",
            "[Training Epoch 60] Batch 4214, Loss 0.2520074248313904\n",
            "[Training Epoch 60] Batch 4215, Loss 0.273272842168808\n",
            "[Training Epoch 60] Batch 4216, Loss 0.2640347182750702\n",
            "[Training Epoch 60] Batch 4217, Loss 0.2713492214679718\n",
            "[Training Epoch 60] Batch 4218, Loss 0.2799068093299866\n",
            "[Training Epoch 60] Batch 4219, Loss 0.2698841691017151\n",
            "[Training Epoch 60] Batch 4220, Loss 0.2528209984302521\n",
            "[Training Epoch 60] Batch 4221, Loss 0.2597365081310272\n",
            "[Training Epoch 60] Batch 4222, Loss 0.25703611969947815\n",
            "[Training Epoch 60] Batch 4223, Loss 0.25865060091018677\n",
            "[Training Epoch 60] Batch 4224, Loss 0.2860284447669983\n",
            "[Training Epoch 60] Batch 4225, Loss 0.2542983889579773\n",
            "[Training Epoch 60] Batch 4226, Loss 0.2720528542995453\n",
            "[Training Epoch 60] Batch 4227, Loss 0.29344502091407776\n",
            "[Training Epoch 60] Batch 4228, Loss 0.25823667645454407\n",
            "[Training Epoch 60] Batch 4229, Loss 0.27491068840026855\n",
            "[Training Epoch 60] Batch 4230, Loss 0.26923251152038574\n",
            "[Training Epoch 60] Batch 4231, Loss 0.2535463571548462\n",
            "[Training Epoch 60] Batch 4232, Loss 0.23900078237056732\n",
            "[Training Epoch 60] Batch 4233, Loss 0.26389503479003906\n",
            "[Training Epoch 60] Batch 4234, Loss 0.25874337553977966\n",
            "[Training Epoch 60] Batch 4235, Loss 0.2887735664844513\n",
            "[Training Epoch 60] Batch 4236, Loss 0.22870445251464844\n",
            "[Training Epoch 60] Batch 4237, Loss 0.24856114387512207\n",
            "[Training Epoch 60] Batch 4238, Loss 0.2651212513446808\n",
            "[Training Epoch 60] Batch 4239, Loss 0.264955997467041\n",
            "[Training Epoch 60] Batch 4240, Loss 0.256472110748291\n",
            "[Training Epoch 60] Batch 4241, Loss 0.2566487789154053\n",
            "[Training Epoch 60] Batch 4242, Loss 0.26244640350341797\n",
            "[Training Epoch 60] Batch 4243, Loss 0.24132680892944336\n",
            "[Training Epoch 60] Batch 4244, Loss 0.2841862738132477\n",
            "[Training Epoch 60] Batch 4245, Loss 0.2479923963546753\n",
            "[Training Epoch 60] Batch 4246, Loss 0.269542932510376\n",
            "[Training Epoch 60] Batch 4247, Loss 0.274531751871109\n",
            "[Training Epoch 60] Batch 4248, Loss 0.2565809488296509\n",
            "[Training Epoch 60] Batch 4249, Loss 0.2532098889350891\n",
            "[Training Epoch 60] Batch 4250, Loss 0.2803393006324768\n",
            "[Training Epoch 60] Batch 4251, Loss 0.26813116669654846\n",
            "[Training Epoch 60] Batch 4252, Loss 0.26258403062820435\n",
            "[Training Epoch 60] Batch 4253, Loss 0.2752998471260071\n",
            "[Training Epoch 60] Batch 4254, Loss 0.261886864900589\n",
            "[Training Epoch 60] Batch 4255, Loss 0.27074533700942993\n",
            "[Training Epoch 60] Batch 4256, Loss 0.2623251676559448\n",
            "[Training Epoch 60] Batch 4257, Loss 0.26353639364242554\n",
            "[Training Epoch 60] Batch 4258, Loss 0.2480001151561737\n",
            "[Training Epoch 60] Batch 4259, Loss 0.27010858058929443\n",
            "[Training Epoch 60] Batch 4260, Loss 0.2768406867980957\n",
            "[Training Epoch 60] Batch 4261, Loss 0.242153137922287\n",
            "[Training Epoch 60] Batch 4262, Loss 0.2793161869049072\n",
            "[Training Epoch 60] Batch 4263, Loss 0.2706413269042969\n",
            "[Training Epoch 60] Batch 4264, Loss 0.2556781768798828\n",
            "[Training Epoch 60] Batch 4265, Loss 0.26405224204063416\n",
            "[Training Epoch 60] Batch 4266, Loss 0.2395268976688385\n",
            "[Training Epoch 60] Batch 4267, Loss 0.26040324568748474\n",
            "[Training Epoch 60] Batch 4268, Loss 0.2871387302875519\n",
            "[Training Epoch 60] Batch 4269, Loss 0.26140570640563965\n",
            "[Training Epoch 60] Batch 4270, Loss 0.28989917039871216\n",
            "[Training Epoch 60] Batch 4271, Loss 0.28849732875823975\n",
            "[Training Epoch 60] Batch 4272, Loss 0.27095937728881836\n",
            "[Training Epoch 60] Batch 4273, Loss 0.29119670391082764\n",
            "[Training Epoch 60] Batch 4274, Loss 0.25681009888648987\n",
            "[Training Epoch 60] Batch 4275, Loss 0.2563309967517853\n",
            "[Training Epoch 60] Batch 4276, Loss 0.27062010765075684\n",
            "[Training Epoch 60] Batch 4277, Loss 0.25548189878463745\n",
            "[Training Epoch 60] Batch 4278, Loss 0.24948392808437347\n",
            "[Training Epoch 60] Batch 4279, Loss 0.26088419556617737\n",
            "[Training Epoch 60] Batch 4280, Loss 0.2679944634437561\n",
            "[Training Epoch 60] Batch 4281, Loss 0.27096909284591675\n",
            "[Training Epoch 60] Batch 4282, Loss 0.26492175459861755\n",
            "[Training Epoch 60] Batch 4283, Loss 0.2724907398223877\n",
            "[Training Epoch 60] Batch 4284, Loss 0.2788679301738739\n",
            "[Training Epoch 60] Batch 4285, Loss 0.2547065317630768\n",
            "[Training Epoch 60] Batch 4286, Loss 0.2694985866546631\n",
            "[Training Epoch 60] Batch 4287, Loss 0.26150065660476685\n",
            "[Training Epoch 60] Batch 4288, Loss 0.2775289714336395\n",
            "[Training Epoch 60] Batch 4289, Loss 0.26552486419677734\n",
            "[Training Epoch 60] Batch 4290, Loss 0.24390307068824768\n",
            "[Training Epoch 60] Batch 4291, Loss 0.26677101850509644\n",
            "[Training Epoch 60] Batch 4292, Loss 0.26234689354896545\n",
            "[Training Epoch 60] Batch 4293, Loss 0.24622684717178345\n",
            "[Training Epoch 60] Batch 4294, Loss 0.27325499057769775\n",
            "[Training Epoch 60] Batch 4295, Loss 0.27191436290740967\n",
            "[Training Epoch 60] Batch 4296, Loss 0.22093291580677032\n",
            "[Training Epoch 60] Batch 4297, Loss 0.2634119391441345\n",
            "[Training Epoch 60] Batch 4298, Loss 0.2718590497970581\n",
            "[Training Epoch 60] Batch 4299, Loss 0.2382274568080902\n",
            "[Training Epoch 60] Batch 4300, Loss 0.26905080676078796\n",
            "[Training Epoch 60] Batch 4301, Loss 0.2781391441822052\n",
            "[Training Epoch 60] Batch 4302, Loss 0.22330041229724884\n",
            "[Training Epoch 60] Batch 4303, Loss 0.2463555932044983\n",
            "[Training Epoch 60] Batch 4304, Loss 0.2760010361671448\n",
            "[Training Epoch 60] Batch 4305, Loss 0.2568054497241974\n",
            "[Training Epoch 60] Batch 4306, Loss 0.25746920704841614\n",
            "[Training Epoch 60] Batch 4307, Loss 0.27104055881500244\n",
            "[Training Epoch 60] Batch 4308, Loss 0.2529941201210022\n",
            "[Training Epoch 60] Batch 4309, Loss 0.2542259991168976\n",
            "[Training Epoch 60] Batch 4310, Loss 0.2634657621383667\n",
            "[Training Epoch 60] Batch 4311, Loss 0.2525293827056885\n",
            "[Training Epoch 60] Batch 4312, Loss 0.2829553484916687\n",
            "[Training Epoch 60] Batch 4313, Loss 0.2598782479763031\n",
            "[Training Epoch 60] Batch 4314, Loss 0.257045716047287\n",
            "[Training Epoch 60] Batch 4315, Loss 0.2834244668483734\n",
            "[Training Epoch 60] Batch 4316, Loss 0.22684074938297272\n",
            "[Training Epoch 60] Batch 4317, Loss 0.28170672059059143\n",
            "[Training Epoch 60] Batch 4318, Loss 0.26807135343551636\n",
            "[Training Epoch 60] Batch 4319, Loss 0.2681632339954376\n",
            "[Training Epoch 60] Batch 4320, Loss 0.2598631680011749\n",
            "[Training Epoch 60] Batch 4321, Loss 0.2565389573574066\n",
            "[Training Epoch 60] Batch 4322, Loss 0.2777004539966583\n",
            "[Training Epoch 60] Batch 4323, Loss 0.23360739648342133\n",
            "[Training Epoch 60] Batch 4324, Loss 0.23968806862831116\n",
            "[Training Epoch 60] Batch 4325, Loss 0.2677059471607208\n",
            "[Training Epoch 60] Batch 4326, Loss 0.26437193155288696\n",
            "[Training Epoch 60] Batch 4327, Loss 0.25460824370384216\n",
            "[Training Epoch 60] Batch 4328, Loss 0.2723267078399658\n",
            "[Training Epoch 60] Batch 4329, Loss 0.26011958718299866\n",
            "[Training Epoch 60] Batch 4330, Loss 0.2554134130477905\n",
            "[Training Epoch 60] Batch 4331, Loss 0.2656874358654022\n",
            "[Training Epoch 60] Batch 4332, Loss 0.24713309109210968\n",
            "[Training Epoch 60] Batch 4333, Loss 0.24409708380699158\n",
            "[Training Epoch 60] Batch 4334, Loss 0.28583991527557373\n",
            "[Training Epoch 60] Batch 4335, Loss 0.2738044857978821\n",
            "[Training Epoch 60] Batch 4336, Loss 0.2962356507778168\n",
            "[Training Epoch 60] Batch 4337, Loss 0.2690567374229431\n",
            "[Training Epoch 60] Batch 4338, Loss 0.27194517850875854\n",
            "[Training Epoch 60] Batch 4339, Loss 0.2648043930530548\n",
            "[Training Epoch 60] Batch 4340, Loss 0.24024033546447754\n",
            "[Training Epoch 60] Batch 4341, Loss 0.28068190813064575\n",
            "[Training Epoch 60] Batch 4342, Loss 0.26126807928085327\n",
            "[Training Epoch 60] Batch 4343, Loss 0.2729969322681427\n",
            "[Training Epoch 60] Batch 4344, Loss 0.29627862572669983\n",
            "[Training Epoch 60] Batch 4345, Loss 0.26019176840782166\n",
            "[Training Epoch 60] Batch 4346, Loss 0.2819575369358063\n",
            "[Training Epoch 60] Batch 4347, Loss 0.27497443556785583\n",
            "[Training Epoch 60] Batch 4348, Loss 0.24916885793209076\n",
            "[Training Epoch 60] Batch 4349, Loss 0.2620885372161865\n",
            "[Training Epoch 60] Batch 4350, Loss 0.27753233909606934\n",
            "[Training Epoch 60] Batch 4351, Loss 0.26638635993003845\n",
            "[Training Epoch 60] Batch 4352, Loss 0.2665087580680847\n",
            "[Training Epoch 60] Batch 4353, Loss 0.2901689112186432\n",
            "[Training Epoch 60] Batch 4354, Loss 0.2600545287132263\n",
            "[Training Epoch 60] Batch 4355, Loss 0.26961421966552734\n",
            "[Training Epoch 60] Batch 4356, Loss 0.26213759183883667\n",
            "[Training Epoch 60] Batch 4357, Loss 0.24135099351406097\n",
            "[Training Epoch 60] Batch 4358, Loss 0.2759176194667816\n",
            "[Training Epoch 60] Batch 4359, Loss 0.27709558606147766\n",
            "[Training Epoch 60] Batch 4360, Loss 0.25891220569610596\n",
            "[Training Epoch 60] Batch 4361, Loss 0.22626042366027832\n",
            "[Training Epoch 60] Batch 4362, Loss 0.2525646686553955\n",
            "[Training Epoch 60] Batch 4363, Loss 0.27800050377845764\n",
            "[Training Epoch 60] Batch 4364, Loss 0.2789442837238312\n",
            "[Training Epoch 60] Batch 4365, Loss 0.2647493779659271\n",
            "[Training Epoch 60] Batch 4366, Loss 0.25191211700439453\n",
            "[Training Epoch 60] Batch 4367, Loss 0.2697403132915497\n",
            "[Training Epoch 60] Batch 4368, Loss 0.2561582624912262\n",
            "[Training Epoch 60] Batch 4369, Loss 0.26233911514282227\n",
            "[Training Epoch 60] Batch 4370, Loss 0.2665572166442871\n",
            "[Training Epoch 60] Batch 4371, Loss 0.24768532812595367\n",
            "[Training Epoch 60] Batch 4372, Loss 0.24654905498027802\n",
            "[Training Epoch 60] Batch 4373, Loss 0.26703423261642456\n",
            "[Training Epoch 60] Batch 4374, Loss 0.264998197555542\n",
            "[Training Epoch 60] Batch 4375, Loss 0.2660441994667053\n",
            "[Training Epoch 60] Batch 4376, Loss 0.25417301058769226\n",
            "[Training Epoch 60] Batch 4377, Loss 0.2830131947994232\n",
            "[Training Epoch 60] Batch 4378, Loss 0.2579385042190552\n",
            "[Training Epoch 60] Batch 4379, Loss 0.2473934292793274\n",
            "[Training Epoch 60] Batch 4380, Loss 0.253958523273468\n",
            "[Training Epoch 60] Batch 4381, Loss 0.26963984966278076\n",
            "[Training Epoch 60] Batch 4382, Loss 0.2540327310562134\n",
            "[Training Epoch 60] Batch 4383, Loss 0.26540887355804443\n",
            "[Training Epoch 60] Batch 4384, Loss 0.2548814117908478\n",
            "[Training Epoch 60] Batch 4385, Loss 0.2728871703147888\n",
            "[Training Epoch 60] Batch 4386, Loss 0.27105194330215454\n",
            "[Training Epoch 60] Batch 4387, Loss 0.31112542748451233\n",
            "[Training Epoch 60] Batch 4388, Loss 0.26247018575668335\n",
            "[Training Epoch 60] Batch 4389, Loss 0.2630070745944977\n",
            "[Training Epoch 60] Batch 4390, Loss 0.28116029500961304\n",
            "[Training Epoch 60] Batch 4391, Loss 0.24897529184818268\n",
            "[Training Epoch 60] Batch 4392, Loss 0.2942352890968323\n",
            "[Training Epoch 60] Batch 4393, Loss 0.3130156993865967\n",
            "[Training Epoch 60] Batch 4394, Loss 0.26573464274406433\n",
            "[Training Epoch 60] Batch 4395, Loss 0.24074530601501465\n",
            "[Training Epoch 60] Batch 4396, Loss 0.2486138641834259\n",
            "[Training Epoch 60] Batch 4397, Loss 0.2713484764099121\n",
            "[Training Epoch 60] Batch 4398, Loss 0.25589731335639954\n",
            "[Training Epoch 60] Batch 4399, Loss 0.2661501169204712\n",
            "[Training Epoch 60] Batch 4400, Loss 0.27836930751800537\n",
            "[Training Epoch 60] Batch 4401, Loss 0.29758507013320923\n",
            "[Training Epoch 60] Batch 4402, Loss 0.29895472526550293\n",
            "[Training Epoch 60] Batch 4403, Loss 0.2584543228149414\n",
            "[Training Epoch 60] Batch 4404, Loss 0.2666437029838562\n",
            "[Training Epoch 60] Batch 4405, Loss 0.250840961933136\n",
            "[Training Epoch 60] Batch 4406, Loss 0.2725423574447632\n",
            "[Training Epoch 60] Batch 4407, Loss 0.27028822898864746\n",
            "[Training Epoch 60] Batch 4408, Loss 0.2685331106185913\n",
            "[Training Epoch 60] Batch 4409, Loss 0.24684566259384155\n",
            "[Training Epoch 60] Batch 4410, Loss 0.25958251953125\n",
            "[Training Epoch 60] Batch 4411, Loss 0.25380468368530273\n",
            "[Training Epoch 60] Batch 4412, Loss 0.26580750942230225\n",
            "[Training Epoch 60] Batch 4413, Loss 0.27658915519714355\n",
            "[Training Epoch 60] Batch 4414, Loss 0.28136035799980164\n",
            "[Training Epoch 60] Batch 4415, Loss 0.26806288957595825\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ae8e2ee92b2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n# cd ./src\\nls\\ncd ./neural-collaborative-filtering/src\\npython train.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m       raise subprocess.CalledProcessError(\n\u001b[0;32m--> 139\u001b[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command '\n# cd ./src\nls\ncd ./neural-collaborative-filtering/src\npython train.py' returned non-zero exit status 1."
          ]
        }
      ]
    }
  ]
}